{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/trupolog/opt/anaconda3/lib/python3.9/site-packages/xgboost/compat.py:36: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  from pandas import MultiIndex, Int64Index\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import keras\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras.applications.densenet import layers\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import *\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.callbacks import EarlyStopping\n",
    "from sklearn.metrics import r2_score\n",
    "from catboost import CatBoostRegressor\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "data = pd.read_csv('../datasets/Processed_DJI.csv', index_col=[0], parse_dates=[0])\n",
    "data = data[['Close','Volume']]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "def make_features(data, max_lag, rolling_mean_size):\n",
    "    data['dayofweek'] = data.index.dayofweek\n",
    "    data['year'] = data.index.year\n",
    "    data['month'] = data.index.month\n",
    "\n",
    "    for lag in range(1, max_lag + 1):\n",
    "        data['lag_{}'.format(lag)] = data['Close'].shift(lag)\n",
    "\n",
    "    data['rolling_mean'] = data['Close'].shift().rolling(rolling_mean_size).mean()\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/47/4y0ypxc92dz4v_h0jyn8jqpw0000gn/T/ipykernel_3174/615298662.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data['lag_{}'.format(lag)] = data['Close'].shift(lag)\n",
      "/var/folders/47/4y0ypxc92dz4v_h0jyn8jqpw0000gn/T/ipykernel_3174/615298662.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data['lag_{}'.format(lag)] = data['Close'].shift(lag)\n",
      "/var/folders/47/4y0ypxc92dz4v_h0jyn8jqpw0000gn/T/ipykernel_3174/615298662.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data['lag_{}'.format(lag)] = data['Close'].shift(lag)\n",
      "/var/folders/47/4y0ypxc92dz4v_h0jyn8jqpw0000gn/T/ipykernel_3174/615298662.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data['lag_{}'.format(lag)] = data['Close'].shift(lag)\n",
      "/var/folders/47/4y0ypxc92dz4v_h0jyn8jqpw0000gn/T/ipykernel_3174/615298662.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data['lag_{}'.format(lag)] = data['Close'].shift(lag)\n",
      "/var/folders/47/4y0ypxc92dz4v_h0jyn8jqpw0000gn/T/ipykernel_3174/615298662.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data['lag_{}'.format(lag)] = data['Close'].shift(lag)\n",
      "/var/folders/47/4y0ypxc92dz4v_h0jyn8jqpw0000gn/T/ipykernel_3174/615298662.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data['lag_{}'.format(lag)] = data['Close'].shift(lag)\n",
      "/var/folders/47/4y0ypxc92dz4v_h0jyn8jqpw0000gn/T/ipykernel_3174/615298662.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data['lag_{}'.format(lag)] = data['Close'].shift(lag)\n",
      "/var/folders/47/4y0ypxc92dz4v_h0jyn8jqpw0000gn/T/ipykernel_3174/615298662.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data['lag_{}'.format(lag)] = data['Close'].shift(lag)\n",
      "/var/folders/47/4y0ypxc92dz4v_h0jyn8jqpw0000gn/T/ipykernel_3174/615298662.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data['lag_{}'.format(lag)] = data['Close'].shift(lag)\n",
      "/var/folders/47/4y0ypxc92dz4v_h0jyn8jqpw0000gn/T/ipykernel_3174/615298662.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data['lag_{}'.format(lag)] = data['Close'].shift(lag)\n",
      "/var/folders/47/4y0ypxc92dz4v_h0jyn8jqpw0000gn/T/ipykernel_3174/615298662.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data['lag_{}'.format(lag)] = data['Close'].shift(lag)\n",
      "/var/folders/47/4y0ypxc92dz4v_h0jyn8jqpw0000gn/T/ipykernel_3174/615298662.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data['lag_{}'.format(lag)] = data['Close'].shift(lag)\n",
      "/var/folders/47/4y0ypxc92dz4v_h0jyn8jqpw0000gn/T/ipykernel_3174/615298662.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data['lag_{}'.format(lag)] = data['Close'].shift(lag)\n",
      "/var/folders/47/4y0ypxc92dz4v_h0jyn8jqpw0000gn/T/ipykernel_3174/615298662.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data['lag_{}'.format(lag)] = data['Close'].shift(lag)\n",
      "/var/folders/47/4y0ypxc92dz4v_h0jyn8jqpw0000gn/T/ipykernel_3174/615298662.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data['lag_{}'.format(lag)] = data['Close'].shift(lag)\n",
      "/var/folders/47/4y0ypxc92dz4v_h0jyn8jqpw0000gn/T/ipykernel_3174/615298662.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data['lag_{}'.format(lag)] = data['Close'].shift(lag)\n",
      "/var/folders/47/4y0ypxc92dz4v_h0jyn8jqpw0000gn/T/ipykernel_3174/615298662.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data['lag_{}'.format(lag)] = data['Close'].shift(lag)\n",
      "/var/folders/47/4y0ypxc92dz4v_h0jyn8jqpw0000gn/T/ipykernel_3174/615298662.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data['lag_{}'.format(lag)] = data['Close'].shift(lag)\n",
      "/var/folders/47/4y0ypxc92dz4v_h0jyn8jqpw0000gn/T/ipykernel_3174/615298662.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data['lag_{}'.format(lag)] = data['Close'].shift(lag)\n",
      "/var/folders/47/4y0ypxc92dz4v_h0jyn8jqpw0000gn/T/ipykernel_3174/615298662.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data['lag_{}'.format(lag)] = data['Close'].shift(lag)\n",
      "/var/folders/47/4y0ypxc92dz4v_h0jyn8jqpw0000gn/T/ipykernel_3174/615298662.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data['lag_{}'.format(lag)] = data['Close'].shift(lag)\n",
      "/var/folders/47/4y0ypxc92dz4v_h0jyn8jqpw0000gn/T/ipykernel_3174/615298662.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data['lag_{}'.format(lag)] = data['Close'].shift(lag)\n",
      "/var/folders/47/4y0ypxc92dz4v_h0jyn8jqpw0000gn/T/ipykernel_3174/615298662.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data['lag_{}'.format(lag)] = data['Close'].shift(lag)\n",
      "/var/folders/47/4y0ypxc92dz4v_h0jyn8jqpw0000gn/T/ipykernel_3174/615298662.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data['lag_{}'.format(lag)] = data['Close'].shift(lag)\n",
      "/var/folders/47/4y0ypxc92dz4v_h0jyn8jqpw0000gn/T/ipykernel_3174/615298662.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data['lag_{}'.format(lag)] = data['Close'].shift(lag)\n",
      "/var/folders/47/4y0ypxc92dz4v_h0jyn8jqpw0000gn/T/ipykernel_3174/615298662.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data['lag_{}'.format(lag)] = data['Close'].shift(lag)\n",
      "/var/folders/47/4y0ypxc92dz4v_h0jyn8jqpw0000gn/T/ipykernel_3174/615298662.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data['lag_{}'.format(lag)] = data['Close'].shift(lag)\n",
      "/var/folders/47/4y0ypxc92dz4v_h0jyn8jqpw0000gn/T/ipykernel_3174/615298662.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data['lag_{}'.format(lag)] = data['Close'].shift(lag)\n",
      "/var/folders/47/4y0ypxc92dz4v_h0jyn8jqpw0000gn/T/ipykernel_3174/615298662.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data['lag_{}'.format(lag)] = data['Close'].shift(lag)\n",
      "/var/folders/47/4y0ypxc92dz4v_h0jyn8jqpw0000gn/T/ipykernel_3174/615298662.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data['lag_{}'.format(lag)] = data['Close'].shift(lag)\n",
      "/var/folders/47/4y0ypxc92dz4v_h0jyn8jqpw0000gn/T/ipykernel_3174/615298662.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data['lag_{}'.format(lag)] = data['Close'].shift(lag)\n",
      "/var/folders/47/4y0ypxc92dz4v_h0jyn8jqpw0000gn/T/ipykernel_3174/615298662.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data['lag_{}'.format(lag)] = data['Close'].shift(lag)\n",
      "/var/folders/47/4y0ypxc92dz4v_h0jyn8jqpw0000gn/T/ipykernel_3174/615298662.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data['lag_{}'.format(lag)] = data['Close'].shift(lag)\n",
      "/var/folders/47/4y0ypxc92dz4v_h0jyn8jqpw0000gn/T/ipykernel_3174/615298662.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data['lag_{}'.format(lag)] = data['Close'].shift(lag)\n",
      "/var/folders/47/4y0ypxc92dz4v_h0jyn8jqpw0000gn/T/ipykernel_3174/615298662.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data['lag_{}'.format(lag)] = data['Close'].shift(lag)\n",
      "/var/folders/47/4y0ypxc92dz4v_h0jyn8jqpw0000gn/T/ipykernel_3174/615298662.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data['lag_{}'.format(lag)] = data['Close'].shift(lag)\n",
      "/var/folders/47/4y0ypxc92dz4v_h0jyn8jqpw0000gn/T/ipykernel_3174/615298662.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data['lag_{}'.format(lag)] = data['Close'].shift(lag)\n",
      "/var/folders/47/4y0ypxc92dz4v_h0jyn8jqpw0000gn/T/ipykernel_3174/615298662.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data['lag_{}'.format(lag)] = data['Close'].shift(lag)\n",
      "/var/folders/47/4y0ypxc92dz4v_h0jyn8jqpw0000gn/T/ipykernel_3174/615298662.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data['lag_{}'.format(lag)] = data['Close'].shift(lag)\n",
      "/var/folders/47/4y0ypxc92dz4v_h0jyn8jqpw0000gn/T/ipykernel_3174/615298662.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data['lag_{}'.format(lag)] = data['Close'].shift(lag)\n",
      "/var/folders/47/4y0ypxc92dz4v_h0jyn8jqpw0000gn/T/ipykernel_3174/615298662.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data['lag_{}'.format(lag)] = data['Close'].shift(lag)\n",
      "/var/folders/47/4y0ypxc92dz4v_h0jyn8jqpw0000gn/T/ipykernel_3174/615298662.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data['lag_{}'.format(lag)] = data['Close'].shift(lag)\n",
      "/var/folders/47/4y0ypxc92dz4v_h0jyn8jqpw0000gn/T/ipykernel_3174/615298662.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data['lag_{}'.format(lag)] = data['Close'].shift(lag)\n",
      "/var/folders/47/4y0ypxc92dz4v_h0jyn8jqpw0000gn/T/ipykernel_3174/615298662.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data['lag_{}'.format(lag)] = data['Close'].shift(lag)\n",
      "/var/folders/47/4y0ypxc92dz4v_h0jyn8jqpw0000gn/T/ipykernel_3174/615298662.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data['lag_{}'.format(lag)] = data['Close'].shift(lag)\n",
      "/var/folders/47/4y0ypxc92dz4v_h0jyn8jqpw0000gn/T/ipykernel_3174/615298662.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data['lag_{}'.format(lag)] = data['Close'].shift(lag)\n",
      "/var/folders/47/4y0ypxc92dz4v_h0jyn8jqpw0000gn/T/ipykernel_3174/615298662.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data['lag_{}'.format(lag)] = data['Close'].shift(lag)\n",
      "/var/folders/47/4y0ypxc92dz4v_h0jyn8jqpw0000gn/T/ipykernel_3174/615298662.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data['lag_{}'.format(lag)] = data['Close'].shift(lag)\n",
      "/var/folders/47/4y0ypxc92dz4v_h0jyn8jqpw0000gn/T/ipykernel_3174/615298662.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data['lag_{}'.format(lag)] = data['Close'].shift(lag)\n",
      "/var/folders/47/4y0ypxc92dz4v_h0jyn8jqpw0000gn/T/ipykernel_3174/615298662.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data['lag_{}'.format(lag)] = data['Close'].shift(lag)\n",
      "/var/folders/47/4y0ypxc92dz4v_h0jyn8jqpw0000gn/T/ipykernel_3174/615298662.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data['lag_{}'.format(lag)] = data['Close'].shift(lag)\n",
      "/var/folders/47/4y0ypxc92dz4v_h0jyn8jqpw0000gn/T/ipykernel_3174/615298662.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data['lag_{}'.format(lag)] = data['Close'].shift(lag)\n",
      "/var/folders/47/4y0ypxc92dz4v_h0jyn8jqpw0000gn/T/ipykernel_3174/615298662.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data['lag_{}'.format(lag)] = data['Close'].shift(lag)\n",
      "/var/folders/47/4y0ypxc92dz4v_h0jyn8jqpw0000gn/T/ipykernel_3174/615298662.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data['lag_{}'.format(lag)] = data['Close'].shift(lag)\n",
      "/var/folders/47/4y0ypxc92dz4v_h0jyn8jqpw0000gn/T/ipykernel_3174/615298662.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data['lag_{}'.format(lag)] = data['Close'].shift(lag)\n",
      "/var/folders/47/4y0ypxc92dz4v_h0jyn8jqpw0000gn/T/ipykernel_3174/615298662.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data['lag_{}'.format(lag)] = data['Close'].shift(lag)\n",
      "/var/folders/47/4y0ypxc92dz4v_h0jyn8jqpw0000gn/T/ipykernel_3174/615298662.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data['lag_{}'.format(lag)] = data['Close'].shift(lag)\n",
      "/var/folders/47/4y0ypxc92dz4v_h0jyn8jqpw0000gn/T/ipykernel_3174/615298662.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data['lag_{}'.format(lag)] = data['Close'].shift(lag)\n",
      "/var/folders/47/4y0ypxc92dz4v_h0jyn8jqpw0000gn/T/ipykernel_3174/615298662.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data['lag_{}'.format(lag)] = data['Close'].shift(lag)\n",
      "/var/folders/47/4y0ypxc92dz4v_h0jyn8jqpw0000gn/T/ipykernel_3174/615298662.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data['lag_{}'.format(lag)] = data['Close'].shift(lag)\n",
      "/var/folders/47/4y0ypxc92dz4v_h0jyn8jqpw0000gn/T/ipykernel_3174/615298662.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data['lag_{}'.format(lag)] = data['Close'].shift(lag)\n",
      "/var/folders/47/4y0ypxc92dz4v_h0jyn8jqpw0000gn/T/ipykernel_3174/615298662.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data['lag_{}'.format(lag)] = data['Close'].shift(lag)\n",
      "/var/folders/47/4y0ypxc92dz4v_h0jyn8jqpw0000gn/T/ipykernel_3174/615298662.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data['lag_{}'.format(lag)] = data['Close'].shift(lag)\n",
      "/var/folders/47/4y0ypxc92dz4v_h0jyn8jqpw0000gn/T/ipykernel_3174/615298662.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data['lag_{}'.format(lag)] = data['Close'].shift(lag)\n",
      "/var/folders/47/4y0ypxc92dz4v_h0jyn8jqpw0000gn/T/ipykernel_3174/615298662.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data['lag_{}'.format(lag)] = data['Close'].shift(lag)\n",
      "/var/folders/47/4y0ypxc92dz4v_h0jyn8jqpw0000gn/T/ipykernel_3174/615298662.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data['lag_{}'.format(lag)] = data['Close'].shift(lag)\n",
      "/var/folders/47/4y0ypxc92dz4v_h0jyn8jqpw0000gn/T/ipykernel_3174/615298662.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data['lag_{}'.format(lag)] = data['Close'].shift(lag)\n",
      "/var/folders/47/4y0ypxc92dz4v_h0jyn8jqpw0000gn/T/ipykernel_3174/615298662.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data['lag_{}'.format(lag)] = data['Close'].shift(lag)\n",
      "/var/folders/47/4y0ypxc92dz4v_h0jyn8jqpw0000gn/T/ipykernel_3174/615298662.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data['lag_{}'.format(lag)] = data['Close'].shift(lag)\n",
      "/var/folders/47/4y0ypxc92dz4v_h0jyn8jqpw0000gn/T/ipykernel_3174/615298662.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data['lag_{}'.format(lag)] = data['Close'].shift(lag)\n",
      "/var/folders/47/4y0ypxc92dz4v_h0jyn8jqpw0000gn/T/ipykernel_3174/615298662.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data['lag_{}'.format(lag)] = data['Close'].shift(lag)\n",
      "/var/folders/47/4y0ypxc92dz4v_h0jyn8jqpw0000gn/T/ipykernel_3174/615298662.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data['rolling_mean'] = data['Close'].shift().rolling(rolling_mean_size).mean()\n"
     ]
    },
    {
     "data": {
      "text/plain": "                   Close     Volume  dayofweek  year  month         lag_1  \\\nDate                                                                        \n2009-12-31  10428.049805        NaN          3  2009     12           NaN   \n2010-01-04  10583.959961        NaN          0  2010      1  10428.049805   \n2010-01-05  10572.019531        NaN          1  2010      1  10583.959961   \n2010-01-06  10573.679688   0.515598          2  2010      1  10572.019531   \n2010-01-07  10606.860352   9.776045          3  2010      1  10573.679688   \n...                  ...        ...        ...   ...    ...           ...   \n2017-11-09  23461.939453   6.511740          3  2017     11  23563.359375   \n2017-11-10  23422.210938  -0.991838          4  2017     11  23461.939453   \n2017-11-13  23439.699219 -65.347705          0  2017     11  23422.210938   \n2017-11-14  23409.470703  -1.387911          1  2017     11  23439.699219   \n2017-11-15  23271.279297   3.612171          2  2017     11  23409.470703   \n\n                   lag_2         lag_3         lag_4         lag_5  ...  \\\nDate                                                                ...   \n2009-12-31           NaN           NaN           NaN           NaN  ...   \n2010-01-04           NaN           NaN           NaN           NaN  ...   \n2010-01-05  10428.049805           NaN           NaN           NaN  ...   \n2010-01-06  10583.959961  10428.049805           NaN           NaN  ...   \n2010-01-07  10572.019531  10583.959961  10428.049805           NaN  ...   \n...                  ...           ...           ...           ...  ...   \n2017-11-09  23557.230469  23548.419922  23539.189453  23516.259766  ...   \n2017-11-10  23563.359375  23557.230469  23548.419922  23539.189453  ...   \n2017-11-13  23461.939453  23563.359375  23557.230469  23548.419922  ...   \n2017-11-14  23422.210938  23461.939453  23563.359375  23557.230469  ...   \n2017-11-15  23439.699219  23422.210938  23461.939453  23563.359375  ...   \n\n                 lag_160       lag_161       lag_162       lag_163  \\\nDate                                                                 \n2009-12-31           NaN           NaN           NaN           NaN   \n2010-01-04           NaN           NaN           NaN           NaN   \n2010-01-05           NaN           NaN           NaN           NaN   \n2010-01-06           NaN           NaN           NaN           NaN   \n2010-01-07           NaN           NaN           NaN           NaN   \n...                  ...           ...           ...           ...   \n2017-11-09  20596.720703  20656.580078  20661.300781  20668.009766   \n2017-11-10  20550.980469  20596.720703  20656.580078  20661.300781   \n2017-11-13  20701.500000  20550.980469  20596.720703  20656.580078   \n2017-11-14  20659.320313  20701.500000  20550.980469  20596.720703   \n2017-11-15  20728.490234  20659.320313  20701.500000  20550.980469   \n\n                 lag_164       lag_165       lag_166       lag_167  \\\nDate                                                                 \n2009-12-31           NaN           NaN           NaN           NaN   \n2010-01-04           NaN           NaN           NaN           NaN   \n2010-01-05           NaN           NaN           NaN           NaN   \n2010-01-06           NaN           NaN           NaN           NaN   \n2010-01-07           NaN           NaN           NaN           NaN   \n...                  ...           ...           ...           ...   \n2017-11-09  20905.859375  20914.619141  20934.550781  20950.099609   \n2017-11-10  20668.009766  20905.859375  20914.619141  20934.550781   \n2017-11-13  20661.300781  20668.009766  20905.859375  20914.619141   \n2017-11-14  20656.580078  20661.300781  20668.009766  20905.859375   \n2017-11-15  20596.720703  20656.580078  20661.300781  20668.009766   \n\n                 lag_168  rolling_mean  \nDate                                    \n2009-12-31           NaN           NaN  \n2010-01-04           NaN           NaN  \n2010-01-05           NaN           NaN  \n2010-01-06           NaN           NaN  \n2010-01-07           NaN           NaN  \n...                  ...           ...  \n2017-11-09  20837.369141  21667.338728  \n2017-11-10  20950.099609  21682.961170  \n2017-11-13  20934.550781  21697.676119  \n2017-11-14  20914.619141  21712.587716  \n2017-11-15  20905.859375  21727.438023  \n\n[1984 rows x 174 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Close</th>\n      <th>Volume</th>\n      <th>dayofweek</th>\n      <th>year</th>\n      <th>month</th>\n      <th>lag_1</th>\n      <th>lag_2</th>\n      <th>lag_3</th>\n      <th>lag_4</th>\n      <th>lag_5</th>\n      <th>...</th>\n      <th>lag_160</th>\n      <th>lag_161</th>\n      <th>lag_162</th>\n      <th>lag_163</th>\n      <th>lag_164</th>\n      <th>lag_165</th>\n      <th>lag_166</th>\n      <th>lag_167</th>\n      <th>lag_168</th>\n      <th>rolling_mean</th>\n    </tr>\n    <tr>\n      <th>Date</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>2009-12-31</th>\n      <td>10428.049805</td>\n      <td>NaN</td>\n      <td>3</td>\n      <td>2009</td>\n      <td>12</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>2010-01-04</th>\n      <td>10583.959961</td>\n      <td>NaN</td>\n      <td>0</td>\n      <td>2010</td>\n      <td>1</td>\n      <td>10428.049805</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>2010-01-05</th>\n      <td>10572.019531</td>\n      <td>NaN</td>\n      <td>1</td>\n      <td>2010</td>\n      <td>1</td>\n      <td>10583.959961</td>\n      <td>10428.049805</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>2010-01-06</th>\n      <td>10573.679688</td>\n      <td>0.515598</td>\n      <td>2</td>\n      <td>2010</td>\n      <td>1</td>\n      <td>10572.019531</td>\n      <td>10583.959961</td>\n      <td>10428.049805</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>2010-01-07</th>\n      <td>10606.860352</td>\n      <td>9.776045</td>\n      <td>3</td>\n      <td>2010</td>\n      <td>1</td>\n      <td>10573.679688</td>\n      <td>10572.019531</td>\n      <td>10583.959961</td>\n      <td>10428.049805</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>2017-11-09</th>\n      <td>23461.939453</td>\n      <td>6.511740</td>\n      <td>3</td>\n      <td>2017</td>\n      <td>11</td>\n      <td>23563.359375</td>\n      <td>23557.230469</td>\n      <td>23548.419922</td>\n      <td>23539.189453</td>\n      <td>23516.259766</td>\n      <td>...</td>\n      <td>20596.720703</td>\n      <td>20656.580078</td>\n      <td>20661.300781</td>\n      <td>20668.009766</td>\n      <td>20905.859375</td>\n      <td>20914.619141</td>\n      <td>20934.550781</td>\n      <td>20950.099609</td>\n      <td>20837.369141</td>\n      <td>21667.338728</td>\n    </tr>\n    <tr>\n      <th>2017-11-10</th>\n      <td>23422.210938</td>\n      <td>-0.991838</td>\n      <td>4</td>\n      <td>2017</td>\n      <td>11</td>\n      <td>23461.939453</td>\n      <td>23563.359375</td>\n      <td>23557.230469</td>\n      <td>23548.419922</td>\n      <td>23539.189453</td>\n      <td>...</td>\n      <td>20550.980469</td>\n      <td>20596.720703</td>\n      <td>20656.580078</td>\n      <td>20661.300781</td>\n      <td>20668.009766</td>\n      <td>20905.859375</td>\n      <td>20914.619141</td>\n      <td>20934.550781</td>\n      <td>20950.099609</td>\n      <td>21682.961170</td>\n    </tr>\n    <tr>\n      <th>2017-11-13</th>\n      <td>23439.699219</td>\n      <td>-65.347705</td>\n      <td>0</td>\n      <td>2017</td>\n      <td>11</td>\n      <td>23422.210938</td>\n      <td>23461.939453</td>\n      <td>23563.359375</td>\n      <td>23557.230469</td>\n      <td>23548.419922</td>\n      <td>...</td>\n      <td>20701.500000</td>\n      <td>20550.980469</td>\n      <td>20596.720703</td>\n      <td>20656.580078</td>\n      <td>20661.300781</td>\n      <td>20668.009766</td>\n      <td>20905.859375</td>\n      <td>20914.619141</td>\n      <td>20934.550781</td>\n      <td>21697.676119</td>\n    </tr>\n    <tr>\n      <th>2017-11-14</th>\n      <td>23409.470703</td>\n      <td>-1.387911</td>\n      <td>1</td>\n      <td>2017</td>\n      <td>11</td>\n      <td>23439.699219</td>\n      <td>23422.210938</td>\n      <td>23461.939453</td>\n      <td>23563.359375</td>\n      <td>23557.230469</td>\n      <td>...</td>\n      <td>20659.320313</td>\n      <td>20701.500000</td>\n      <td>20550.980469</td>\n      <td>20596.720703</td>\n      <td>20656.580078</td>\n      <td>20661.300781</td>\n      <td>20668.009766</td>\n      <td>20905.859375</td>\n      <td>20914.619141</td>\n      <td>21712.587716</td>\n    </tr>\n    <tr>\n      <th>2017-11-15</th>\n      <td>23271.279297</td>\n      <td>3.612171</td>\n      <td>2</td>\n      <td>2017</td>\n      <td>11</td>\n      <td>23409.470703</td>\n      <td>23439.699219</td>\n      <td>23422.210938</td>\n      <td>23461.939453</td>\n      <td>23563.359375</td>\n      <td>...</td>\n      <td>20728.490234</td>\n      <td>20659.320313</td>\n      <td>20701.500000</td>\n      <td>20550.980469</td>\n      <td>20596.720703</td>\n      <td>20656.580078</td>\n      <td>20661.300781</td>\n      <td>20668.009766</td>\n      <td>20905.859375</td>\n      <td>21727.438023</td>\n    </tr>\n  </tbody>\n</table>\n<p>1984 rows × 174 columns</p>\n</div>"
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "make_features(data, 168, 168)\n",
    "data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "data = data.drop('Volume', axis=1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "train, test = train_test_split(data, test_size=0.1, random_state=12345, shuffle=False)\n",
    "train = train.dropna()\n",
    "\n",
    "\n",
    "features_train = train.drop('Close', axis=1)\n",
    "target_train = train['Close']\n",
    "features_test = test.drop('Close', axis=1)\n",
    "target_test = test['Close']\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(features_train)\n",
    "features_train = scaler.transform(features_train)\n",
    "features_test = scaler.transform(features_test)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE на кросс-валидации -343.5608172433523\n",
      "R2 на кросс-валидации 0.2756232792573243\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score, TimeSeriesSplit\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "lin_model = LinearRegression()\n",
    "lin_model.fit(features_train, target_train)\n",
    "grid = cross_val_score(lin_model, X=features_train, y=target_train, n_jobs=-1, scoring='neg_root_mean_squared_error', cv=TimeSeriesSplit(n_splits=5))\n",
    "print('RMSE на кросс-валидации', grid.mean())\n",
    "grid = cross_val_score(lin_model, X=features_train, y=target_train, n_jobs=-1, scoring='r2', cv=TimeSeriesSplit(n_splits=5))\n",
    "print('R2 на кросс-валидации', grid.mean())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "\n",
    "\n",
    "model = RandomForestRegressor(random_state=12345)\n",
    "params = {'n_estimators': range(100, 601, 50), 'max_depth': range(5, 51, 10)}\n",
    "cv = TimeSeriesSplit(n_splits=5)\n",
    "grid = GridSearchCV(model, params, n_jobs=-1, verbose=True, cv=cv, scoring='r2')\n",
    "grid.fit(features_train, target_train)\n",
    "print('Лучшие параметры:', grid.best_params_)\n",
    "print('Лучший счет:', grid.best_score_)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "forest = RandomForestRegressor(max_depth=35, n_estimators=600, random_state=12345)\n",
    "forest.fit(features_train, target_train)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "\n",
    "model = CatBoostRegressor(loss_function=\"MAPE\")\n",
    "params = {'learning_rate': [0.01, 0.03, 0.1, 0.12],\n",
    "        'depth': [4, 5, 6, 7, 10, 20],\n",
    "        'l2_leaf_reg': [1, 3, 5, 6,  7, 8, 9]}\n",
    "grid_search_result = model.grid_search(params, X=features_train, y=target_train, cv = TimeSeriesSplit(n_splits=5))\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "outputs": [],
   "source": [
    "print(grid_search_result[\"params\"][\"depth\"])\n",
    "print(grid_search_result[\"params\"][\"learning_rate\"])\n",
    "print(grid_search_result[\"params\"][\"l2_leaf_reg\"])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [],
   "source": [
    "best_depth = 6\n",
    "best_learning_rate = 0.12 #ЛУЧШИЕ ДЛЯ МАПЕ\n",
    "best_l2 = 3"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def smape(A, F):\n",
    "    return 100 / len(A) * np.sum(2 * np.abs(F - A) / (np.abs(A) + np.abs(F)))\n",
    "\n",
    "\n",
    "def metrics(y_true, y_pred):\n",
    "    print('R2 Score:', r2_score(y_true=y_true, y_pred=y_pred))\n",
    "    print('RMSE:', mean_squared_error(y_true=y_true, y_pred=y_pred, squared=False))\n",
    "    print('MAPE:', mean_absolute_percentage_error(y_true=y_true, y_pred=y_pred))\n",
    "    return r2_score(y_true=y_true, y_pred=y_pred), mean_squared_error(y_true=y_true, y_pred=y_pred, squared=False), mean_absolute_percentage_error(y_true=y_true, y_pred=y_pred)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 0.1460308\ttotal: 14.8ms\tremaining: 14.8s\n",
      "1:\tlearn: 0.1320703\ttotal: 22.8ms\tremaining: 11.4s\n",
      "2:\tlearn: 0.1188133\ttotal: 29.3ms\tremaining: 9.75s\n",
      "3:\tlearn: 0.1127556\ttotal: 39.3ms\tremaining: 9.78s\n",
      "4:\tlearn: 0.1073027\ttotal: 46.3ms\tremaining: 9.21s\n",
      "5:\tlearn: 0.1014448\ttotal: 52.4ms\tremaining: 8.68s\n",
      "6:\tlearn: 0.0926364\ttotal: 59ms\tremaining: 8.37s\n",
      "7:\tlearn: 0.0885089\ttotal: 65.9ms\tremaining: 8.17s\n",
      "8:\tlearn: 0.0820857\ttotal: 72.4ms\tremaining: 7.98s\n",
      "9:\tlearn: 0.0797856\ttotal: 78.1ms\tremaining: 7.73s\n",
      "10:\tlearn: 0.0744111\ttotal: 83.6ms\tremaining: 7.51s\n",
      "11:\tlearn: 0.0717590\ttotal: 89.2ms\tremaining: 7.34s\n",
      "12:\tlearn: 0.0697892\ttotal: 95.5ms\tremaining: 7.25s\n",
      "13:\tlearn: 0.0667040\ttotal: 101ms\tremaining: 7.1s\n",
      "14:\tlearn: 0.0647808\ttotal: 106ms\tremaining: 6.96s\n",
      "15:\tlearn: 0.0621054\ttotal: 111ms\tremaining: 6.85s\n",
      "16:\tlearn: 0.0607585\ttotal: 117ms\tremaining: 6.76s\n",
      "17:\tlearn: 0.0588060\ttotal: 123ms\tremaining: 6.69s\n",
      "18:\tlearn: 0.0579018\ttotal: 129ms\tremaining: 6.66s\n",
      "19:\tlearn: 0.0565370\ttotal: 136ms\tremaining: 6.65s\n",
      "20:\tlearn: 0.0556930\ttotal: 141ms\tremaining: 6.6s\n",
      "21:\tlearn: 0.0537152\ttotal: 153ms\tremaining: 6.8s\n",
      "22:\tlearn: 0.0527408\ttotal: 159ms\tremaining: 6.74s\n",
      "23:\tlearn: 0.0514190\ttotal: 166ms\tremaining: 6.74s\n",
      "24:\tlearn: 0.0511645\ttotal: 172ms\tremaining: 6.7s\n",
      "25:\tlearn: 0.0510414\ttotal: 177ms\tremaining: 6.64s\n",
      "26:\tlearn: 0.0505732\ttotal: 185ms\tremaining: 6.67s\n",
      "27:\tlearn: 0.0505189\ttotal: 193ms\tremaining: 6.71s\n",
      "28:\tlearn: 0.0477460\ttotal: 198ms\tremaining: 6.64s\n",
      "29:\tlearn: 0.0472860\ttotal: 204ms\tremaining: 6.58s\n",
      "30:\tlearn: 0.0465654\ttotal: 209ms\tremaining: 6.54s\n",
      "31:\tlearn: 0.0456655\ttotal: 215ms\tremaining: 6.5s\n",
      "32:\tlearn: 0.0454886\ttotal: 221ms\tremaining: 6.48s\n",
      "33:\tlearn: 0.0448370\ttotal: 228ms\tremaining: 6.49s\n",
      "34:\tlearn: 0.0442712\ttotal: 234ms\tremaining: 6.46s\n",
      "35:\tlearn: 0.0441480\ttotal: 240ms\tremaining: 6.41s\n",
      "36:\tlearn: 0.0437438\ttotal: 245ms\tremaining: 6.37s\n",
      "37:\tlearn: 0.0433861\ttotal: 253ms\tremaining: 6.4s\n",
      "38:\tlearn: 0.0428085\ttotal: 260ms\tremaining: 6.4s\n",
      "39:\tlearn: 0.0424412\ttotal: 266ms\tremaining: 6.39s\n",
      "40:\tlearn: 0.0417574\ttotal: 273ms\tremaining: 6.38s\n",
      "41:\tlearn: 0.0414666\ttotal: 280ms\tremaining: 6.38s\n",
      "42:\tlearn: 0.0406143\ttotal: 291ms\tremaining: 6.48s\n",
      "43:\tlearn: 0.0404704\ttotal: 297ms\tremaining: 6.45s\n",
      "44:\tlearn: 0.0403438\ttotal: 303ms\tremaining: 6.42s\n",
      "45:\tlearn: 0.0401344\ttotal: 309ms\tremaining: 6.41s\n",
      "46:\tlearn: 0.0396365\ttotal: 319ms\tremaining: 6.47s\n",
      "47:\tlearn: 0.0392603\ttotal: 325ms\tremaining: 6.45s\n",
      "48:\tlearn: 0.0382134\ttotal: 330ms\tremaining: 6.41s\n",
      "49:\tlearn: 0.0381916\ttotal: 340ms\tremaining: 6.46s\n",
      "50:\tlearn: 0.0376608\ttotal: 346ms\tremaining: 6.45s\n",
      "51:\tlearn: 0.0374034\ttotal: 354ms\tremaining: 6.46s\n",
      "52:\tlearn: 0.0373814\ttotal: 360ms\tremaining: 6.43s\n",
      "53:\tlearn: 0.0368051\ttotal: 366ms\tremaining: 6.41s\n",
      "54:\tlearn: 0.0366700\ttotal: 372ms\tremaining: 6.39s\n",
      "55:\tlearn: 0.0361106\ttotal: 378ms\tremaining: 6.37s\n",
      "56:\tlearn: 0.0358727\ttotal: 383ms\tremaining: 6.34s\n",
      "57:\tlearn: 0.0354490\ttotal: 392ms\tremaining: 6.37s\n",
      "58:\tlearn: 0.0353890\ttotal: 398ms\tremaining: 6.35s\n",
      "59:\tlearn: 0.0353841\ttotal: 404ms\tremaining: 6.33s\n",
      "60:\tlearn: 0.0353576\ttotal: 410ms\tremaining: 6.32s\n",
      "61:\tlearn: 0.0353186\ttotal: 416ms\tremaining: 6.29s\n",
      "62:\tlearn: 0.0352673\ttotal: 422ms\tremaining: 6.27s\n",
      "63:\tlearn: 0.0351683\ttotal: 428ms\tremaining: 6.26s\n",
      "64:\tlearn: 0.0350416\ttotal: 434ms\tremaining: 6.24s\n",
      "65:\tlearn: 0.0349607\ttotal: 439ms\tremaining: 6.22s\n",
      "66:\tlearn: 0.0345933\ttotal: 446ms\tremaining: 6.21s\n",
      "67:\tlearn: 0.0345397\ttotal: 452ms\tremaining: 6.2s\n",
      "68:\tlearn: 0.0344466\ttotal: 459ms\tremaining: 6.19s\n",
      "69:\tlearn: 0.0342693\ttotal: 465ms\tremaining: 6.17s\n",
      "70:\tlearn: 0.0339004\ttotal: 471ms\tremaining: 6.16s\n",
      "71:\tlearn: 0.0334789\ttotal: 476ms\tremaining: 6.13s\n",
      "72:\tlearn: 0.0332975\ttotal: 484ms\tremaining: 6.15s\n",
      "73:\tlearn: 0.0331291\ttotal: 491ms\tremaining: 6.14s\n",
      "74:\tlearn: 0.0328425\ttotal: 496ms\tremaining: 6.12s\n",
      "75:\tlearn: 0.0325623\ttotal: 502ms\tremaining: 6.1s\n",
      "76:\tlearn: 0.0325601\ttotal: 508ms\tremaining: 6.09s\n",
      "77:\tlearn: 0.0323403\ttotal: 514ms\tremaining: 6.08s\n",
      "78:\tlearn: 0.0322235\ttotal: 520ms\tremaining: 6.07s\n",
      "79:\tlearn: 0.0322219\ttotal: 526ms\tremaining: 6.05s\n",
      "80:\tlearn: 0.0321576\ttotal: 531ms\tremaining: 6.03s\n",
      "81:\tlearn: 0.0319249\ttotal: 538ms\tremaining: 6.02s\n",
      "82:\tlearn: 0.0318829\ttotal: 547ms\tremaining: 6.04s\n",
      "83:\tlearn: 0.0318797\ttotal: 552ms\tremaining: 6.02s\n",
      "84:\tlearn: 0.0315110\ttotal: 558ms\tremaining: 6s\n",
      "85:\tlearn: 0.0309669\ttotal: 563ms\tremaining: 5.98s\n",
      "86:\tlearn: 0.0308476\ttotal: 569ms\tremaining: 5.97s\n",
      "87:\tlearn: 0.0308315\ttotal: 575ms\tremaining: 5.96s\n",
      "88:\tlearn: 0.0308143\ttotal: 582ms\tremaining: 5.95s\n",
      "89:\tlearn: 0.0305901\ttotal: 589ms\tremaining: 5.95s\n",
      "90:\tlearn: 0.0305849\ttotal: 594ms\tremaining: 5.93s\n",
      "91:\tlearn: 0.0305011\ttotal: 600ms\tremaining: 5.92s\n",
      "92:\tlearn: 0.0304469\ttotal: 609ms\tremaining: 5.94s\n",
      "93:\tlearn: 0.0303497\ttotal: 615ms\tremaining: 5.93s\n",
      "94:\tlearn: 0.0302512\ttotal: 621ms\tremaining: 5.91s\n",
      "95:\tlearn: 0.0302459\ttotal: 626ms\tremaining: 5.9s\n",
      "96:\tlearn: 0.0302410\ttotal: 632ms\tremaining: 5.88s\n",
      "97:\tlearn: 0.0302055\ttotal: 638ms\tremaining: 5.87s\n",
      "98:\tlearn: 0.0301925\ttotal: 646ms\tremaining: 5.88s\n",
      "99:\tlearn: 0.0301877\ttotal: 654ms\tremaining: 5.88s\n",
      "100:\tlearn: 0.0301715\ttotal: 660ms\tremaining: 5.87s\n",
      "101:\tlearn: 0.0301645\ttotal: 665ms\tremaining: 5.86s\n",
      "102:\tlearn: 0.0299604\ttotal: 671ms\tremaining: 5.84s\n",
      "103:\tlearn: 0.0299518\ttotal: 676ms\tremaining: 5.82s\n",
      "104:\tlearn: 0.0299402\ttotal: 684ms\tremaining: 5.83s\n",
      "105:\tlearn: 0.0298068\ttotal: 690ms\tremaining: 5.82s\n",
      "106:\tlearn: 0.0297183\ttotal: 695ms\tremaining: 5.8s\n",
      "107:\tlearn: 0.0297064\ttotal: 701ms\tremaining: 5.79s\n",
      "108:\tlearn: 0.0296951\ttotal: 707ms\tremaining: 5.78s\n",
      "109:\tlearn: 0.0296356\ttotal: 713ms\tremaining: 5.77s\n",
      "110:\tlearn: 0.0295515\ttotal: 719ms\tremaining: 5.76s\n",
      "111:\tlearn: 0.0293781\ttotal: 725ms\tremaining: 5.74s\n",
      "112:\tlearn: 0.0293769\ttotal: 730ms\tremaining: 5.73s\n",
      "113:\tlearn: 0.0293612\ttotal: 736ms\tremaining: 5.72s\n",
      "114:\tlearn: 0.0293485\ttotal: 744ms\tremaining: 5.72s\n",
      "115:\tlearn: 0.0293287\ttotal: 749ms\tremaining: 5.7s\n",
      "116:\tlearn: 0.0293091\ttotal: 754ms\tremaining: 5.69s\n",
      "117:\tlearn: 0.0293080\ttotal: 759ms\tremaining: 5.68s\n",
      "118:\tlearn: 0.0292996\ttotal: 766ms\tremaining: 5.67s\n",
      "119:\tlearn: 0.0292243\ttotal: 772ms\tremaining: 5.66s\n",
      "120:\tlearn: 0.0291793\ttotal: 780ms\tremaining: 5.67s\n",
      "121:\tlearn: 0.0291725\ttotal: 795ms\tremaining: 5.72s\n",
      "122:\tlearn: 0.0291172\ttotal: 806ms\tremaining: 5.75s\n",
      "123:\tlearn: 0.0290540\ttotal: 813ms\tremaining: 5.74s\n",
      "124:\tlearn: 0.0290534\ttotal: 820ms\tremaining: 5.74s\n",
      "125:\tlearn: 0.0289971\ttotal: 826ms\tremaining: 5.73s\n",
      "126:\tlearn: 0.0289922\ttotal: 832ms\tremaining: 5.72s\n",
      "127:\tlearn: 0.0289818\ttotal: 844ms\tremaining: 5.75s\n",
      "128:\tlearn: 0.0289805\ttotal: 852ms\tremaining: 5.75s\n",
      "129:\tlearn: 0.0289744\ttotal: 860ms\tremaining: 5.75s\n",
      "130:\tlearn: 0.0289402\ttotal: 865ms\tremaining: 5.74s\n",
      "131:\tlearn: 0.0289404\ttotal: 871ms\tremaining: 5.73s\n",
      "132:\tlearn: 0.0289392\ttotal: 879ms\tremaining: 5.73s\n",
      "133:\tlearn: 0.0289211\ttotal: 885ms\tremaining: 5.72s\n",
      "134:\tlearn: 0.0289154\ttotal: 891ms\tremaining: 5.71s\n",
      "135:\tlearn: 0.0289102\ttotal: 897ms\tremaining: 5.7s\n",
      "136:\tlearn: 0.0289095\ttotal: 903ms\tremaining: 5.68s\n",
      "137:\tlearn: 0.0288204\ttotal: 917ms\tremaining: 5.73s\n",
      "138:\tlearn: 0.0288197\ttotal: 922ms\tremaining: 5.71s\n",
      "139:\tlearn: 0.0288120\ttotal: 928ms\tremaining: 5.7s\n",
      "140:\tlearn: 0.0287985\ttotal: 934ms\tremaining: 5.69s\n",
      "141:\tlearn: 0.0287815\ttotal: 939ms\tremaining: 5.68s\n",
      "142:\tlearn: 0.0287775\ttotal: 946ms\tremaining: 5.67s\n",
      "143:\tlearn: 0.0287286\ttotal: 952ms\tremaining: 5.66s\n",
      "144:\tlearn: 0.0286818\ttotal: 958ms\tremaining: 5.65s\n",
      "145:\tlearn: 0.0286272\ttotal: 968ms\tremaining: 5.66s\n",
      "146:\tlearn: 0.0286213\ttotal: 980ms\tremaining: 5.68s\n",
      "147:\tlearn: 0.0286179\ttotal: 985ms\tremaining: 5.67s\n",
      "148:\tlearn: 0.0286030\ttotal: 991ms\tremaining: 5.66s\n",
      "149:\tlearn: 0.0286011\ttotal: 997ms\tremaining: 5.65s\n",
      "150:\tlearn: 0.0285802\ttotal: 1.01s\tremaining: 5.69s\n",
      "151:\tlearn: 0.0285714\ttotal: 1.02s\tremaining: 5.68s\n",
      "152:\tlearn: 0.0285493\ttotal: 1.02s\tremaining: 5.67s\n",
      "153:\tlearn: 0.0285478\ttotal: 1.03s\tremaining: 5.66s\n",
      "154:\tlearn: 0.0285464\ttotal: 1.04s\tremaining: 5.66s\n",
      "155:\tlearn: 0.0284530\ttotal: 1.04s\tremaining: 5.66s\n",
      "156:\tlearn: 0.0283432\ttotal: 1.05s\tremaining: 5.65s\n",
      "157:\tlearn: 0.0283160\ttotal: 1.06s\tremaining: 5.64s\n",
      "158:\tlearn: 0.0282582\ttotal: 1.06s\tremaining: 5.63s\n",
      "159:\tlearn: 0.0282482\ttotal: 1.07s\tremaining: 5.62s\n",
      "160:\tlearn: 0.0282486\ttotal: 1.08s\tremaining: 5.61s\n",
      "161:\tlearn: 0.0282483\ttotal: 1.08s\tremaining: 5.6s\n",
      "162:\tlearn: 0.0282473\ttotal: 1.09s\tremaining: 5.58s\n",
      "163:\tlearn: 0.0282470\ttotal: 1.09s\tremaining: 5.57s\n",
      "164:\tlearn: 0.0282347\ttotal: 1.1s\tremaining: 5.57s\n",
      "165:\tlearn: 0.0282332\ttotal: 1.1s\tremaining: 5.55s\n",
      "166:\tlearn: 0.0282167\ttotal: 1.11s\tremaining: 5.54s\n",
      "167:\tlearn: 0.0282148\ttotal: 1.12s\tremaining: 5.53s\n",
      "168:\tlearn: 0.0282075\ttotal: 1.12s\tremaining: 5.52s\n",
      "169:\tlearn: 0.0282024\ttotal: 1.13s\tremaining: 5.51s\n",
      "170:\tlearn: 0.0281741\ttotal: 1.14s\tremaining: 5.51s\n",
      "171:\tlearn: 0.0281692\ttotal: 1.14s\tremaining: 5.5s\n",
      "172:\tlearn: 0.0281691\ttotal: 1.15s\tremaining: 5.49s\n",
      "173:\tlearn: 0.0281664\ttotal: 1.16s\tremaining: 5.49s\n",
      "174:\tlearn: 0.0281592\ttotal: 1.16s\tremaining: 5.49s\n",
      "175:\tlearn: 0.0281591\ttotal: 1.17s\tremaining: 5.48s\n",
      "176:\tlearn: 0.0281586\ttotal: 1.18s\tremaining: 5.47s\n",
      "177:\tlearn: 0.0281581\ttotal: 1.18s\tremaining: 5.46s\n",
      "178:\tlearn: 0.0281572\ttotal: 1.19s\tremaining: 5.46s\n",
      "179:\tlearn: 0.0281550\ttotal: 1.2s\tremaining: 5.47s\n",
      "180:\tlearn: 0.0281545\ttotal: 1.21s\tremaining: 5.48s\n",
      "181:\tlearn: 0.0281534\ttotal: 1.22s\tremaining: 5.47s\n",
      "182:\tlearn: 0.0281511\ttotal: 1.22s\tremaining: 5.45s\n",
      "183:\tlearn: 0.0281515\ttotal: 1.23s\tremaining: 5.45s\n",
      "184:\tlearn: 0.0281513\ttotal: 1.24s\tremaining: 5.44s\n",
      "185:\tlearn: 0.0281485\ttotal: 1.24s\tremaining: 5.43s\n",
      "186:\tlearn: 0.0281375\ttotal: 1.25s\tremaining: 5.42s\n",
      "187:\tlearn: 0.0281314\ttotal: 1.25s\tremaining: 5.41s\n",
      "188:\tlearn: 0.0279962\ttotal: 1.26s\tremaining: 5.41s\n",
      "189:\tlearn: 0.0279934\ttotal: 1.27s\tremaining: 5.41s\n",
      "190:\tlearn: 0.0279934\ttotal: 1.27s\tremaining: 5.4s\n",
      "191:\tlearn: 0.0279930\ttotal: 1.28s\tremaining: 5.39s\n",
      "192:\tlearn: 0.0279921\ttotal: 1.29s\tremaining: 5.38s\n",
      "193:\tlearn: 0.0279920\ttotal: 1.3s\tremaining: 5.39s\n",
      "194:\tlearn: 0.0279242\ttotal: 1.3s\tremaining: 5.39s\n",
      "195:\tlearn: 0.0279242\ttotal: 1.31s\tremaining: 5.38s\n",
      "196:\tlearn: 0.0279226\ttotal: 1.32s\tremaining: 5.36s\n",
      "197:\tlearn: 0.0279198\ttotal: 1.32s\tremaining: 5.35s\n",
      "198:\tlearn: 0.0279156\ttotal: 1.33s\tremaining: 5.34s\n",
      "199:\tlearn: 0.0279113\ttotal: 1.33s\tremaining: 5.34s\n",
      "200:\tlearn: 0.0279104\ttotal: 1.34s\tremaining: 5.33s\n",
      "201:\tlearn: 0.0279063\ttotal: 1.35s\tremaining: 5.32s\n",
      "202:\tlearn: 0.0279011\ttotal: 1.35s\tremaining: 5.31s\n",
      "203:\tlearn: 0.0278997\ttotal: 1.36s\tremaining: 5.3s\n",
      "204:\tlearn: 0.0278969\ttotal: 1.36s\tremaining: 5.29s\n",
      "205:\tlearn: 0.0278966\ttotal: 1.37s\tremaining: 5.29s\n",
      "206:\tlearn: 0.0278948\ttotal: 1.38s\tremaining: 5.27s\n",
      "207:\tlearn: 0.0278946\ttotal: 1.38s\tremaining: 5.26s\n",
      "208:\tlearn: 0.0278943\ttotal: 1.39s\tremaining: 5.25s\n",
      "209:\tlearn: 0.0278936\ttotal: 1.39s\tremaining: 5.24s\n",
      "210:\tlearn: 0.0278906\ttotal: 1.4s\tremaining: 5.23s\n",
      "211:\tlearn: 0.0278897\ttotal: 1.4s\tremaining: 5.22s\n",
      "212:\tlearn: 0.0278893\ttotal: 1.41s\tremaining: 5.21s\n",
      "213:\tlearn: 0.0278892\ttotal: 1.42s\tremaining: 5.2s\n",
      "214:\tlearn: 0.0278846\ttotal: 1.42s\tremaining: 5.19s\n",
      "215:\tlearn: 0.0278056\ttotal: 1.43s\tremaining: 5.19s\n",
      "216:\tlearn: 0.0278051\ttotal: 1.44s\tremaining: 5.2s\n",
      "217:\tlearn: 0.0277454\ttotal: 1.45s\tremaining: 5.19s\n",
      "218:\tlearn: 0.0277287\ttotal: 1.45s\tremaining: 5.18s\n",
      "219:\tlearn: 0.0277285\ttotal: 1.46s\tremaining: 5.18s\n",
      "220:\tlearn: 0.0277286\ttotal: 1.47s\tremaining: 5.18s\n",
      "221:\tlearn: 0.0277017\ttotal: 1.47s\tremaining: 5.17s\n",
      "222:\tlearn: 0.0276436\ttotal: 1.48s\tremaining: 5.16s\n",
      "223:\tlearn: 0.0276231\ttotal: 1.49s\tremaining: 5.15s\n",
      "224:\tlearn: 0.0276136\ttotal: 1.49s\tremaining: 5.14s\n",
      "225:\tlearn: 0.0276134\ttotal: 1.5s\tremaining: 5.13s\n",
      "226:\tlearn: 0.0275584\ttotal: 1.5s\tremaining: 5.13s\n",
      "227:\tlearn: 0.0275584\ttotal: 1.51s\tremaining: 5.12s\n",
      "228:\tlearn: 0.0275551\ttotal: 1.52s\tremaining: 5.11s\n",
      "229:\tlearn: 0.0275542\ttotal: 1.52s\tremaining: 5.11s\n",
      "230:\tlearn: 0.0275429\ttotal: 1.53s\tremaining: 5.1s\n",
      "231:\tlearn: 0.0275430\ttotal: 1.54s\tremaining: 5.08s\n",
      "232:\tlearn: 0.0275401\ttotal: 1.54s\tremaining: 5.08s\n",
      "233:\tlearn: 0.0274921\ttotal: 1.55s\tremaining: 5.06s\n",
      "234:\tlearn: 0.0274923\ttotal: 1.55s\tremaining: 5.05s\n",
      "235:\tlearn: 0.0274863\ttotal: 1.56s\tremaining: 5.04s\n",
      "236:\tlearn: 0.0274860\ttotal: 1.56s\tremaining: 5.04s\n",
      "237:\tlearn: 0.0274864\ttotal: 1.57s\tremaining: 5.03s\n",
      "238:\tlearn: 0.0274866\ttotal: 1.58s\tremaining: 5.02s\n",
      "239:\tlearn: 0.0274868\ttotal: 1.58s\tremaining: 5.01s\n",
      "240:\tlearn: 0.0274852\ttotal: 1.59s\tremaining: 5.01s\n",
      "241:\tlearn: 0.0273979\ttotal: 1.6s\tremaining: 5s\n",
      "242:\tlearn: 0.0273916\ttotal: 1.6s\tremaining: 4.99s\n",
      "243:\tlearn: 0.0273915\ttotal: 1.61s\tremaining: 4.98s\n",
      "244:\tlearn: 0.0273917\ttotal: 1.61s\tremaining: 4.97s\n",
      "245:\tlearn: 0.0273898\ttotal: 1.62s\tremaining: 4.96s\n",
      "246:\tlearn: 0.0273856\ttotal: 1.63s\tremaining: 4.96s\n",
      "247:\tlearn: 0.0273861\ttotal: 1.63s\tremaining: 4.95s\n",
      "248:\tlearn: 0.0273764\ttotal: 1.64s\tremaining: 4.94s\n",
      "249:\tlearn: 0.0273766\ttotal: 1.65s\tremaining: 4.94s\n",
      "250:\tlearn: 0.0273734\ttotal: 1.65s\tremaining: 4.94s\n",
      "251:\tlearn: 0.0273737\ttotal: 1.66s\tremaining: 4.93s\n",
      "252:\tlearn: 0.0272132\ttotal: 1.67s\tremaining: 4.92s\n",
      "253:\tlearn: 0.0272124\ttotal: 1.67s\tremaining: 4.91s\n",
      "254:\tlearn: 0.0272129\ttotal: 1.68s\tremaining: 4.91s\n",
      "255:\tlearn: 0.0272124\ttotal: 1.69s\tremaining: 4.9s\n",
      "256:\tlearn: 0.0271986\ttotal: 1.69s\tremaining: 4.9s\n",
      "257:\tlearn: 0.0271931\ttotal: 1.7s\tremaining: 4.89s\n",
      "258:\tlearn: 0.0271932\ttotal: 1.71s\tremaining: 4.88s\n",
      "259:\tlearn: 0.0271790\ttotal: 1.71s\tremaining: 4.88s\n",
      "260:\tlearn: 0.0271788\ttotal: 1.72s\tremaining: 4.87s\n",
      "261:\tlearn: 0.0271727\ttotal: 1.73s\tremaining: 4.87s\n",
      "262:\tlearn: 0.0271674\ttotal: 1.73s\tremaining: 4.86s\n",
      "263:\tlearn: 0.0271661\ttotal: 1.74s\tremaining: 4.86s\n",
      "264:\tlearn: 0.0271613\ttotal: 1.75s\tremaining: 4.86s\n",
      "265:\tlearn: 0.0271527\ttotal: 1.76s\tremaining: 4.86s\n",
      "266:\tlearn: 0.0271508\ttotal: 1.76s\tremaining: 4.85s\n",
      "267:\tlearn: 0.0271503\ttotal: 1.77s\tremaining: 4.84s\n",
      "268:\tlearn: 0.0271491\ttotal: 1.78s\tremaining: 4.83s\n",
      "269:\tlearn: 0.0271480\ttotal: 1.78s\tremaining: 4.82s\n",
      "270:\tlearn: 0.0271394\ttotal: 1.79s\tremaining: 4.81s\n",
      "271:\tlearn: 0.0271394\ttotal: 1.8s\tremaining: 4.81s\n",
      "272:\tlearn: 0.0271385\ttotal: 1.8s\tremaining: 4.8s\n",
      "273:\tlearn: 0.0271280\ttotal: 1.81s\tremaining: 4.79s\n",
      "274:\tlearn: 0.0271237\ttotal: 1.81s\tremaining: 4.78s\n",
      "275:\tlearn: 0.0271238\ttotal: 1.82s\tremaining: 4.78s\n",
      "276:\tlearn: 0.0271220\ttotal: 1.83s\tremaining: 4.78s\n",
      "277:\tlearn: 0.0271208\ttotal: 1.84s\tremaining: 4.77s\n",
      "278:\tlearn: 0.0271202\ttotal: 1.84s\tremaining: 4.76s\n",
      "279:\tlearn: 0.0271198\ttotal: 1.85s\tremaining: 4.75s\n",
      "280:\tlearn: 0.0271060\ttotal: 1.85s\tremaining: 4.75s\n",
      "281:\tlearn: 0.0271049\ttotal: 1.86s\tremaining: 4.74s\n",
      "282:\tlearn: 0.0271045\ttotal: 1.87s\tremaining: 4.73s\n",
      "283:\tlearn: 0.0270351\ttotal: 1.87s\tremaining: 4.72s\n",
      "284:\tlearn: 0.0270323\ttotal: 1.88s\tremaining: 4.71s\n",
      "285:\tlearn: 0.0270319\ttotal: 1.88s\tremaining: 4.71s\n",
      "286:\tlearn: 0.0270315\ttotal: 1.89s\tremaining: 4.7s\n",
      "287:\tlearn: 0.0270307\ttotal: 1.9s\tremaining: 4.69s\n",
      "288:\tlearn: 0.0270213\ttotal: 1.9s\tremaining: 4.68s\n",
      "289:\tlearn: 0.0270169\ttotal: 1.91s\tremaining: 4.68s\n",
      "290:\tlearn: 0.0270158\ttotal: 1.92s\tremaining: 4.67s\n",
      "291:\tlearn: 0.0270154\ttotal: 1.92s\tremaining: 4.67s\n",
      "292:\tlearn: 0.0270141\ttotal: 1.93s\tremaining: 4.66s\n",
      "293:\tlearn: 0.0270127\ttotal: 1.94s\tremaining: 4.65s\n",
      "294:\tlearn: 0.0270123\ttotal: 1.95s\tremaining: 4.66s\n",
      "295:\tlearn: 0.0270070\ttotal: 1.96s\tremaining: 4.66s\n",
      "296:\tlearn: 0.0270060\ttotal: 1.96s\tremaining: 4.65s\n",
      "297:\tlearn: 0.0270056\ttotal: 1.97s\tremaining: 4.64s\n",
      "298:\tlearn: 0.0270051\ttotal: 1.98s\tremaining: 4.63s\n",
      "299:\tlearn: 0.0270048\ttotal: 1.98s\tremaining: 4.63s\n",
      "300:\tlearn: 0.0270035\ttotal: 1.99s\tremaining: 4.62s\n",
      "301:\tlearn: 0.0270032\ttotal: 2s\tremaining: 4.62s\n",
      "302:\tlearn: 0.0270035\ttotal: 2s\tremaining: 4.61s\n",
      "303:\tlearn: 0.0269485\ttotal: 2.01s\tremaining: 4.61s\n",
      "304:\tlearn: 0.0269324\ttotal: 2.02s\tremaining: 4.6s\n",
      "305:\tlearn: 0.0269320\ttotal: 2.02s\tremaining: 4.59s\n",
      "306:\tlearn: 0.0269319\ttotal: 2.03s\tremaining: 4.58s\n",
      "307:\tlearn: 0.0269316\ttotal: 2.04s\tremaining: 4.58s\n",
      "308:\tlearn: 0.0269313\ttotal: 2.04s\tremaining: 4.57s\n",
      "309:\tlearn: 0.0269273\ttotal: 2.05s\tremaining: 4.56s\n",
      "310:\tlearn: 0.0269275\ttotal: 2.06s\tremaining: 4.56s\n",
      "311:\tlearn: 0.0269272\ttotal: 2.06s\tremaining: 4.55s\n",
      "312:\tlearn: 0.0269268\ttotal: 2.07s\tremaining: 4.54s\n",
      "313:\tlearn: 0.0269264\ttotal: 2.08s\tremaining: 4.54s\n",
      "314:\tlearn: 0.0269263\ttotal: 2.08s\tremaining: 4.53s\n",
      "315:\tlearn: 0.0269248\ttotal: 2.09s\tremaining: 4.52s\n",
      "316:\tlearn: 0.0269244\ttotal: 2.09s\tremaining: 4.51s\n",
      "317:\tlearn: 0.0269238\ttotal: 2.1s\tremaining: 4.5s\n",
      "318:\tlearn: 0.0269212\ttotal: 2.1s\tremaining: 4.49s\n",
      "319:\tlearn: 0.0269214\ttotal: 2.11s\tremaining: 4.49s\n",
      "320:\tlearn: 0.0268740\ttotal: 2.12s\tremaining: 4.48s\n",
      "321:\tlearn: 0.0268684\ttotal: 2.13s\tremaining: 4.48s\n",
      "322:\tlearn: 0.0267608\ttotal: 2.13s\tremaining: 4.47s\n",
      "323:\tlearn: 0.0267599\ttotal: 2.14s\tremaining: 4.46s\n",
      "324:\tlearn: 0.0267597\ttotal: 2.14s\tremaining: 4.45s\n",
      "325:\tlearn: 0.0267565\ttotal: 2.15s\tremaining: 4.46s\n",
      "326:\tlearn: 0.0267495\ttotal: 2.16s\tremaining: 4.45s\n",
      "327:\tlearn: 0.0267471\ttotal: 2.17s\tremaining: 4.44s\n",
      "328:\tlearn: 0.0267044\ttotal: 2.17s\tremaining: 4.43s\n",
      "329:\tlearn: 0.0267041\ttotal: 2.18s\tremaining: 4.42s\n",
      "330:\tlearn: 0.0266745\ttotal: 2.19s\tremaining: 4.42s\n",
      "331:\tlearn: 0.0266736\ttotal: 2.19s\tremaining: 4.41s\n",
      "332:\tlearn: 0.0266695\ttotal: 2.2s\tremaining: 4.4s\n",
      "333:\tlearn: 0.0266634\ttotal: 2.2s\tremaining: 4.39s\n",
      "334:\tlearn: 0.0266630\ttotal: 2.21s\tremaining: 4.38s\n",
      "335:\tlearn: 0.0266239\ttotal: 2.21s\tremaining: 4.38s\n",
      "336:\tlearn: 0.0266238\ttotal: 2.22s\tremaining: 4.37s\n",
      "337:\tlearn: 0.0266237\ttotal: 2.23s\tremaining: 4.36s\n",
      "338:\tlearn: 0.0266068\ttotal: 2.23s\tremaining: 4.36s\n",
      "339:\tlearn: 0.0266030\ttotal: 2.24s\tremaining: 4.35s\n",
      "340:\tlearn: 0.0266003\ttotal: 2.25s\tremaining: 4.34s\n",
      "341:\tlearn: 0.0265975\ttotal: 2.25s\tremaining: 4.33s\n",
      "342:\tlearn: 0.0265974\ttotal: 2.26s\tremaining: 4.33s\n",
      "343:\tlearn: 0.0265966\ttotal: 2.26s\tremaining: 4.32s\n",
      "344:\tlearn: 0.0265969\ttotal: 2.27s\tremaining: 4.31s\n",
      "345:\tlearn: 0.0265917\ttotal: 2.28s\tremaining: 4.3s\n",
      "346:\tlearn: 0.0265915\ttotal: 2.28s\tremaining: 4.29s\n",
      "347:\tlearn: 0.0265915\ttotal: 2.29s\tremaining: 4.3s\n",
      "348:\tlearn: 0.0265915\ttotal: 2.3s\tremaining: 4.29s\n",
      "349:\tlearn: 0.0265812\ttotal: 2.31s\tremaining: 4.29s\n",
      "350:\tlearn: 0.0265808\ttotal: 2.31s\tremaining: 4.28s\n",
      "351:\tlearn: 0.0265713\ttotal: 2.32s\tremaining: 4.27s\n",
      "352:\tlearn: 0.0265710\ttotal: 2.33s\tremaining: 4.26s\n",
      "353:\tlearn: 0.0265707\ttotal: 2.33s\tremaining: 4.25s\n",
      "354:\tlearn: 0.0265705\ttotal: 2.34s\tremaining: 4.25s\n",
      "355:\tlearn: 0.0265705\ttotal: 2.34s\tremaining: 4.24s\n",
      "356:\tlearn: 0.0265706\ttotal: 2.35s\tremaining: 4.24s\n",
      "357:\tlearn: 0.0265679\ttotal: 2.36s\tremaining: 4.23s\n",
      "358:\tlearn: 0.0265651\ttotal: 2.36s\tremaining: 4.22s\n",
      "359:\tlearn: 0.0265650\ttotal: 2.37s\tremaining: 4.21s\n",
      "360:\tlearn: 0.0265648\ttotal: 2.38s\tremaining: 4.2s\n",
      "361:\tlearn: 0.0265544\ttotal: 2.38s\tremaining: 4.2s\n",
      "362:\tlearn: 0.0265494\ttotal: 2.39s\tremaining: 4.19s\n",
      "363:\tlearn: 0.0265492\ttotal: 2.39s\tremaining: 4.18s\n",
      "364:\tlearn: 0.0265486\ttotal: 2.4s\tremaining: 4.17s\n",
      "365:\tlearn: 0.0265329\ttotal: 2.41s\tremaining: 4.17s\n",
      "366:\tlearn: 0.0265329\ttotal: 2.41s\tremaining: 4.16s\n",
      "367:\tlearn: 0.0265294\ttotal: 2.42s\tremaining: 4.15s\n",
      "368:\tlearn: 0.0265293\ttotal: 2.42s\tremaining: 4.14s\n",
      "369:\tlearn: 0.0265249\ttotal: 2.43s\tremaining: 4.13s\n",
      "370:\tlearn: 0.0265234\ttotal: 2.44s\tremaining: 4.13s\n",
      "371:\tlearn: 0.0265215\ttotal: 2.44s\tremaining: 4.12s\n",
      "372:\tlearn: 0.0265217\ttotal: 2.46s\tremaining: 4.13s\n",
      "373:\tlearn: 0.0265056\ttotal: 2.46s\tremaining: 4.12s\n",
      "374:\tlearn: 0.0265055\ttotal: 2.47s\tremaining: 4.12s\n",
      "375:\tlearn: 0.0265047\ttotal: 2.48s\tremaining: 4.11s\n",
      "376:\tlearn: 0.0265018\ttotal: 2.48s\tremaining: 4.1s\n",
      "377:\tlearn: 0.0265015\ttotal: 2.49s\tremaining: 4.09s\n",
      "378:\tlearn: 0.0265011\ttotal: 2.49s\tremaining: 4.08s\n",
      "379:\tlearn: 0.0264999\ttotal: 2.5s\tremaining: 4.08s\n",
      "380:\tlearn: 0.0264982\ttotal: 2.51s\tremaining: 4.08s\n",
      "381:\tlearn: 0.0264979\ttotal: 2.52s\tremaining: 4.07s\n",
      "382:\tlearn: 0.0264981\ttotal: 2.52s\tremaining: 4.06s\n",
      "383:\tlearn: 0.0264982\ttotal: 2.53s\tremaining: 4.05s\n",
      "384:\tlearn: 0.0264981\ttotal: 2.53s\tremaining: 4.04s\n",
      "385:\tlearn: 0.0264976\ttotal: 2.54s\tremaining: 4.04s\n",
      "386:\tlearn: 0.0264968\ttotal: 2.55s\tremaining: 4.03s\n",
      "387:\tlearn: 0.0264960\ttotal: 2.55s\tremaining: 4.03s\n",
      "388:\tlearn: 0.0264953\ttotal: 2.56s\tremaining: 4.02s\n",
      "389:\tlearn: 0.0264494\ttotal: 2.56s\tremaining: 4.01s\n",
      "390:\tlearn: 0.0264466\ttotal: 2.57s\tremaining: 4s\n",
      "391:\tlearn: 0.0264462\ttotal: 2.58s\tremaining: 4s\n",
      "392:\tlearn: 0.0264457\ttotal: 2.58s\tremaining: 3.99s\n",
      "393:\tlearn: 0.0264450\ttotal: 2.59s\tremaining: 3.98s\n",
      "394:\tlearn: 0.0264297\ttotal: 2.6s\tremaining: 3.98s\n",
      "395:\tlearn: 0.0263665\ttotal: 2.6s\tremaining: 3.97s\n",
      "396:\tlearn: 0.0263663\ttotal: 2.61s\tremaining: 3.96s\n",
      "397:\tlearn: 0.0263662\ttotal: 2.62s\tremaining: 3.96s\n",
      "398:\tlearn: 0.0263611\ttotal: 2.62s\tremaining: 3.95s\n",
      "399:\tlearn: 0.0263606\ttotal: 2.63s\tremaining: 3.94s\n",
      "400:\tlearn: 0.0263606\ttotal: 2.63s\tremaining: 3.94s\n",
      "401:\tlearn: 0.0263609\ttotal: 2.64s\tremaining: 3.93s\n",
      "402:\tlearn: 0.0263606\ttotal: 2.65s\tremaining: 3.92s\n",
      "403:\tlearn: 0.0263607\ttotal: 2.65s\tremaining: 3.91s\n",
      "404:\tlearn: 0.0263608\ttotal: 2.66s\tremaining: 3.91s\n",
      "405:\tlearn: 0.0263609\ttotal: 2.66s\tremaining: 3.9s\n",
      "406:\tlearn: 0.0263320\ttotal: 2.67s\tremaining: 3.89s\n",
      "407:\tlearn: 0.0263319\ttotal: 2.68s\tremaining: 3.89s\n",
      "408:\tlearn: 0.0263319\ttotal: 2.68s\tremaining: 3.88s\n",
      "409:\tlearn: 0.0263319\ttotal: 2.69s\tremaining: 3.87s\n",
      "410:\tlearn: 0.0263076\ttotal: 2.7s\tremaining: 3.87s\n",
      "411:\tlearn: 0.0263075\ttotal: 2.71s\tremaining: 3.86s\n",
      "412:\tlearn: 0.0263072\ttotal: 2.71s\tremaining: 3.85s\n",
      "413:\tlearn: 0.0263062\ttotal: 2.72s\tremaining: 3.85s\n",
      "414:\tlearn: 0.0263061\ttotal: 2.72s\tremaining: 3.84s\n",
      "415:\tlearn: 0.0263062\ttotal: 2.73s\tremaining: 3.83s\n",
      "416:\tlearn: 0.0263042\ttotal: 2.73s\tremaining: 3.82s\n",
      "417:\tlearn: 0.0263048\ttotal: 2.74s\tremaining: 3.82s\n",
      "418:\tlearn: 0.0262598\ttotal: 2.75s\tremaining: 3.81s\n",
      "419:\tlearn: 0.0262598\ttotal: 2.75s\tremaining: 3.8s\n",
      "420:\tlearn: 0.0262448\ttotal: 2.76s\tremaining: 3.79s\n",
      "421:\tlearn: 0.0262442\ttotal: 2.76s\tremaining: 3.79s\n",
      "422:\tlearn: 0.0262437\ttotal: 2.77s\tremaining: 3.78s\n",
      "423:\tlearn: 0.0262428\ttotal: 2.78s\tremaining: 3.77s\n",
      "424:\tlearn: 0.0262415\ttotal: 2.78s\tremaining: 3.77s\n",
      "425:\tlearn: 0.0262406\ttotal: 2.79s\tremaining: 3.76s\n",
      "426:\tlearn: 0.0262406\ttotal: 2.8s\tremaining: 3.75s\n",
      "427:\tlearn: 0.0262406\ttotal: 2.8s\tremaining: 3.75s\n",
      "428:\tlearn: 0.0262304\ttotal: 2.81s\tremaining: 3.74s\n",
      "429:\tlearn: 0.0262150\ttotal: 2.81s\tremaining: 3.73s\n",
      "430:\tlearn: 0.0262143\ttotal: 2.82s\tremaining: 3.73s\n",
      "431:\tlearn: 0.0262143\ttotal: 2.83s\tremaining: 3.72s\n",
      "432:\tlearn: 0.0262010\ttotal: 2.83s\tremaining: 3.71s\n",
      "433:\tlearn: 0.0262011\ttotal: 2.84s\tremaining: 3.71s\n",
      "434:\tlearn: 0.0262008\ttotal: 2.85s\tremaining: 3.7s\n",
      "435:\tlearn: 0.0261998\ttotal: 2.85s\tremaining: 3.69s\n",
      "436:\tlearn: 0.0261998\ttotal: 2.86s\tremaining: 3.68s\n",
      "437:\tlearn: 0.0261997\ttotal: 2.87s\tremaining: 3.68s\n",
      "438:\tlearn: 0.0261997\ttotal: 2.87s\tremaining: 3.67s\n",
      "439:\tlearn: 0.0261980\ttotal: 2.88s\tremaining: 3.66s\n",
      "440:\tlearn: 0.0261976\ttotal: 2.88s\tremaining: 3.65s\n",
      "441:\tlearn: 0.0261851\ttotal: 2.89s\tremaining: 3.65s\n",
      "442:\tlearn: 0.0261851\ttotal: 2.9s\tremaining: 3.64s\n",
      "443:\tlearn: 0.0261539\ttotal: 2.9s\tremaining: 3.63s\n",
      "444:\tlearn: 0.0261500\ttotal: 2.91s\tremaining: 3.63s\n",
      "445:\tlearn: 0.0261153\ttotal: 2.91s\tremaining: 3.62s\n",
      "446:\tlearn: 0.0261136\ttotal: 2.92s\tremaining: 3.61s\n",
      "447:\tlearn: 0.0261133\ttotal: 2.92s\tremaining: 3.6s\n",
      "448:\tlearn: 0.0261131\ttotal: 2.93s\tremaining: 3.6s\n",
      "449:\tlearn: 0.0260786\ttotal: 2.94s\tremaining: 3.59s\n",
      "450:\tlearn: 0.0260786\ttotal: 2.94s\tremaining: 3.58s\n",
      "451:\tlearn: 0.0260786\ttotal: 2.95s\tremaining: 3.58s\n",
      "452:\tlearn: 0.0260786\ttotal: 2.96s\tremaining: 3.57s\n",
      "453:\tlearn: 0.0260618\ttotal: 2.97s\tremaining: 3.57s\n",
      "454:\tlearn: 0.0260607\ttotal: 2.98s\tremaining: 3.56s\n",
      "455:\tlearn: 0.0260610\ttotal: 2.98s\tremaining: 3.56s\n",
      "456:\tlearn: 0.0260605\ttotal: 3s\tremaining: 3.56s\n",
      "457:\tlearn: 0.0260603\ttotal: 3s\tremaining: 3.56s\n",
      "458:\tlearn: 0.0260604\ttotal: 3.01s\tremaining: 3.55s\n",
      "459:\tlearn: 0.0260602\ttotal: 3.02s\tremaining: 3.54s\n",
      "460:\tlearn: 0.0260587\ttotal: 3.03s\tremaining: 3.54s\n",
      "461:\tlearn: 0.0260572\ttotal: 3.04s\tremaining: 3.54s\n",
      "462:\tlearn: 0.0260220\ttotal: 3.04s\tremaining: 3.53s\n",
      "463:\tlearn: 0.0260221\ttotal: 3.05s\tremaining: 3.52s\n",
      "464:\tlearn: 0.0260208\ttotal: 3.06s\tremaining: 3.52s\n",
      "465:\tlearn: 0.0260207\ttotal: 3.06s\tremaining: 3.51s\n",
      "466:\tlearn: 0.0260192\ttotal: 3.07s\tremaining: 3.5s\n",
      "467:\tlearn: 0.0260180\ttotal: 3.08s\tremaining: 3.5s\n",
      "468:\tlearn: 0.0260179\ttotal: 3.08s\tremaining: 3.49s\n",
      "469:\tlearn: 0.0260169\ttotal: 3.09s\tremaining: 3.48s\n",
      "470:\tlearn: 0.0260161\ttotal: 3.1s\tremaining: 3.48s\n",
      "471:\tlearn: 0.0260161\ttotal: 3.1s\tremaining: 3.47s\n",
      "472:\tlearn: 0.0260158\ttotal: 3.11s\tremaining: 3.46s\n",
      "473:\tlearn: 0.0259921\ttotal: 3.12s\tremaining: 3.46s\n",
      "474:\tlearn: 0.0259915\ttotal: 3.12s\tremaining: 3.45s\n",
      "475:\tlearn: 0.0259914\ttotal: 3.13s\tremaining: 3.45s\n",
      "476:\tlearn: 0.0259915\ttotal: 3.14s\tremaining: 3.44s\n",
      "477:\tlearn: 0.0259911\ttotal: 3.15s\tremaining: 3.44s\n",
      "478:\tlearn: 0.0259878\ttotal: 3.16s\tremaining: 3.44s\n",
      "479:\tlearn: 0.0259416\ttotal: 3.17s\tremaining: 3.43s\n",
      "480:\tlearn: 0.0259416\ttotal: 3.17s\tremaining: 3.42s\n",
      "481:\tlearn: 0.0259418\ttotal: 3.18s\tremaining: 3.42s\n",
      "482:\tlearn: 0.0259404\ttotal: 3.18s\tremaining: 3.41s\n",
      "483:\tlearn: 0.0259398\ttotal: 3.19s\tremaining: 3.4s\n",
      "484:\tlearn: 0.0258538\ttotal: 3.19s\tremaining: 3.39s\n",
      "485:\tlearn: 0.0258531\ttotal: 3.2s\tremaining: 3.38s\n",
      "486:\tlearn: 0.0258531\ttotal: 3.21s\tremaining: 3.38s\n",
      "487:\tlearn: 0.0258473\ttotal: 3.21s\tremaining: 3.37s\n",
      "488:\tlearn: 0.0258473\ttotal: 3.22s\tremaining: 3.36s\n",
      "489:\tlearn: 0.0258362\ttotal: 3.22s\tremaining: 3.35s\n",
      "490:\tlearn: 0.0258360\ttotal: 3.23s\tremaining: 3.35s\n",
      "491:\tlearn: 0.0258339\ttotal: 3.24s\tremaining: 3.35s\n",
      "492:\tlearn: 0.0258334\ttotal: 3.25s\tremaining: 3.34s\n",
      "493:\tlearn: 0.0258320\ttotal: 3.25s\tremaining: 3.33s\n",
      "494:\tlearn: 0.0258319\ttotal: 3.26s\tremaining: 3.33s\n",
      "495:\tlearn: 0.0258314\ttotal: 3.27s\tremaining: 3.33s\n",
      "496:\tlearn: 0.0258259\ttotal: 3.28s\tremaining: 3.32s\n",
      "497:\tlearn: 0.0258249\ttotal: 3.28s\tremaining: 3.31s\n",
      "498:\tlearn: 0.0258248\ttotal: 3.29s\tremaining: 3.3s\n",
      "499:\tlearn: 0.0258192\ttotal: 3.3s\tremaining: 3.3s\n",
      "500:\tlearn: 0.0258186\ttotal: 3.3s\tremaining: 3.29s\n",
      "501:\tlearn: 0.0257826\ttotal: 3.31s\tremaining: 3.28s\n",
      "502:\tlearn: 0.0257825\ttotal: 3.31s\tremaining: 3.27s\n",
      "503:\tlearn: 0.0257707\ttotal: 3.32s\tremaining: 3.27s\n",
      "504:\tlearn: 0.0257654\ttotal: 3.33s\tremaining: 3.26s\n",
      "505:\tlearn: 0.0257646\ttotal: 3.33s\tremaining: 3.25s\n",
      "506:\tlearn: 0.0257638\ttotal: 3.34s\tremaining: 3.25s\n",
      "507:\tlearn: 0.0257634\ttotal: 3.34s\tremaining: 3.24s\n",
      "508:\tlearn: 0.0257344\ttotal: 3.35s\tremaining: 3.23s\n",
      "509:\tlearn: 0.0257324\ttotal: 3.36s\tremaining: 3.23s\n",
      "510:\tlearn: 0.0257316\ttotal: 3.37s\tremaining: 3.22s\n",
      "511:\tlearn: 0.0257308\ttotal: 3.37s\tremaining: 3.21s\n",
      "512:\tlearn: 0.0257281\ttotal: 3.38s\tremaining: 3.21s\n",
      "513:\tlearn: 0.0257257\ttotal: 3.38s\tremaining: 3.2s\n",
      "514:\tlearn: 0.0257248\ttotal: 3.39s\tremaining: 3.19s\n",
      "515:\tlearn: 0.0257239\ttotal: 3.4s\tremaining: 3.19s\n",
      "516:\tlearn: 0.0257214\ttotal: 3.4s\tremaining: 3.18s\n",
      "517:\tlearn: 0.0257198\ttotal: 3.41s\tremaining: 3.17s\n",
      "518:\tlearn: 0.0257191\ttotal: 3.42s\tremaining: 3.17s\n",
      "519:\tlearn: 0.0256966\ttotal: 3.43s\tremaining: 3.16s\n",
      "520:\tlearn: 0.0256966\ttotal: 3.44s\tremaining: 3.16s\n",
      "521:\tlearn: 0.0256965\ttotal: 3.44s\tremaining: 3.15s\n",
      "522:\tlearn: 0.0256969\ttotal: 3.45s\tremaining: 3.15s\n",
      "523:\tlearn: 0.0256962\ttotal: 3.46s\tremaining: 3.15s\n",
      "524:\tlearn: 0.0256960\ttotal: 3.47s\tremaining: 3.14s\n",
      "525:\tlearn: 0.0256962\ttotal: 3.47s\tremaining: 3.13s\n",
      "526:\tlearn: 0.0256960\ttotal: 3.48s\tremaining: 3.12s\n",
      "527:\tlearn: 0.0256953\ttotal: 3.49s\tremaining: 3.12s\n",
      "528:\tlearn: 0.0256954\ttotal: 3.49s\tremaining: 3.11s\n",
      "529:\tlearn: 0.0256922\ttotal: 3.5s\tremaining: 3.1s\n",
      "530:\tlearn: 0.0256901\ttotal: 3.5s\tremaining: 3.1s\n",
      "531:\tlearn: 0.0256900\ttotal: 3.51s\tremaining: 3.09s\n",
      "532:\tlearn: 0.0256872\ttotal: 3.52s\tremaining: 3.08s\n",
      "533:\tlearn: 0.0256873\ttotal: 3.52s\tremaining: 3.08s\n",
      "534:\tlearn: 0.0256867\ttotal: 3.53s\tremaining: 3.07s\n",
      "535:\tlearn: 0.0256865\ttotal: 3.54s\tremaining: 3.06s\n",
      "536:\tlearn: 0.0256735\ttotal: 3.54s\tremaining: 3.05s\n",
      "537:\tlearn: 0.0256501\ttotal: 3.55s\tremaining: 3.05s\n",
      "538:\tlearn: 0.0256330\ttotal: 3.56s\tremaining: 3.04s\n",
      "539:\tlearn: 0.0256318\ttotal: 3.56s\tremaining: 3.03s\n",
      "540:\tlearn: 0.0256288\ttotal: 3.57s\tremaining: 3.03s\n",
      "541:\tlearn: 0.0256163\ttotal: 3.57s\tremaining: 3.02s\n",
      "542:\tlearn: 0.0256159\ttotal: 3.58s\tremaining: 3.01s\n",
      "543:\tlearn: 0.0256157\ttotal: 3.59s\tremaining: 3.01s\n",
      "544:\tlearn: 0.0255984\ttotal: 3.59s\tremaining: 3s\n",
      "545:\tlearn: 0.0255978\ttotal: 3.6s\tremaining: 2.99s\n",
      "546:\tlearn: 0.0255958\ttotal: 3.61s\tremaining: 2.99s\n",
      "547:\tlearn: 0.0255955\ttotal: 3.61s\tremaining: 2.98s\n",
      "548:\tlearn: 0.0255952\ttotal: 3.62s\tremaining: 2.97s\n",
      "549:\tlearn: 0.0255934\ttotal: 3.63s\tremaining: 2.97s\n",
      "550:\tlearn: 0.0255933\ttotal: 3.63s\tremaining: 2.96s\n",
      "551:\tlearn: 0.0255933\ttotal: 3.64s\tremaining: 2.95s\n",
      "552:\tlearn: 0.0255924\ttotal: 3.65s\tremaining: 2.95s\n",
      "553:\tlearn: 0.0255924\ttotal: 3.65s\tremaining: 2.94s\n",
      "554:\tlearn: 0.0255901\ttotal: 3.66s\tremaining: 2.93s\n",
      "555:\tlearn: 0.0255897\ttotal: 3.67s\tremaining: 2.93s\n",
      "556:\tlearn: 0.0255895\ttotal: 3.67s\tremaining: 2.92s\n",
      "557:\tlearn: 0.0255891\ttotal: 3.68s\tremaining: 2.91s\n",
      "558:\tlearn: 0.0255161\ttotal: 3.69s\tremaining: 2.91s\n",
      "559:\tlearn: 0.0254932\ttotal: 3.69s\tremaining: 2.9s\n",
      "560:\tlearn: 0.0254920\ttotal: 3.7s\tremaining: 2.89s\n",
      "561:\tlearn: 0.0254919\ttotal: 3.7s\tremaining: 2.88s\n",
      "562:\tlearn: 0.0254915\ttotal: 3.71s\tremaining: 2.88s\n",
      "563:\tlearn: 0.0254856\ttotal: 3.72s\tremaining: 2.87s\n",
      "564:\tlearn: 0.0254856\ttotal: 3.72s\tremaining: 2.87s\n",
      "565:\tlearn: 0.0254856\ttotal: 3.73s\tremaining: 2.86s\n",
      "566:\tlearn: 0.0254853\ttotal: 3.73s\tremaining: 2.85s\n",
      "567:\tlearn: 0.0254845\ttotal: 3.74s\tremaining: 2.84s\n",
      "568:\tlearn: 0.0254742\ttotal: 3.75s\tremaining: 2.84s\n",
      "569:\tlearn: 0.0254737\ttotal: 3.75s\tremaining: 2.83s\n",
      "570:\tlearn: 0.0254734\ttotal: 3.76s\tremaining: 2.82s\n",
      "571:\tlearn: 0.0254638\ttotal: 3.76s\tremaining: 2.82s\n",
      "572:\tlearn: 0.0254635\ttotal: 3.77s\tremaining: 2.81s\n",
      "573:\tlearn: 0.0254634\ttotal: 3.78s\tremaining: 2.8s\n",
      "574:\tlearn: 0.0254628\ttotal: 3.78s\tremaining: 2.8s\n",
      "575:\tlearn: 0.0254627\ttotal: 3.79s\tremaining: 2.79s\n",
      "576:\tlearn: 0.0254626\ttotal: 3.8s\tremaining: 2.78s\n",
      "577:\tlearn: 0.0254624\ttotal: 3.8s\tremaining: 2.78s\n",
      "578:\tlearn: 0.0254610\ttotal: 3.81s\tremaining: 2.77s\n",
      "579:\tlearn: 0.0254162\ttotal: 3.81s\tremaining: 2.76s\n",
      "580:\tlearn: 0.0254106\ttotal: 3.82s\tremaining: 2.76s\n",
      "581:\tlearn: 0.0254033\ttotal: 3.83s\tremaining: 2.75s\n",
      "582:\tlearn: 0.0253826\ttotal: 3.83s\tremaining: 2.74s\n",
      "583:\tlearn: 0.0253720\ttotal: 3.84s\tremaining: 2.73s\n",
      "584:\tlearn: 0.0253716\ttotal: 3.85s\tremaining: 2.73s\n",
      "585:\tlearn: 0.0253716\ttotal: 3.85s\tremaining: 2.72s\n",
      "586:\tlearn: 0.0253704\ttotal: 3.86s\tremaining: 2.72s\n",
      "587:\tlearn: 0.0253705\ttotal: 3.87s\tremaining: 2.71s\n",
      "588:\tlearn: 0.0253694\ttotal: 3.87s\tremaining: 2.7s\n",
      "589:\tlearn: 0.0253694\ttotal: 3.88s\tremaining: 2.7s\n",
      "590:\tlearn: 0.0253681\ttotal: 3.89s\tremaining: 2.69s\n",
      "591:\tlearn: 0.0253676\ttotal: 3.9s\tremaining: 2.69s\n",
      "592:\tlearn: 0.0253674\ttotal: 3.9s\tremaining: 2.68s\n",
      "593:\tlearn: 0.0253666\ttotal: 3.91s\tremaining: 2.67s\n",
      "594:\tlearn: 0.0253667\ttotal: 3.92s\tremaining: 2.67s\n",
      "595:\tlearn: 0.0253630\ttotal: 3.92s\tremaining: 2.66s\n",
      "596:\tlearn: 0.0253631\ttotal: 3.93s\tremaining: 2.65s\n",
      "597:\tlearn: 0.0253629\ttotal: 3.93s\tremaining: 2.65s\n",
      "598:\tlearn: 0.0253609\ttotal: 3.95s\tremaining: 2.64s\n",
      "599:\tlearn: 0.0253602\ttotal: 3.95s\tremaining: 2.64s\n",
      "600:\tlearn: 0.0253602\ttotal: 3.96s\tremaining: 2.63s\n",
      "601:\tlearn: 0.0253559\ttotal: 3.96s\tremaining: 2.62s\n",
      "602:\tlearn: 0.0253549\ttotal: 3.97s\tremaining: 2.62s\n",
      "603:\tlearn: 0.0253551\ttotal: 3.98s\tremaining: 2.61s\n",
      "604:\tlearn: 0.0253549\ttotal: 3.99s\tremaining: 2.6s\n",
      "605:\tlearn: 0.0253550\ttotal: 4s\tremaining: 2.6s\n",
      "606:\tlearn: 0.0253553\ttotal: 4s\tremaining: 2.59s\n",
      "607:\tlearn: 0.0253553\ttotal: 4.01s\tremaining: 2.58s\n",
      "608:\tlearn: 0.0253536\ttotal: 4.02s\tremaining: 2.58s\n",
      "609:\tlearn: 0.0253538\ttotal: 4.02s\tremaining: 2.57s\n",
      "610:\tlearn: 0.0253516\ttotal: 4.03s\tremaining: 2.56s\n",
      "611:\tlearn: 0.0253481\ttotal: 4.03s\tremaining: 2.56s\n",
      "612:\tlearn: 0.0253197\ttotal: 4.04s\tremaining: 2.55s\n",
      "613:\tlearn: 0.0253190\ttotal: 4.05s\tremaining: 2.54s\n",
      "614:\tlearn: 0.0253184\ttotal: 4.05s\tremaining: 2.54s\n",
      "615:\tlearn: 0.0253184\ttotal: 4.06s\tremaining: 2.53s\n",
      "616:\tlearn: 0.0253177\ttotal: 4.07s\tremaining: 2.52s\n",
      "617:\tlearn: 0.0253174\ttotal: 4.08s\tremaining: 2.52s\n",
      "618:\tlearn: 0.0253155\ttotal: 4.08s\tremaining: 2.51s\n",
      "619:\tlearn: 0.0253157\ttotal: 4.09s\tremaining: 2.5s\n",
      "620:\tlearn: 0.0253143\ttotal: 4.09s\tremaining: 2.5s\n",
      "621:\tlearn: 0.0253145\ttotal: 4.1s\tremaining: 2.49s\n",
      "622:\tlearn: 0.0253147\ttotal: 4.11s\tremaining: 2.48s\n",
      "623:\tlearn: 0.0253103\ttotal: 4.11s\tremaining: 2.48s\n",
      "624:\tlearn: 0.0253094\ttotal: 4.12s\tremaining: 2.47s\n",
      "625:\tlearn: 0.0253078\ttotal: 4.13s\tremaining: 2.46s\n",
      "626:\tlearn: 0.0253070\ttotal: 4.13s\tremaining: 2.46s\n",
      "627:\tlearn: 0.0253071\ttotal: 4.14s\tremaining: 2.45s\n",
      "628:\tlearn: 0.0253070\ttotal: 4.14s\tremaining: 2.44s\n",
      "629:\tlearn: 0.0253026\ttotal: 4.15s\tremaining: 2.44s\n",
      "630:\tlearn: 0.0253010\ttotal: 4.15s\tremaining: 2.43s\n",
      "631:\tlearn: 0.0253005\ttotal: 4.16s\tremaining: 2.42s\n",
      "632:\tlearn: 0.0252976\ttotal: 4.17s\tremaining: 2.42s\n",
      "633:\tlearn: 0.0252973\ttotal: 4.17s\tremaining: 2.41s\n",
      "634:\tlearn: 0.0252965\ttotal: 4.18s\tremaining: 2.4s\n",
      "635:\tlearn: 0.0252964\ttotal: 4.19s\tremaining: 2.4s\n",
      "636:\tlearn: 0.0252964\ttotal: 4.19s\tremaining: 2.39s\n",
      "637:\tlearn: 0.0252961\ttotal: 4.2s\tremaining: 2.38s\n",
      "638:\tlearn: 0.0252958\ttotal: 4.21s\tremaining: 2.38s\n",
      "639:\tlearn: 0.0252954\ttotal: 4.21s\tremaining: 2.37s\n",
      "640:\tlearn: 0.0252954\ttotal: 4.22s\tremaining: 2.36s\n",
      "641:\tlearn: 0.0252954\ttotal: 4.23s\tremaining: 2.36s\n",
      "642:\tlearn: 0.0252948\ttotal: 4.24s\tremaining: 2.35s\n",
      "643:\tlearn: 0.0252942\ttotal: 4.24s\tremaining: 2.35s\n",
      "644:\tlearn: 0.0252930\ttotal: 4.25s\tremaining: 2.34s\n",
      "645:\tlearn: 0.0252925\ttotal: 4.26s\tremaining: 2.33s\n",
      "646:\tlearn: 0.0252916\ttotal: 4.26s\tremaining: 2.33s\n",
      "647:\tlearn: 0.0252914\ttotal: 4.27s\tremaining: 2.32s\n",
      "648:\tlearn: 0.0252912\ttotal: 4.28s\tremaining: 2.31s\n",
      "649:\tlearn: 0.0252908\ttotal: 4.28s\tremaining: 2.31s\n",
      "650:\tlearn: 0.0252888\ttotal: 4.29s\tremaining: 2.3s\n",
      "651:\tlearn: 0.0252804\ttotal: 4.29s\tremaining: 2.29s\n",
      "652:\tlearn: 0.0252805\ttotal: 4.3s\tremaining: 2.29s\n",
      "653:\tlearn: 0.0252789\ttotal: 4.31s\tremaining: 2.28s\n",
      "654:\tlearn: 0.0252781\ttotal: 4.32s\tremaining: 2.27s\n",
      "655:\tlearn: 0.0252781\ttotal: 4.32s\tremaining: 2.27s\n",
      "656:\tlearn: 0.0252781\ttotal: 4.33s\tremaining: 2.26s\n",
      "657:\tlearn: 0.0252685\ttotal: 4.33s\tremaining: 2.25s\n",
      "658:\tlearn: 0.0252691\ttotal: 4.34s\tremaining: 2.25s\n",
      "659:\tlearn: 0.0252690\ttotal: 4.34s\tremaining: 2.24s\n",
      "660:\tlearn: 0.0252689\ttotal: 4.35s\tremaining: 2.23s\n",
      "661:\tlearn: 0.0252688\ttotal: 4.36s\tremaining: 2.22s\n",
      "662:\tlearn: 0.0252668\ttotal: 4.37s\tremaining: 2.22s\n",
      "663:\tlearn: 0.0252658\ttotal: 4.38s\tremaining: 2.21s\n",
      "664:\tlearn: 0.0252658\ttotal: 4.38s\tremaining: 2.21s\n",
      "665:\tlearn: 0.0252657\ttotal: 4.39s\tremaining: 2.2s\n",
      "666:\tlearn: 0.0252646\ttotal: 4.39s\tremaining: 2.19s\n",
      "667:\tlearn: 0.0252644\ttotal: 4.4s\tremaining: 2.19s\n",
      "668:\tlearn: 0.0252644\ttotal: 4.41s\tremaining: 2.18s\n",
      "669:\tlearn: 0.0252630\ttotal: 4.41s\tremaining: 2.17s\n",
      "670:\tlearn: 0.0252611\ttotal: 4.42s\tremaining: 2.17s\n",
      "671:\tlearn: 0.0252593\ttotal: 4.42s\tremaining: 2.16s\n",
      "672:\tlearn: 0.0252592\ttotal: 4.43s\tremaining: 2.15s\n",
      "673:\tlearn: 0.0252585\ttotal: 4.44s\tremaining: 2.15s\n",
      "674:\tlearn: 0.0252583\ttotal: 4.44s\tremaining: 2.14s\n",
      "675:\tlearn: 0.0252584\ttotal: 4.45s\tremaining: 2.13s\n",
      "676:\tlearn: 0.0252583\ttotal: 4.46s\tremaining: 2.13s\n",
      "677:\tlearn: 0.0252579\ttotal: 4.46s\tremaining: 2.12s\n",
      "678:\tlearn: 0.0252577\ttotal: 4.47s\tremaining: 2.11s\n",
      "679:\tlearn: 0.0252533\ttotal: 4.48s\tremaining: 2.11s\n",
      "680:\tlearn: 0.0252532\ttotal: 4.48s\tremaining: 2.1s\n",
      "681:\tlearn: 0.0252533\ttotal: 4.49s\tremaining: 2.09s\n",
      "682:\tlearn: 0.0252507\ttotal: 4.5s\tremaining: 2.09s\n",
      "683:\tlearn: 0.0252507\ttotal: 4.5s\tremaining: 2.08s\n",
      "684:\tlearn: 0.0252508\ttotal: 4.51s\tremaining: 2.07s\n",
      "685:\tlearn: 0.0252508\ttotal: 4.51s\tremaining: 2.07s\n",
      "686:\tlearn: 0.0252502\ttotal: 4.52s\tremaining: 2.06s\n",
      "687:\tlearn: 0.0252499\ttotal: 4.53s\tremaining: 2.05s\n",
      "688:\tlearn: 0.0252448\ttotal: 4.53s\tremaining: 2.04s\n",
      "689:\tlearn: 0.0252136\ttotal: 4.54s\tremaining: 2.04s\n",
      "690:\tlearn: 0.0252110\ttotal: 4.54s\tremaining: 2.03s\n",
      "691:\tlearn: 0.0252108\ttotal: 4.55s\tremaining: 2.02s\n",
      "692:\tlearn: 0.0252094\ttotal: 4.55s\tremaining: 2.02s\n",
      "693:\tlearn: 0.0252085\ttotal: 4.56s\tremaining: 2.01s\n",
      "694:\tlearn: 0.0252032\ttotal: 4.57s\tremaining: 2s\n",
      "695:\tlearn: 0.0252032\ttotal: 4.57s\tremaining: 2s\n",
      "696:\tlearn: 0.0252031\ttotal: 4.58s\tremaining: 1.99s\n",
      "697:\tlearn: 0.0252020\ttotal: 4.58s\tremaining: 1.98s\n",
      "698:\tlearn: 0.0252020\ttotal: 4.59s\tremaining: 1.98s\n",
      "699:\tlearn: 0.0252002\ttotal: 4.6s\tremaining: 1.97s\n",
      "700:\tlearn: 0.0252001\ttotal: 4.61s\tremaining: 1.97s\n",
      "701:\tlearn: 0.0252001\ttotal: 4.61s\tremaining: 1.96s\n",
      "702:\tlearn: 0.0251995\ttotal: 4.62s\tremaining: 1.95s\n",
      "703:\tlearn: 0.0251996\ttotal: 4.63s\tremaining: 1.94s\n",
      "704:\tlearn: 0.0251996\ttotal: 4.63s\tremaining: 1.94s\n",
      "705:\tlearn: 0.0251542\ttotal: 4.64s\tremaining: 1.93s\n",
      "706:\tlearn: 0.0251488\ttotal: 4.64s\tremaining: 1.92s\n",
      "707:\tlearn: 0.0251489\ttotal: 4.65s\tremaining: 1.92s\n",
      "708:\tlearn: 0.0251490\ttotal: 4.66s\tremaining: 1.91s\n",
      "709:\tlearn: 0.0251486\ttotal: 4.66s\tremaining: 1.9s\n",
      "710:\tlearn: 0.0251470\ttotal: 4.67s\tremaining: 1.9s\n",
      "711:\tlearn: 0.0251445\ttotal: 4.68s\tremaining: 1.89s\n",
      "712:\tlearn: 0.0251444\ttotal: 4.68s\tremaining: 1.88s\n",
      "713:\tlearn: 0.0251158\ttotal: 4.69s\tremaining: 1.88s\n",
      "714:\tlearn: 0.0251155\ttotal: 4.69s\tremaining: 1.87s\n",
      "715:\tlearn: 0.0251137\ttotal: 4.7s\tremaining: 1.86s\n",
      "716:\tlearn: 0.0250942\ttotal: 4.71s\tremaining: 1.86s\n",
      "717:\tlearn: 0.0250942\ttotal: 4.71s\tremaining: 1.85s\n",
      "718:\tlearn: 0.0250940\ttotal: 4.72s\tremaining: 1.85s\n",
      "719:\tlearn: 0.0250808\ttotal: 4.74s\tremaining: 1.84s\n",
      "720:\tlearn: 0.0250805\ttotal: 4.74s\tremaining: 1.83s\n",
      "721:\tlearn: 0.0250804\ttotal: 4.75s\tremaining: 1.83s\n",
      "722:\tlearn: 0.0250805\ttotal: 4.75s\tremaining: 1.82s\n",
      "723:\tlearn: 0.0250806\ttotal: 4.76s\tremaining: 1.81s\n",
      "724:\tlearn: 0.0250806\ttotal: 4.77s\tremaining: 1.81s\n",
      "725:\tlearn: 0.0250805\ttotal: 4.78s\tremaining: 1.8s\n",
      "726:\tlearn: 0.0250804\ttotal: 4.78s\tremaining: 1.79s\n",
      "727:\tlearn: 0.0250802\ttotal: 4.79s\tremaining: 1.79s\n",
      "728:\tlearn: 0.0250799\ttotal: 4.8s\tremaining: 1.78s\n",
      "729:\tlearn: 0.0250725\ttotal: 4.8s\tremaining: 1.78s\n",
      "730:\tlearn: 0.0250710\ttotal: 4.81s\tremaining: 1.77s\n",
      "731:\tlearn: 0.0250704\ttotal: 4.82s\tremaining: 1.76s\n",
      "732:\tlearn: 0.0250703\ttotal: 4.82s\tremaining: 1.76s\n",
      "733:\tlearn: 0.0250658\ttotal: 4.83s\tremaining: 1.75s\n",
      "734:\tlearn: 0.0250657\ttotal: 4.83s\tremaining: 1.74s\n",
      "735:\tlearn: 0.0250610\ttotal: 4.84s\tremaining: 1.74s\n",
      "736:\tlearn: 0.0250607\ttotal: 4.85s\tremaining: 1.73s\n",
      "737:\tlearn: 0.0250606\ttotal: 4.86s\tremaining: 1.73s\n",
      "738:\tlearn: 0.0250604\ttotal: 4.87s\tremaining: 1.72s\n",
      "739:\tlearn: 0.0250530\ttotal: 4.87s\tremaining: 1.71s\n",
      "740:\tlearn: 0.0250432\ttotal: 4.88s\tremaining: 1.71s\n",
      "741:\tlearn: 0.0250429\ttotal: 4.89s\tremaining: 1.7s\n",
      "742:\tlearn: 0.0250187\ttotal: 4.89s\tremaining: 1.69s\n",
      "743:\tlearn: 0.0250186\ttotal: 4.9s\tremaining: 1.69s\n",
      "744:\tlearn: 0.0250185\ttotal: 4.91s\tremaining: 1.68s\n",
      "745:\tlearn: 0.0250173\ttotal: 4.91s\tremaining: 1.67s\n",
      "746:\tlearn: 0.0250170\ttotal: 4.92s\tremaining: 1.67s\n",
      "747:\tlearn: 0.0250170\ttotal: 4.92s\tremaining: 1.66s\n",
      "748:\tlearn: 0.0250163\ttotal: 4.94s\tremaining: 1.66s\n",
      "749:\tlearn: 0.0250160\ttotal: 4.95s\tremaining: 1.65s\n",
      "750:\tlearn: 0.0250148\ttotal: 4.95s\tremaining: 1.64s\n",
      "751:\tlearn: 0.0250148\ttotal: 4.96s\tremaining: 1.64s\n",
      "752:\tlearn: 0.0250148\ttotal: 4.97s\tremaining: 1.63s\n",
      "753:\tlearn: 0.0250145\ttotal: 4.97s\tremaining: 1.62s\n",
      "754:\tlearn: 0.0250146\ttotal: 4.98s\tremaining: 1.61s\n",
      "755:\tlearn: 0.0250145\ttotal: 4.99s\tremaining: 1.61s\n",
      "756:\tlearn: 0.0250143\ttotal: 5s\tremaining: 1.6s\n",
      "757:\tlearn: 0.0250143\ttotal: 5s\tremaining: 1.6s\n",
      "758:\tlearn: 0.0250118\ttotal: 5.01s\tremaining: 1.59s\n",
      "759:\tlearn: 0.0250118\ttotal: 5.02s\tremaining: 1.58s\n",
      "760:\tlearn: 0.0250117\ttotal: 5.03s\tremaining: 1.58s\n",
      "761:\tlearn: 0.0250116\ttotal: 5.04s\tremaining: 1.57s\n",
      "762:\tlearn: 0.0250115\ttotal: 5.04s\tremaining: 1.57s\n",
      "763:\tlearn: 0.0250073\ttotal: 5.05s\tremaining: 1.56s\n",
      "764:\tlearn: 0.0250073\ttotal: 5.06s\tremaining: 1.55s\n",
      "765:\tlearn: 0.0250073\ttotal: 5.07s\tremaining: 1.55s\n",
      "766:\tlearn: 0.0250047\ttotal: 5.07s\tremaining: 1.54s\n",
      "767:\tlearn: 0.0250046\ttotal: 5.08s\tremaining: 1.53s\n",
      "768:\tlearn: 0.0250045\ttotal: 5.09s\tremaining: 1.53s\n",
      "769:\tlearn: 0.0250045\ttotal: 5.09s\tremaining: 1.52s\n",
      "770:\tlearn: 0.0250045\ttotal: 5.1s\tremaining: 1.51s\n",
      "771:\tlearn: 0.0249775\ttotal: 5.11s\tremaining: 1.51s\n",
      "772:\tlearn: 0.0249774\ttotal: 5.11s\tremaining: 1.5s\n",
      "773:\tlearn: 0.0249768\ttotal: 5.13s\tremaining: 1.5s\n",
      "774:\tlearn: 0.0249765\ttotal: 5.13s\tremaining: 1.49s\n",
      "775:\tlearn: 0.0249765\ttotal: 5.14s\tremaining: 1.48s\n",
      "776:\tlearn: 0.0249761\ttotal: 5.14s\tremaining: 1.48s\n",
      "777:\tlearn: 0.0249756\ttotal: 5.15s\tremaining: 1.47s\n",
      "778:\tlearn: 0.0249372\ttotal: 5.16s\tremaining: 1.46s\n",
      "779:\tlearn: 0.0249372\ttotal: 5.16s\tremaining: 1.46s\n",
      "780:\tlearn: 0.0249338\ttotal: 5.17s\tremaining: 1.45s\n",
      "781:\tlearn: 0.0249338\ttotal: 5.17s\tremaining: 1.44s\n",
      "782:\tlearn: 0.0248943\ttotal: 5.18s\tremaining: 1.44s\n",
      "783:\tlearn: 0.0248893\ttotal: 5.19s\tremaining: 1.43s\n",
      "784:\tlearn: 0.0248845\ttotal: 5.2s\tremaining: 1.42s\n",
      "785:\tlearn: 0.0248844\ttotal: 5.2s\tremaining: 1.42s\n",
      "786:\tlearn: 0.0248802\ttotal: 5.21s\tremaining: 1.41s\n",
      "787:\tlearn: 0.0248795\ttotal: 5.22s\tremaining: 1.4s\n",
      "788:\tlearn: 0.0248758\ttotal: 5.22s\tremaining: 1.4s\n",
      "789:\tlearn: 0.0248725\ttotal: 5.23s\tremaining: 1.39s\n",
      "790:\tlearn: 0.0248725\ttotal: 5.24s\tremaining: 1.38s\n",
      "791:\tlearn: 0.0248721\ttotal: 5.24s\tremaining: 1.38s\n",
      "792:\tlearn: 0.0248712\ttotal: 5.25s\tremaining: 1.37s\n",
      "793:\tlearn: 0.0248712\ttotal: 5.25s\tremaining: 1.36s\n",
      "794:\tlearn: 0.0248679\ttotal: 5.26s\tremaining: 1.36s\n",
      "795:\tlearn: 0.0248636\ttotal: 5.27s\tremaining: 1.35s\n",
      "796:\tlearn: 0.0248635\ttotal: 5.28s\tremaining: 1.34s\n",
      "797:\tlearn: 0.0248635\ttotal: 5.29s\tremaining: 1.34s\n",
      "798:\tlearn: 0.0248635\ttotal: 5.3s\tremaining: 1.33s\n",
      "799:\tlearn: 0.0248619\ttotal: 5.3s\tremaining: 1.32s\n",
      "800:\tlearn: 0.0248619\ttotal: 5.31s\tremaining: 1.32s\n",
      "801:\tlearn: 0.0248618\ttotal: 5.32s\tremaining: 1.31s\n",
      "802:\tlearn: 0.0248604\ttotal: 5.32s\tremaining: 1.31s\n",
      "803:\tlearn: 0.0248591\ttotal: 5.33s\tremaining: 1.3s\n",
      "804:\tlearn: 0.0248137\ttotal: 5.34s\tremaining: 1.29s\n",
      "805:\tlearn: 0.0248137\ttotal: 5.34s\tremaining: 1.28s\n",
      "806:\tlearn: 0.0248136\ttotal: 5.35s\tremaining: 1.28s\n",
      "807:\tlearn: 0.0248136\ttotal: 5.36s\tremaining: 1.27s\n",
      "808:\tlearn: 0.0248131\ttotal: 5.37s\tremaining: 1.27s\n",
      "809:\tlearn: 0.0247141\ttotal: 5.37s\tremaining: 1.26s\n",
      "810:\tlearn: 0.0247117\ttotal: 5.38s\tremaining: 1.25s\n",
      "811:\tlearn: 0.0247065\ttotal: 5.39s\tremaining: 1.25s\n",
      "812:\tlearn: 0.0247062\ttotal: 5.39s\tremaining: 1.24s\n",
      "813:\tlearn: 0.0247061\ttotal: 5.4s\tremaining: 1.23s\n",
      "814:\tlearn: 0.0247058\ttotal: 5.41s\tremaining: 1.23s\n",
      "815:\tlearn: 0.0247058\ttotal: 5.41s\tremaining: 1.22s\n",
      "816:\tlearn: 0.0247047\ttotal: 5.42s\tremaining: 1.21s\n",
      "817:\tlearn: 0.0247044\ttotal: 5.43s\tremaining: 1.21s\n",
      "818:\tlearn: 0.0247044\ttotal: 5.44s\tremaining: 1.2s\n",
      "819:\tlearn: 0.0247044\ttotal: 5.44s\tremaining: 1.19s\n",
      "820:\tlearn: 0.0247045\ttotal: 5.45s\tremaining: 1.19s\n",
      "821:\tlearn: 0.0247038\ttotal: 5.46s\tremaining: 1.18s\n",
      "822:\tlearn: 0.0247036\ttotal: 5.46s\tremaining: 1.18s\n",
      "823:\tlearn: 0.0246949\ttotal: 5.47s\tremaining: 1.17s\n",
      "824:\tlearn: 0.0246928\ttotal: 5.48s\tremaining: 1.16s\n",
      "825:\tlearn: 0.0246907\ttotal: 5.48s\tremaining: 1.16s\n",
      "826:\tlearn: 0.0246907\ttotal: 5.49s\tremaining: 1.15s\n",
      "827:\tlearn: 0.0246903\ttotal: 5.5s\tremaining: 1.14s\n",
      "828:\tlearn: 0.0246865\ttotal: 5.5s\tremaining: 1.14s\n",
      "829:\tlearn: 0.0246857\ttotal: 5.51s\tremaining: 1.13s\n",
      "830:\tlearn: 0.0246856\ttotal: 5.51s\tremaining: 1.12s\n",
      "831:\tlearn: 0.0246857\ttotal: 5.52s\tremaining: 1.11s\n",
      "832:\tlearn: 0.0246857\ttotal: 5.53s\tremaining: 1.11s\n",
      "833:\tlearn: 0.0246837\ttotal: 5.54s\tremaining: 1.1s\n",
      "834:\tlearn: 0.0246711\ttotal: 5.54s\tremaining: 1.09s\n",
      "835:\tlearn: 0.0246693\ttotal: 5.55s\tremaining: 1.09s\n",
      "836:\tlearn: 0.0246693\ttotal: 5.56s\tremaining: 1.08s\n",
      "837:\tlearn: 0.0246686\ttotal: 5.57s\tremaining: 1.08s\n",
      "838:\tlearn: 0.0246686\ttotal: 5.57s\tremaining: 1.07s\n",
      "839:\tlearn: 0.0246674\ttotal: 5.58s\tremaining: 1.06s\n",
      "840:\tlearn: 0.0246673\ttotal: 5.59s\tremaining: 1.06s\n",
      "841:\tlearn: 0.0246664\ttotal: 5.59s\tremaining: 1.05s\n",
      "842:\tlearn: 0.0246664\ttotal: 5.6s\tremaining: 1.04s\n",
      "843:\tlearn: 0.0246647\ttotal: 5.61s\tremaining: 1.04s\n",
      "844:\tlearn: 0.0246647\ttotal: 5.62s\tremaining: 1.03s\n",
      "845:\tlearn: 0.0246647\ttotal: 5.62s\tremaining: 1.02s\n",
      "846:\tlearn: 0.0246599\ttotal: 5.63s\tremaining: 1.02s\n",
      "847:\tlearn: 0.0246598\ttotal: 5.63s\tremaining: 1.01s\n",
      "848:\tlearn: 0.0246598\ttotal: 5.64s\tremaining: 1s\n",
      "849:\tlearn: 0.0246581\ttotal: 5.65s\tremaining: 997ms\n",
      "850:\tlearn: 0.0246549\ttotal: 5.66s\tremaining: 991ms\n",
      "851:\tlearn: 0.0246548\ttotal: 5.67s\tremaining: 985ms\n",
      "852:\tlearn: 0.0246547\ttotal: 5.68s\tremaining: 979ms\n",
      "853:\tlearn: 0.0246536\ttotal: 5.69s\tremaining: 972ms\n",
      "854:\tlearn: 0.0246529\ttotal: 5.69s\tremaining: 965ms\n",
      "855:\tlearn: 0.0246528\ttotal: 5.7s\tremaining: 959ms\n",
      "856:\tlearn: 0.0246528\ttotal: 5.71s\tremaining: 952ms\n",
      "857:\tlearn: 0.0246510\ttotal: 5.71s\tremaining: 946ms\n",
      "858:\tlearn: 0.0246510\ttotal: 5.72s\tremaining: 939ms\n",
      "859:\tlearn: 0.0246508\ttotal: 5.72s\tremaining: 932ms\n",
      "860:\tlearn: 0.0246504\ttotal: 5.73s\tremaining: 925ms\n",
      "861:\tlearn: 0.0246504\ttotal: 5.74s\tremaining: 919ms\n",
      "862:\tlearn: 0.0246504\ttotal: 5.75s\tremaining: 912ms\n",
      "863:\tlearn: 0.0246347\ttotal: 5.75s\tremaining: 906ms\n",
      "864:\tlearn: 0.0246123\ttotal: 5.76s\tremaining: 899ms\n",
      "865:\tlearn: 0.0246121\ttotal: 5.76s\tremaining: 892ms\n",
      "866:\tlearn: 0.0245702\ttotal: 5.78s\tremaining: 886ms\n",
      "867:\tlearn: 0.0245702\ttotal: 5.79s\tremaining: 880ms\n",
      "868:\tlearn: 0.0245699\ttotal: 5.79s\tremaining: 873ms\n",
      "869:\tlearn: 0.0245688\ttotal: 5.8s\tremaining: 867ms\n",
      "870:\tlearn: 0.0245670\ttotal: 5.81s\tremaining: 860ms\n",
      "871:\tlearn: 0.0245669\ttotal: 5.81s\tremaining: 853ms\n",
      "872:\tlearn: 0.0245594\ttotal: 5.82s\tremaining: 847ms\n",
      "873:\tlearn: 0.0245588\ttotal: 5.83s\tremaining: 840ms\n",
      "874:\tlearn: 0.0245585\ttotal: 5.83s\tremaining: 833ms\n",
      "875:\tlearn: 0.0245567\ttotal: 5.84s\tremaining: 827ms\n",
      "876:\tlearn: 0.0245557\ttotal: 5.85s\tremaining: 820ms\n",
      "877:\tlearn: 0.0245556\ttotal: 5.86s\tremaining: 814ms\n",
      "878:\tlearn: 0.0245487\ttotal: 5.86s\tremaining: 807ms\n",
      "879:\tlearn: 0.0245051\ttotal: 5.87s\tremaining: 800ms\n",
      "880:\tlearn: 0.0245051\ttotal: 5.88s\tremaining: 794ms\n",
      "881:\tlearn: 0.0245050\ttotal: 5.88s\tremaining: 787ms\n",
      "882:\tlearn: 0.0245049\ttotal: 5.89s\tremaining: 780ms\n",
      "883:\tlearn: 0.0245047\ttotal: 5.9s\tremaining: 774ms\n",
      "884:\tlearn: 0.0245045\ttotal: 5.9s\tremaining: 767ms\n",
      "885:\tlearn: 0.0245041\ttotal: 5.91s\tremaining: 760ms\n",
      "886:\tlearn: 0.0245041\ttotal: 5.92s\tremaining: 754ms\n",
      "887:\tlearn: 0.0245028\ttotal: 5.92s\tremaining: 747ms\n",
      "888:\tlearn: 0.0244852\ttotal: 5.93s\tremaining: 740ms\n",
      "889:\tlearn: 0.0244851\ttotal: 5.94s\tremaining: 734ms\n",
      "890:\tlearn: 0.0244833\ttotal: 5.94s\tremaining: 727ms\n",
      "891:\tlearn: 0.0244831\ttotal: 5.95s\tremaining: 721ms\n",
      "892:\tlearn: 0.0244806\ttotal: 5.96s\tremaining: 714ms\n",
      "893:\tlearn: 0.0244802\ttotal: 5.96s\tremaining: 707ms\n",
      "894:\tlearn: 0.0244798\ttotal: 5.97s\tremaining: 700ms\n",
      "895:\tlearn: 0.0244792\ttotal: 5.98s\tremaining: 694ms\n",
      "896:\tlearn: 0.0244791\ttotal: 5.99s\tremaining: 687ms\n",
      "897:\tlearn: 0.0244792\ttotal: 5.99s\tremaining: 681ms\n",
      "898:\tlearn: 0.0244791\ttotal: 6s\tremaining: 674ms\n",
      "899:\tlearn: 0.0244790\ttotal: 6.01s\tremaining: 667ms\n",
      "900:\tlearn: 0.0244790\ttotal: 6.01s\tremaining: 661ms\n",
      "901:\tlearn: 0.0244790\ttotal: 6.02s\tremaining: 654ms\n",
      "902:\tlearn: 0.0244752\ttotal: 6.03s\tremaining: 648ms\n",
      "903:\tlearn: 0.0244747\ttotal: 6.04s\tremaining: 642ms\n",
      "904:\tlearn: 0.0244741\ttotal: 6.05s\tremaining: 635ms\n",
      "905:\tlearn: 0.0244737\ttotal: 6.06s\tremaining: 628ms\n",
      "906:\tlearn: 0.0244734\ttotal: 6.07s\tremaining: 623ms\n",
      "907:\tlearn: 0.0244692\ttotal: 6.08s\tremaining: 616ms\n",
      "908:\tlearn: 0.0244680\ttotal: 6.09s\tremaining: 609ms\n",
      "909:\tlearn: 0.0244674\ttotal: 6.09s\tremaining: 603ms\n",
      "910:\tlearn: 0.0244675\ttotal: 6.1s\tremaining: 596ms\n",
      "911:\tlearn: 0.0244674\ttotal: 6.11s\tremaining: 589ms\n",
      "912:\tlearn: 0.0244627\ttotal: 6.12s\tremaining: 583ms\n",
      "913:\tlearn: 0.0244627\ttotal: 6.12s\tremaining: 576ms\n",
      "914:\tlearn: 0.0244626\ttotal: 6.13s\tremaining: 569ms\n",
      "915:\tlearn: 0.0244612\ttotal: 6.14s\tremaining: 563ms\n",
      "916:\tlearn: 0.0244581\ttotal: 6.15s\tremaining: 557ms\n",
      "917:\tlearn: 0.0244581\ttotal: 6.16s\tremaining: 550ms\n",
      "918:\tlearn: 0.0244582\ttotal: 6.16s\tremaining: 543ms\n",
      "919:\tlearn: 0.0244581\ttotal: 6.17s\tremaining: 536ms\n",
      "920:\tlearn: 0.0244578\ttotal: 6.18s\tremaining: 530ms\n",
      "921:\tlearn: 0.0244575\ttotal: 6.18s\tremaining: 523ms\n",
      "922:\tlearn: 0.0244575\ttotal: 6.19s\tremaining: 516ms\n",
      "923:\tlearn: 0.0244575\ttotal: 6.2s\tremaining: 510ms\n",
      "924:\tlearn: 0.0244571\ttotal: 6.2s\tremaining: 503ms\n",
      "925:\tlearn: 0.0244568\ttotal: 6.21s\tremaining: 496ms\n",
      "926:\tlearn: 0.0244565\ttotal: 6.22s\tremaining: 490ms\n",
      "927:\tlearn: 0.0244564\ttotal: 6.22s\tremaining: 483ms\n",
      "928:\tlearn: 0.0244561\ttotal: 6.23s\tremaining: 476ms\n",
      "929:\tlearn: 0.0244560\ttotal: 6.24s\tremaining: 469ms\n",
      "930:\tlearn: 0.0244560\ttotal: 6.25s\tremaining: 463ms\n",
      "931:\tlearn: 0.0244553\ttotal: 6.25s\tremaining: 456ms\n",
      "932:\tlearn: 0.0244552\ttotal: 6.26s\tremaining: 449ms\n",
      "933:\tlearn: 0.0244552\ttotal: 6.27s\tremaining: 443ms\n",
      "934:\tlearn: 0.0244546\ttotal: 6.27s\tremaining: 436ms\n",
      "935:\tlearn: 0.0244545\ttotal: 6.28s\tremaining: 430ms\n",
      "936:\tlearn: 0.0244544\ttotal: 6.29s\tremaining: 423ms\n",
      "937:\tlearn: 0.0244499\ttotal: 6.29s\tremaining: 416ms\n",
      "938:\tlearn: 0.0244499\ttotal: 6.3s\tremaining: 409ms\n",
      "939:\tlearn: 0.0244498\ttotal: 6.31s\tremaining: 403ms\n",
      "940:\tlearn: 0.0244496\ttotal: 6.31s\tremaining: 396ms\n",
      "941:\tlearn: 0.0244455\ttotal: 6.32s\tremaining: 389ms\n",
      "942:\tlearn: 0.0244454\ttotal: 6.33s\tremaining: 382ms\n",
      "943:\tlearn: 0.0244454\ttotal: 6.33s\tremaining: 376ms\n",
      "944:\tlearn: 0.0244454\ttotal: 6.34s\tremaining: 369ms\n",
      "945:\tlearn: 0.0244446\ttotal: 6.35s\tremaining: 362ms\n",
      "946:\tlearn: 0.0244445\ttotal: 6.35s\tremaining: 356ms\n",
      "947:\tlearn: 0.0244445\ttotal: 6.36s\tremaining: 349ms\n",
      "948:\tlearn: 0.0244443\ttotal: 6.37s\tremaining: 342ms\n",
      "949:\tlearn: 0.0244445\ttotal: 6.37s\tremaining: 335ms\n",
      "950:\tlearn: 0.0244441\ttotal: 6.38s\tremaining: 329ms\n",
      "951:\tlearn: 0.0244441\ttotal: 6.39s\tremaining: 322ms\n",
      "952:\tlearn: 0.0244440\ttotal: 6.39s\tremaining: 315ms\n",
      "953:\tlearn: 0.0244441\ttotal: 6.4s\tremaining: 309ms\n",
      "954:\tlearn: 0.0244433\ttotal: 6.41s\tremaining: 302ms\n",
      "955:\tlearn: 0.0244319\ttotal: 6.42s\tremaining: 295ms\n",
      "956:\tlearn: 0.0244319\ttotal: 6.42s\tremaining: 289ms\n",
      "957:\tlearn: 0.0244305\ttotal: 6.43s\tremaining: 282ms\n",
      "958:\tlearn: 0.0244306\ttotal: 6.43s\tremaining: 275ms\n",
      "959:\tlearn: 0.0244306\ttotal: 6.44s\tremaining: 268ms\n",
      "960:\tlearn: 0.0244305\ttotal: 6.45s\tremaining: 262ms\n",
      "961:\tlearn: 0.0244305\ttotal: 6.46s\tremaining: 255ms\n",
      "962:\tlearn: 0.0244303\ttotal: 6.46s\tremaining: 248ms\n",
      "963:\tlearn: 0.0244303\ttotal: 6.47s\tremaining: 242ms\n",
      "964:\tlearn: 0.0244293\ttotal: 6.48s\tremaining: 235ms\n",
      "965:\tlearn: 0.0244282\ttotal: 6.48s\tremaining: 228ms\n",
      "966:\tlearn: 0.0244281\ttotal: 6.49s\tremaining: 221ms\n",
      "967:\tlearn: 0.0244279\ttotal: 6.5s\tremaining: 215ms\n",
      "968:\tlearn: 0.0244279\ttotal: 6.5s\tremaining: 208ms\n",
      "969:\tlearn: 0.0244279\ttotal: 6.51s\tremaining: 201ms\n",
      "970:\tlearn: 0.0244173\ttotal: 6.52s\tremaining: 195ms\n",
      "971:\tlearn: 0.0243571\ttotal: 6.53s\tremaining: 188ms\n",
      "972:\tlearn: 0.0243571\ttotal: 6.53s\tremaining: 181ms\n",
      "973:\tlearn: 0.0243571\ttotal: 6.54s\tremaining: 175ms\n",
      "974:\tlearn: 0.0243567\ttotal: 6.55s\tremaining: 168ms\n",
      "975:\tlearn: 0.0243567\ttotal: 6.56s\tremaining: 161ms\n",
      "976:\tlearn: 0.0243565\ttotal: 6.57s\tremaining: 155ms\n",
      "977:\tlearn: 0.0243563\ttotal: 6.57s\tremaining: 148ms\n",
      "978:\tlearn: 0.0243561\ttotal: 6.58s\tremaining: 141ms\n",
      "979:\tlearn: 0.0243561\ttotal: 6.59s\tremaining: 135ms\n",
      "980:\tlearn: 0.0243560\ttotal: 6.6s\tremaining: 128ms\n",
      "981:\tlearn: 0.0243532\ttotal: 6.61s\tremaining: 121ms\n",
      "982:\tlearn: 0.0243443\ttotal: 6.61s\tremaining: 114ms\n",
      "983:\tlearn: 0.0243441\ttotal: 6.62s\tremaining: 108ms\n",
      "984:\tlearn: 0.0243445\ttotal: 6.63s\tremaining: 101ms\n",
      "985:\tlearn: 0.0243433\ttotal: 6.63s\tremaining: 94.2ms\n",
      "986:\tlearn: 0.0243427\ttotal: 6.64s\tremaining: 87.5ms\n",
      "987:\tlearn: 0.0243428\ttotal: 6.65s\tremaining: 80.8ms\n",
      "988:\tlearn: 0.0243395\ttotal: 6.66s\tremaining: 74.1ms\n",
      "989:\tlearn: 0.0243394\ttotal: 6.67s\tremaining: 67.3ms\n",
      "990:\tlearn: 0.0243394\ttotal: 6.67s\tremaining: 60.6ms\n",
      "991:\tlearn: 0.0243394\ttotal: 6.68s\tremaining: 53.9ms\n",
      "992:\tlearn: 0.0243395\ttotal: 6.69s\tremaining: 47.2ms\n",
      "993:\tlearn: 0.0243392\ttotal: 6.7s\tremaining: 40.4ms\n",
      "994:\tlearn: 0.0243392\ttotal: 6.71s\tremaining: 33.7ms\n",
      "995:\tlearn: 0.0243393\ttotal: 6.71s\tremaining: 27ms\n",
      "996:\tlearn: 0.0243392\ttotal: 6.72s\tremaining: 20.2ms\n",
      "997:\tlearn: 0.0243389\ttotal: 6.73s\tremaining: 13.5ms\n",
      "998:\tlearn: 0.0243389\ttotal: 6.74s\tremaining: 6.74ms\n",
      "999:\tlearn: 0.0243335\ttotal: 6.74s\tremaining: 0us\n",
      "MAPE 0.06309672524168936\n"
     ]
    }
   ],
   "source": [
    "model_cat = CatBoostRegressor(depth=best_depth, learning_rate=best_learning_rate, l2_leaf_reg= best_l2, loss_function='MAPE')\n",
    "model_cat.fit(features_train, target_train)\n",
    "print('MAPE', mean_absolute_percentage_error(y_true=target_test, y_pred=model_cat.predict(features_test)))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
