{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HSE 2021: Mathematical Methods for Data Analysis\n",
    "\n",
    "## Homework 4\n",
    "\n",
    "**Warning 1**: You have 3 weeks for this assignemnt.  **it is better to start early (!)**\n",
    "\n",
    "**Warning 2**: it is critical to describe and explain what you are doing and why, use markdown cells\n",
    "\n",
    "\n",
    "### Contents\n",
    "\n",
    "#### Decision Trees - 7 points\n",
    "* [Task 1](#task1) (0.5 points)\n",
    "* [Task 2](#task2) (0.5 points)\n",
    "* [Task 3](#task3) (2 points)\n",
    "* [Task 4](#task4) (0.5 points)\n",
    "* [Task 5](#task5) (0.5 points)\n",
    "* [Task 6](#task6) (2 points)\n",
    "* [Task 7](#task7) (0.5 points)\n",
    "* [Task 8](#task8) (0.5 points)\n",
    "\n",
    "#### Ensembles - 3 points\n",
    "* [Task 1](#task2_1) (1 point)\n",
    "* [Task 2](#task2_2) (0.7 points)\n",
    "* [Task 3](#task2_3) (0.5 points)\n",
    "* [Task 4](#task2_4) (0.7 points)\n",
    "* [Task 5](#task2_5) (0.1 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (11, 5)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1. Decision Tree Regressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this task you will be implementing decision tree for the regression by hands. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1 <a id=\"task1\"></a> (0.5 points)\n",
    "\n",
    "Implement the function `H()` which calculates impurity criterion. We will be training regression tree, therefore, impurity criterion will be variance.\n",
    "\n",
    "* You cannot use loops\n",
    "* If `y` is empty, the function should return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def H(y):\n",
    "    \"\"\"\n",
    "    Calculate impurity criterion\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    y : np.array\n",
    "        array of objects target values in the node\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    H(R) : float\n",
    "        Impurity in the node (measuread by variance)\n",
    "    \"\"\"\n",
    "    if len(y) == 0:\n",
    "        return 0\n",
    "    yr = np.mean(y)\n",
    "    return np.mean(np.power(y - yr,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the function\n",
    "assert np.allclose(H(np.array([4,2,2, 2])), 0.75)\n",
    "assert np.allclose(H(np.array([])), 0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2 <a id=\"task2\"></a>  (0.5 points)\n",
    "\n",
    "To find the best split in the node we need to calculate the cost function. Denote: \n",
    "- `R` all the object in the node\n",
    "- `j` index of the feature selected for the split\n",
    "- `t` threshold\n",
    "- `R_l` and `R_r` objects in the left and right child nodes correspondingly\n",
    "\n",
    "We get the following cost function:\n",
    "\n",
    "$$\n",
    "Q(R, j, t) =\\frac{|R_\\ell|}{|R|}H(R_\\ell) + \\frac{|R_r|}{|R|}H(R_r) \\to \\min_{j, t},\n",
    "$$\n",
    "\n",
    "Implement the function `Q`, which should calculate value of the cost function for a given feature and threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Q(X, y, j, t):\n",
    "    \"\"\"\n",
    "    Calculate cost function\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : ndarray\n",
    "        array of objects in the node \n",
    "    y : ndarray\n",
    "        array of target values in the node \n",
    "    j : int\n",
    "        feature index (column in X)\n",
    "    t : float\n",
    "        threshold\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Q : float\n",
    "        Value of the cost function\n",
    "    \"\"\"   \n",
    "    R_l = y[X[:,j]<t]\n",
    "    R_r = y[X[:,j]>=t]\n",
    "    Q = len(R_l)/len(X)*H(R_l) + len(R_r)/len(X)*H(R_r)\n",
    "    return Q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3 <a id=\"task3\"></a>  (2 points)\n",
    "\n",
    "Now, let's implement `MyDecisionTreeRegressor` class. More specifically, you need to implement the following methods:\n",
    "\n",
    "- `best_split`\n",
    "- `grow_tree`\n",
    "- `get_prediction`\n",
    "\n",
    "Read docstrings for more details. Do not forget to use function `Q` implemented above, when finding the `best_split`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node(object):\n",
    "    \"\"\"\n",
    "    Class for a decision tree node.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    right : Node() or None\n",
    "        Right child\n",
    "    right : Node() or None\n",
    "        Left child\n",
    "    threshold: float\n",
    "        \n",
    "    column: int\n",
    "        \n",
    "    depth: int\n",
    "        \n",
    "    prediction: float\n",
    "        prediction of the target value in the node (average values calculated on a train dataset)\n",
    "    is_terminal:bool\n",
    "        indicates whether it is a terminal node (leaf) or not\n",
    "    \"\"\"    \n",
    "    def __init__(self):        \n",
    "        self.right = None\n",
    "        self.left = None\n",
    "        self.threshold = None\n",
    "        self.column = None\n",
    "        self.depth = None\n",
    "        self.is_terminal = False\n",
    "        self.prediction = None\n",
    "        \n",
    "    def __repr__(self):\n",
    "        if self.is_terminal:\n",
    "            node_desc = 'Pred: {:.2f}'.format(self.prediction)\n",
    "        else:\n",
    "            node_desc = 'Col {}, t {:.2f}, Pred: {:.2f}'.format(self.column, self.threshold, self.prediction)\n",
    "        return node_desc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, RegressorMixin\n",
    "from sklearn.utils.validation import check_X_y, check_array, check_is_fitted\n",
    "\n",
    "class MyDecisionTreeRegressor(RegressorMixin, BaseEstimator):\n",
    "    \"\"\"\n",
    "    Class for a Decision Tree Regressor.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    max_depth : int\n",
    "        Max depth of a decision tree.\n",
    "    min_samples_split : int\n",
    "        Minimal number of samples (objects) in a node to make a split.\n",
    "    \"\"\" \n",
    "    def __init__(self, max_depth=3, min_samples_split=2):\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_split = min_samples_split\n",
    "            \n",
    "    def best_split(self, X, y):\n",
    "        \"\"\"\n",
    "        Find the best split in terms of Q of data in a given decision tree node. \n",
    "        Try all features and thresholds. \n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : ndarray, shape (n_objects, n_features)\n",
    "            Objects in the parent node\n",
    "        y : ndarray, shape (n_objects, )\n",
    "            1D array with the object labels. \n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        best_split_column : int\n",
    "            Index of the best split column\n",
    "        best_threshold : float\n",
    "            The best split condition.\n",
    "        X_left : ndarray, shape (n_objects_l, n_features)\n",
    "            Objects in the left child\n",
    "        y_left : ndarray, shape (n_objects_l, )\n",
    "            Objects labels in the left child. \n",
    "        X_right : ndarray, shape (n_objects_r, n_features)\n",
    "            Objects in the right child\n",
    "        y_right : ndarray, shape (n_objects_r, )\n",
    "            Objects labels in the right child. \n",
    "        \"\"\"\n",
    "        \n",
    "        # To store best split parameters\n",
    "        best_split_column = None\n",
    "        best_threshold = None\n",
    "        # without splitting\n",
    "        X_left = None\n",
    "        X_right = None\n",
    "        y_left = None\n",
    "        y_right = None\n",
    "        best_cost = H(y) \n",
    "        for i in range(len(X[0,:])):\n",
    "            for j in range(len(X[:,0])):\n",
    "                cost_tmp = Q(X,y,i,X[j,i])\n",
    "                if(best_cost > cost_tmp):\n",
    "                    best_cost = cost_tmp\n",
    "                    best_split_column = i\n",
    "                    best_threshold = X[j,i]\n",
    "                    X_left = X[X[:,i] < X[j,i]]\n",
    "                    X_right = X[X[:,i] >= X[j,i]]\n",
    "                    y_left = y[X[:,i] < X[j,i]]\n",
    "                    y_right = y[X[:,i] >= X[j,i]]\n",
    "                            \n",
    "        return best_split_column, best_threshold, X_left, y_left, X_right, y_right\n",
    "    \n",
    "    def is_terminal(self, node, y):\n",
    "        \"\"\"\n",
    "        Check terminality conditions based on `max_depth` and `min_samples_split` parameters for a given node. \n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        node : Node, \n",
    "            \n",
    "        y : ndarray, shape (n_objects, )\n",
    "            Object labels. \n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        Is_termial : bool\n",
    "            If True, node is terminal\n",
    "        \"\"\"\n",
    "        if node.depth >= self.max_depth:    \n",
    "            return True\n",
    "        if len(y) < self.min_samples_split:   \n",
    "            return True\n",
    "        return False\n",
    "        \n",
    "    def grow_tree(self, node, X, y):\n",
    "        \"\"\"\n",
    "        Reccurently grow the tree from the `node` using a `X` and `y` as a dataset:\n",
    "         - check terminality conditions\n",
    "         - find best split if node is not terminal\n",
    "         - add child nodes to the node\n",
    "         - call the function recursively for the added child nodes\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        node : Node() object\n",
    "            Current node of the decision tree.\n",
    "        X : ndarray, shape (n_objects, n_features)\n",
    "            Objects \n",
    "        y : ndarray, shape (n_objects)\n",
    "            Labels\n",
    "        \"\"\"\n",
    "        \n",
    "        if self.is_terminal(node, y):\n",
    "            node.is_terminal =True\n",
    "            return\n",
    "        if np.all(y == y[0]):\n",
    "            node.is_terminal =True\n",
    "            return\n",
    "            \n",
    "        best_split_column, best_threshold, X_left, y_left, X_right, y_right = self.best_split(X,y)\n",
    "        node_left = Node()\n",
    "        node.column = best_split_column\n",
    "        node.threshold = best_threshold\n",
    "        node_left.depth = node.depth + 1\n",
    "        node_left.prediction = np.mean(y_left)\n",
    "        node.left = node_left\n",
    "        self.grow_tree(node_left, X_left, y_left)\n",
    "        \n",
    "        node_right = Node()\n",
    "        node_right.depth = node.depth + 1\n",
    "        node_right.prediction = np.mean(y_right)\n",
    "        node.right = node_right\n",
    "        self.grow_tree(node_right, X_right, y_right)\n",
    "        # YOUR CODE HERE\n",
    "        \n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Fit the Decision Tree Regressor.\n",
    "            \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : ndarray, shape (n_samples, n_features)\n",
    "            The input samples.\n",
    "        y : ndarray, shape (n_samples,) or (n_samples, n_outputs)\n",
    "            The target values.\n",
    "        Returns\n",
    "        -------\n",
    "        self : object\n",
    "            Returns self.\n",
    "        \"\"\"\n",
    "        X, y = check_X_y(X, y, accept_sparse=False)\n",
    "        self.is_fitted_ = True\n",
    "        \n",
    "        # Initialize the tree (root node)\n",
    "        self.tree_ = Node()                             \n",
    "        self.tree_.depth = 1                            \n",
    "        self.tree_.prediction = np.mean(y)\n",
    "        \n",
    "        # Grow the tree\n",
    "        self.grow_tree(self.tree_, X, y)\n",
    "        return self        \n",
    "    \n",
    "    def get_prediction(self, node, x):\n",
    "        \"\"\"\n",
    "        Get prediction for an object `x`\n",
    "            - Return prediction of the `node` if it is terminal\n",
    "            - Otherwise, recursively call the function to get predictions of the proper child\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        node : Node() object\n",
    "            Current node of the decision tree.\n",
    "        x : ndarray, shape (n_features,)\n",
    "            Array of feature values of one object.\n",
    "        Returns\n",
    "        -------\n",
    "        y_pred : float\n",
    "            Prediction for an object x\n",
    "        \"\"\"\n",
    "        # YOUR CODE HERE\n",
    "        if node.is_terminal:\n",
    "            return node.prediction\n",
    "        if x[node.column] < node.threshold:\n",
    "            y_pred = self.get_prediction(node.left, x)\n",
    "        else:\n",
    "            y_pred = self.get_prediction(node.right, x)\n",
    "        \n",
    "        \n",
    "        return y_pred\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\" \n",
    "        Get prediction for each object in X\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : ndarray, shape (n_samples, n_features)\n",
    "            The input samples.\n",
    "        Returns\n",
    "        -------\n",
    "        y : ndarray, shape (n_samples,)\n",
    "            Returns predictions.\n",
    "        \"\"\"\n",
    "        # Check input and that `fit` had been called\n",
    "        X = check_array(X, accept_sparse=False)\n",
    "        check_is_fitted(self, 'is_fitted_')\n",
    "        \n",
    "        # Get predictions\n",
    "        y_predicted = []\n",
    "        for x in X:\n",
    "            y_curr = self.get_prediction(self.tree_, x)\n",
    "            y_predicted.append(y_curr)\n",
    "        return np.array(y_predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\f.deryabin\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\estimator_checks.py:3005: FutureWarning: As of scikit-learn 0.23, estimators should expose a n_features_in_ attribute, unless the 'no_validation' tag is True. This attribute should be equal to the number of features passed to the fit method. An error will be raised from version 0.25 when calling check_estimator(). See SLEP010: https://scikit-learn-enhancement-proposals.readthedocs.io/en/latest/slep010/proposal.html\n",
      "  warnings.warn(\n",
      "C:\\Users\\f.deryabin\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\estimator_checks.py:3047: FutureWarning: As of scikit-learn 0.23, estimators should have a 'requires_y' tag set to the appropriate value. The default value of the tag is False. An error will be raised from version 0.25 when calling check_estimator() if the tag isn't properly set.\n",
      "  warnings.warn(warning_msg, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# check yourself\n",
    "from sklearn.utils.estimator_checks import check_estimator\n",
    "\n",
    "check_estimator(MyDecisionTreeRegressor())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4 <a id=\"task4\"></a>  (0.5 points)\n",
    "\n",
    "Load boston dataset and split it on the train ($70\\%$) and test ($30\\%$). Fit Decision Tree of depth 1 and make the following plot:\n",
    "\n",
    "- Scatter plot of the traning points (selected for split feature on the x-axis, target variable on the y-axis)\n",
    "- Fitted model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.lines.Line2D at 0x248883f28b0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "# YOUR CODE HERE\n",
    "data = load_boston()\n",
    "\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 0)\n",
    "\n",
    "mdtr = MyDecisionTreeRegressor(2)\n",
    "mdtr.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "plt.scatter(X_train[:,mdtr.tree_.column], y_train)\n",
    "plt.axvline(mdtr.tree_.threshold, color = 'Red')\n",
    "plt.show()\n",
    "\n",
    "y_p = mdtr.predict(X_test)\n",
    "\n",
    "\n",
    "plt.scatter(X_test[:,mdtr.tree_.column], y_test)\n",
    "plt.scatter(X_test[:,mdtr.tree_.column], y_p, color = 'orange')\n",
    "\n",
    "plt.axvline(mdtr.tree_.threshold, color = 'Red')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 5 <a id=\"task5\"></a>  (0.5 points)\n",
    "\n",
    "Keep working with boston dataset. \n",
    "- Use `GridSearchCV` to find the best hyperparameters (`max_depth` and `min_samples_split`) on 5-Fold cross-validation\n",
    "- Train the model with the best set of hyperparameters on the whole train dataset. \n",
    "- Report `RMSE` on test dataset and hyperparameters of the best estimator. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[CV] max_depth=2, min_samples_split=3 ................................\n",
      "[CV] ... max_depth=2, min_samples_split=3, score=-6.562, total=   0.2s\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s remaining:    0.0s\n",
      "[CV] max_depth=2, min_samples_split=3 ................................\n",
      "[CV] ... max_depth=2, min_samples_split=3, score=-6.536, total=   0.2s\n",
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    0.2s remaining:    0.0s\n",
      "[CV] max_depth=2, min_samples_split=3 ................................\n",
      "[CV] ... max_depth=2, min_samples_split=3, score=-7.295, total=   0.2s\n",
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    0.4s remaining:    0.0s\n",
      "[CV] max_depth=2, min_samples_split=3 ................................\n",
      "[CV] ... max_depth=2, min_samples_split=3, score=-8.451, total=   0.1s\n",
      "[Parallel(n_jobs=1)]: Done   4 out of   4 | elapsed:    0.5s remaining:    0.0s\n",
      "[CV] max_depth=2, min_samples_split=3 ................................\n",
      "[CV] ... max_depth=2, min_samples_split=3, score=-6.895, total=   0.2s\n",
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:    0.7s remaining:    0.0s\n",
      "[CV] max_depth=2, min_samples_split=5 ................................\n",
      "[CV] ... max_depth=2, min_samples_split=5, score=-6.562, total=   0.2s\n",
      "[Parallel(n_jobs=1)]: Done   6 out of   6 | elapsed:    0.8s remaining:    0.0s\n",
      "[CV] max_depth=2, min_samples_split=5 ................................\n",
      "[CV] ... max_depth=2, min_samples_split=5, score=-6.536, total=   0.2s\n",
      "[Parallel(n_jobs=1)]: Done   7 out of   7 | elapsed:    1.0s remaining:    0.0s\n",
      "[CV] max_depth=2, min_samples_split=5 ................................\n",
      "[CV] ... max_depth=2, min_samples_split=5, score=-7.295, total=   0.2s\n",
      "[Parallel(n_jobs=1)]: Done   8 out of   8 | elapsed:    1.2s remaining:    0.0s\n",
      "[CV] max_depth=2, min_samples_split=5 ................................\n",
      "[CV] ... max_depth=2, min_samples_split=5, score=-8.451, total=   0.2s\n",
      "[Parallel(n_jobs=1)]: Done   9 out of   9 | elapsed:    1.3s remaining:    0.0s\n",
      "[CV] max_depth=2, min_samples_split=5 ................................\n",
      "[CV] ... max_depth=2, min_samples_split=5, score=-6.895, total=   0.2s\n",
      "[Parallel(n_jobs=1)]: Done  10 out of  10 | elapsed:    1.5s remaining:    0.0s\n",
      "[CV] max_depth=2, min_samples_split=8 ................................\n",
      "[CV] ... max_depth=2, min_samples_split=8, score=-6.562, total=   0.2s\n",
      "[Parallel(n_jobs=1)]: Done  11 out of  11 | elapsed:    1.7s remaining:    0.0s\n",
      "[CV] max_depth=2, min_samples_split=8 ................................\n",
      "[CV] ... max_depth=2, min_samples_split=8, score=-6.536, total=   0.2s\n",
      "[Parallel(n_jobs=1)]: Done  12 out of  12 | elapsed:    1.8s remaining:    0.0s\n",
      "[CV] max_depth=2, min_samples_split=8 ................................\n",
      "[CV] ... max_depth=2, min_samples_split=8, score=-7.295, total=   0.2s\n",
      "[Parallel(n_jobs=1)]: Done  13 out of  13 | elapsed:    2.0s remaining:    0.0s\n",
      "[CV] max_depth=2, min_samples_split=8 ................................\n",
      "[CV] ... max_depth=2, min_samples_split=8, score=-8.451, total=   0.2s\n",
      "[Parallel(n_jobs=1)]: Done  14 out of  14 | elapsed:    2.2s remaining:    0.0s\n",
      "[CV] max_depth=2, min_samples_split=8 ................................\n",
      "[CV] ... max_depth=2, min_samples_split=8, score=-6.895, total=   0.2s\n",
      "[Parallel(n_jobs=1)]: Done  15 out of  15 | elapsed:    2.4s remaining:    0.0s\n",
      "[CV] max_depth=2, min_samples_split=9 ................................\n",
      "[CV] ... max_depth=2, min_samples_split=9, score=-6.562, total=   0.2s\n",
      "[Parallel(n_jobs=1)]: Done  16 out of  16 | elapsed:    2.7s remaining:    0.0s\n",
      "[CV] max_depth=2, min_samples_split=9 ................................\n",
      "[CV] ... max_depth=2, min_samples_split=9, score=-6.536, total=   0.2s\n",
      "[Parallel(n_jobs=1)]: Done  17 out of  17 | elapsed:    2.9s remaining:    0.0s\n",
      "[CV] max_depth=2, min_samples_split=9 ................................\n",
      "[CV] ... max_depth=2, min_samples_split=9, score=-7.295, total=   0.2s\n",
      "[Parallel(n_jobs=1)]: Done  18 out of  18 | elapsed:    3.1s remaining:    0.0s\n",
      "[CV] max_depth=2, min_samples_split=9 ................................\n",
      "[CV] ... max_depth=2, min_samples_split=9, score=-8.451, total=   0.2s\n",
      "[Parallel(n_jobs=1)]: Done  19 out of  19 | elapsed:    3.4s remaining:    0.0s\n",
      "[CV] max_depth=2, min_samples_split=9 ................................\n",
      "[CV] ... max_depth=2, min_samples_split=9, score=-6.895, total=   0.2s\n",
      "[Parallel(n_jobs=1)]: Done  20 out of  20 | elapsed:    3.6s remaining:    0.0s\n",
      "[CV] max_depth=4, min_samples_split=3 ................................\n",
      "[CV] ... max_depth=4, min_samples_split=3, score=-4.001, total=   0.6s\n",
      "[Parallel(n_jobs=1)]: Done  21 out of  21 | elapsed:    4.2s remaining:    0.0s\n",
      "[CV] max_depth=4, min_samples_split=3 ................................\n",
      "[CV] ... max_depth=4, min_samples_split=3, score=-5.396, total=   0.6s\n",
      "[Parallel(n_jobs=1)]: Done  22 out of  22 | elapsed:    4.8s remaining:    0.0s\n",
      "[CV] max_depth=4, min_samples_split=3 ................................\n",
      "[CV] ... max_depth=4, min_samples_split=3, score=-7.689, total=   0.6s\n",
      "[Parallel(n_jobs=1)]: Done  23 out of  23 | elapsed:    5.4s remaining:    0.0s\n",
      "[CV] max_depth=4, min_samples_split=3 ................................\n",
      "[CV] ... max_depth=4, min_samples_split=3, score=-5.673, total=   0.6s\n",
      "[Parallel(n_jobs=1)]: Done  24 out of  24 | elapsed:    6.1s remaining:    0.0s\n",
      "[CV] max_depth=4, min_samples_split=3 ................................\n",
      "[CV] ... max_depth=4, min_samples_split=3, score=-5.893, total=   0.6s\n",
      "[Parallel(n_jobs=1)]: Done  25 out of  25 | elapsed:    6.7s remaining:    0.0s\n",
      "[CV] max_depth=4, min_samples_split=5 ................................\n",
      "[CV] ... max_depth=4, min_samples_split=5, score=-4.001, total=   0.6s\n",
      "[Parallel(n_jobs=1)]: Done  26 out of  26 | elapsed:    7.3s remaining:    0.0s\n",
      "[CV] max_depth=4, min_samples_split=5 ................................\n",
      "[CV] ... max_depth=4, min_samples_split=5, score=-5.396, total=   0.7s\n",
      "[Parallel(n_jobs=1)]: Done  27 out of  27 | elapsed:    8.0s remaining:    0.0s\n",
      "[CV] max_depth=4, min_samples_split=5 ................................\n",
      "[CV] ... max_depth=4, min_samples_split=5, score=-7.689, total=   0.4s\n",
      "[Parallel(n_jobs=1)]: Done  28 out of  28 | elapsed:    8.4s remaining:    0.0s\n",
      "[CV] max_depth=4, min_samples_split=5 ................................\n",
      "[CV] ... max_depth=4, min_samples_split=5, score=-5.673, total=   0.5s\n",
      "[Parallel(n_jobs=1)]: Done  29 out of  29 | elapsed:    8.9s remaining:    0.0s\n",
      "[CV] max_depth=4, min_samples_split=5 ................................\n",
      "[CV] ... max_depth=4, min_samples_split=5, score=-5.893, total=   0.5s\n",
      "[Parallel(n_jobs=1)]: Done  30 out of  30 | elapsed:    9.4s remaining:    0.0s\n",
      "[CV] max_depth=4, min_samples_split=8 ................................\n",
      "[CV] ... max_depth=4, min_samples_split=8, score=-4.001, total=   0.5s\n",
      "[Parallel(n_jobs=1)]: Done  31 out of  31 | elapsed:    9.9s remaining:    0.0s\n",
      "[CV] max_depth=4, min_samples_split=8 ................................\n",
      "[CV] ... max_depth=4, min_samples_split=8, score=-5.396, total=   0.5s\n",
      "[Parallel(n_jobs=1)]: Done  32 out of  32 | elapsed:   10.4s remaining:    0.0s\n",
      "[CV] max_depth=4, min_samples_split=8 ................................\n",
      "[CV] ... max_depth=4, min_samples_split=8, score=-7.689, total=   0.4s\n",
      "[Parallel(n_jobs=1)]: Done  33 out of  33 | elapsed:   10.8s remaining:    0.0s\n",
      "[CV] max_depth=4, min_samples_split=8 ................................\n",
      "[CV] ... max_depth=4, min_samples_split=8, score=-5.673, total=   0.4s\n",
      "[Parallel(n_jobs=1)]: Done  34 out of  34 | elapsed:   11.2s remaining:    0.0s\n",
      "[CV] max_depth=4, min_samples_split=8 ................................\n",
      "[CV] ... max_depth=4, min_samples_split=8, score=-5.893, total=   0.4s\n",
      "[Parallel(n_jobs=1)]: Done  35 out of  35 | elapsed:   11.6s remaining:    0.0s\n",
      "[CV] max_depth=4, min_samples_split=9 ................................\n",
      "[CV] ... max_depth=4, min_samples_split=9, score=-4.001, total=   0.4s\n",
      "[Parallel(n_jobs=1)]: Done  36 out of  36 | elapsed:   12.1s remaining:    0.0s\n",
      "[CV] max_depth=4, min_samples_split=9 ................................\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ... max_depth=4, min_samples_split=9, score=-5.396, total=   0.5s\n",
      "[Parallel(n_jobs=1)]: Done  37 out of  37 | elapsed:   12.5s remaining:    0.0s\n",
      "[CV] max_depth=4, min_samples_split=9 ................................\n",
      "[CV] ... max_depth=4, min_samples_split=9, score=-7.689, total=   0.5s\n",
      "[Parallel(n_jobs=1)]: Done  38 out of  38 | elapsed:   13.0s remaining:    0.0s\n",
      "[CV] max_depth=4, min_samples_split=9 ................................\n",
      "[CV] ... max_depth=4, min_samples_split=9, score=-5.673, total=   0.5s\n",
      "[Parallel(n_jobs=1)]: Done  39 out of  39 | elapsed:   13.5s remaining:    0.0s\n",
      "[CV] max_depth=4, min_samples_split=9 ................................\n",
      "[CV] ... max_depth=4, min_samples_split=9, score=-5.893, total=   0.5s\n",
      "[Parallel(n_jobs=1)]: Done  40 out of  40 | elapsed:   13.9s remaining:    0.0s\n",
      "[CV] max_depth=6, min_samples_split=3 ................................\n",
      "[CV] ... max_depth=6, min_samples_split=3, score=-3.362, total=   0.7s\n",
      "[Parallel(n_jobs=1)]: Done  41 out of  41 | elapsed:   14.6s remaining:    0.0s\n",
      "[CV] max_depth=6, min_samples_split=3 ................................\n",
      "[CV] ... max_depth=6, min_samples_split=3, score=-4.964, total=   0.7s\n",
      "[Parallel(n_jobs=1)]: Done  42 out of  42 | elapsed:   15.4s remaining:    0.0s\n",
      "[CV] max_depth=6, min_samples_split=3 ................................\n",
      "[CV] ... max_depth=6, min_samples_split=3, score=-7.399, total=   0.7s\n",
      "[Parallel(n_jobs=1)]: Done  43 out of  43 | elapsed:   16.1s remaining:    0.0s\n",
      "[CV] max_depth=6, min_samples_split=3 ................................\n",
      "[CV] ... max_depth=6, min_samples_split=3, score=-5.396, total=   0.7s\n",
      "[Parallel(n_jobs=1)]: Done  44 out of  44 | elapsed:   16.8s remaining:    0.0s\n",
      "[CV] max_depth=6, min_samples_split=3 ................................\n",
      "[CV] ... max_depth=6, min_samples_split=3, score=-5.758, total=   0.7s\n",
      "[Parallel(n_jobs=1)]: Done  45 out of  45 | elapsed:   17.5s remaining:    0.0s\n",
      "[CV] max_depth=6, min_samples_split=5 ................................\n",
      "[CV] ... max_depth=6, min_samples_split=5, score=-3.332, total=   0.6s\n",
      "[Parallel(n_jobs=1)]: Done  46 out of  46 | elapsed:   18.1s remaining:    0.0s\n",
      "[CV] max_depth=6, min_samples_split=5 ................................\n",
      "[CV] ... max_depth=6, min_samples_split=5, score=-4.964, total=   0.6s\n",
      "[Parallel(n_jobs=1)]: Done  47 out of  47 | elapsed:   18.8s remaining:    0.0s\n",
      "[CV] max_depth=6, min_samples_split=5 ................................\n",
      "[CV] ... max_depth=6, min_samples_split=5, score=-7.399, total=   0.7s\n",
      "[Parallel(n_jobs=1)]: Done  48 out of  48 | elapsed:   19.5s remaining:    0.0s\n",
      "[CV] max_depth=6, min_samples_split=5 ................................\n",
      "[CV] ... max_depth=6, min_samples_split=5, score=-5.443, total=   0.6s\n",
      "[Parallel(n_jobs=1)]: Done  49 out of  49 | elapsed:   20.1s remaining:    0.0s\n",
      "[CV] max_depth=6, min_samples_split=5 ................................\n",
      "[CV] ... max_depth=6, min_samples_split=5, score=-5.937, total=   0.7s\n",
      "[Parallel(n_jobs=1)]: Done  50 out of  50 | elapsed:   20.9s remaining:    0.0s\n",
      "[CV] max_depth=6, min_samples_split=8 ................................\n",
      "[CV] ... max_depth=6, min_samples_split=8, score=-3.404, total=   0.7s\n",
      "[Parallel(n_jobs=1)]: Done  51 out of  51 | elapsed:   21.6s remaining:    0.0s\n",
      "[CV] max_depth=6, min_samples_split=8 ................................\n",
      "[CV] ... max_depth=6, min_samples_split=8, score=-4.964, total=   0.7s\n",
      "[Parallel(n_jobs=1)]: Done  52 out of  52 | elapsed:   22.3s remaining:    0.0s\n",
      "[CV] max_depth=6, min_samples_split=8 ................................\n",
      "[CV] ... max_depth=6, min_samples_split=8, score=-7.412, total=   0.7s\n",
      "[Parallel(n_jobs=1)]: Done  53 out of  53 | elapsed:   22.9s remaining:    0.0s\n",
      "[CV] max_depth=6, min_samples_split=8 ................................\n",
      "[CV] ... max_depth=6, min_samples_split=8, score=-5.443, total=   0.7s\n",
      "[Parallel(n_jobs=1)]: Done  54 out of  54 | elapsed:   23.7s remaining:    0.0s\n",
      "[CV] max_depth=6, min_samples_split=8 ................................\n",
      "[CV] ... max_depth=6, min_samples_split=8, score=-5.937, total=   0.7s\n",
      "[Parallel(n_jobs=1)]: Done  55 out of  55 | elapsed:   24.4s remaining:    0.0s\n",
      "[CV] max_depth=6, min_samples_split=9 ................................\n",
      "[CV] ... max_depth=6, min_samples_split=9, score=-3.404, total=   0.6s\n",
      "[Parallel(n_jobs=1)]: Done  56 out of  56 | elapsed:   25.0s remaining:    0.0s\n",
      "[CV] max_depth=6, min_samples_split=9 ................................\n",
      "[CV] ... max_depth=6, min_samples_split=9, score=-4.964, total=   0.7s\n",
      "[Parallel(n_jobs=1)]: Done  57 out of  57 | elapsed:   25.7s remaining:    0.0s\n",
      "[CV] max_depth=6, min_samples_split=9 ................................\n",
      "[CV] ... max_depth=6, min_samples_split=9, score=-7.412, total=   0.7s\n",
      "[Parallel(n_jobs=1)]: Done  58 out of  58 | elapsed:   26.4s remaining:    0.0s\n",
      "[CV] max_depth=6, min_samples_split=9 ................................\n",
      "[CV] ... max_depth=6, min_samples_split=9, score=-5.443, total=   0.7s\n",
      "[Parallel(n_jobs=1)]: Done  59 out of  59 | elapsed:   27.1s remaining:    0.0s\n",
      "[CV] max_depth=6, min_samples_split=9 ................................\n",
      "[CV] ... max_depth=6, min_samples_split=9, score=-5.937, total=   0.7s\n",
      "[Parallel(n_jobs=1)]: Done  60 out of  60 | elapsed:   27.8s remaining:    0.0s\n",
      "[CV] max_depth=8, min_samples_split=3 ................................\n",
      "[CV] ... max_depth=8, min_samples_split=3, score=-3.326, total=   1.0s\n",
      "[Parallel(n_jobs=1)]: Done  61 out of  61 | elapsed:   28.8s remaining:    0.0s\n",
      "[CV] max_depth=8, min_samples_split=3 ................................\n",
      "[CV] ... max_depth=8, min_samples_split=3, score=-4.775, total=   0.9s\n",
      "[Parallel(n_jobs=1)]: Done  62 out of  62 | elapsed:   29.7s remaining:    0.0s\n",
      "[CV] max_depth=8, min_samples_split=3 ................................\n",
      "[CV] ... max_depth=8, min_samples_split=3, score=-7.633, total=   0.9s\n",
      "[Parallel(n_jobs=1)]: Done  63 out of  63 | elapsed:   30.6s remaining:    0.0s\n",
      "[CV] max_depth=8, min_samples_split=3 ................................\n",
      "[CV] ... max_depth=8, min_samples_split=3, score=-5.134, total=   0.9s\n",
      "[Parallel(n_jobs=1)]: Done  64 out of  64 | elapsed:   31.5s remaining:    0.0s\n",
      "[CV] max_depth=8, min_samples_split=3 ................................\n",
      "[CV] ... max_depth=8, min_samples_split=3, score=-5.365, total=   0.9s\n",
      "[Parallel(n_jobs=1)]: Done  65 out of  65 | elapsed:   32.4s remaining:    0.0s\n",
      "[CV] max_depth=8, min_samples_split=5 ................................\n",
      "[CV] ... max_depth=8, min_samples_split=5, score=-3.319, total=   0.9s\n",
      "[Parallel(n_jobs=1)]: Done  66 out of  66 | elapsed:   33.3s remaining:    0.0s\n",
      "[CV] max_depth=8, min_samples_split=5 ................................\n",
      "[CV] ... max_depth=8, min_samples_split=5, score=-4.818, total=   1.0s\n",
      "[Parallel(n_jobs=1)]: Done  67 out of  67 | elapsed:   34.3s remaining:    0.0s\n",
      "[CV] max_depth=8, min_samples_split=5 ................................\n",
      "[CV] ... max_depth=8, min_samples_split=5, score=-7.640, total=   0.9s\n",
      "[Parallel(n_jobs=1)]: Done  68 out of  68 | elapsed:   35.2s remaining:    0.0s\n",
      "[CV] max_depth=8, min_samples_split=5 ................................\n",
      "[CV] ... max_depth=8, min_samples_split=5, score=-5.218, total=   1.0s\n",
      "[Parallel(n_jobs=1)]: Done  69 out of  69 | elapsed:   36.2s remaining:    0.0s\n",
      "[CV] max_depth=8, min_samples_split=5 ................................\n",
      "[CV] ... max_depth=8, min_samples_split=5, score=-5.617, total=   0.9s\n",
      "[Parallel(n_jobs=1)]: Done  70 out of  70 | elapsed:   37.2s remaining:    0.0s\n",
      "[CV] max_depth=8, min_samples_split=8 ................................\n",
      "[CV] ... max_depth=8, min_samples_split=8, score=-3.315, total=   0.9s\n",
      "[Parallel(n_jobs=1)]: Done  71 out of  71 | elapsed:   38.1s remaining:    0.0s\n",
      "[CV] max_depth=8, min_samples_split=8 ................................\n",
      "[CV] ... max_depth=8, min_samples_split=8, score=-4.821, total=   0.9s\n",
      "[Parallel(n_jobs=1)]: Done  72 out of  72 | elapsed:   39.0s remaining:    0.0s\n",
      "[CV] max_depth=8, min_samples_split=8 ................................\n",
      "[CV] ... max_depth=8, min_samples_split=8, score=-7.676, total=   0.9s\n",
      "[Parallel(n_jobs=1)]: Done  73 out of  73 | elapsed:   39.9s remaining:    0.0s\n",
      "[CV] max_depth=8, min_samples_split=8 ................................\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ... max_depth=8, min_samples_split=8, score=-5.265, total=   0.9s\n",
      "[Parallel(n_jobs=1)]: Done  74 out of  74 | elapsed:   40.8s remaining:    0.0s\n",
      "[CV] max_depth=8, min_samples_split=8 ................................\n",
      "[CV] ... max_depth=8, min_samples_split=8, score=-5.675, total=   0.9s\n",
      "[Parallel(n_jobs=1)]: Done  75 out of  75 | elapsed:   41.7s remaining:    0.0s\n",
      "[CV] max_depth=8, min_samples_split=9 ................................\n",
      "[CV] ... max_depth=8, min_samples_split=9, score=-3.313, total=   1.0s\n",
      "[Parallel(n_jobs=1)]: Done  76 out of  76 | elapsed:   42.7s remaining:    0.0s\n",
      "[CV] max_depth=8, min_samples_split=9 ................................\n",
      "[CV] ... max_depth=8, min_samples_split=9, score=-4.819, total=   1.0s\n",
      "[Parallel(n_jobs=1)]: Done  77 out of  77 | elapsed:   43.7s remaining:    0.0s\n",
      "[CV] max_depth=8, min_samples_split=9 ................................\n",
      "[CV] ... max_depth=8, min_samples_split=9, score=-7.665, total=   1.0s\n",
      "[Parallel(n_jobs=1)]: Done  78 out of  78 | elapsed:   44.6s remaining:    0.0s\n",
      "[CV] max_depth=8, min_samples_split=9 ................................\n",
      "[CV] ... max_depth=8, min_samples_split=9, score=-5.220, total=   1.0s\n",
      "[Parallel(n_jobs=1)]: Done  79 out of  79 | elapsed:   45.6s remaining:    0.0s\n",
      "[CV] max_depth=8, min_samples_split=9 ................................\n",
      "[CV] ... max_depth=8, min_samples_split=9, score=-5.675, total=   1.0s\n",
      "[Parallel(n_jobs=1)]: Done  80 out of  80 | elapsed:   46.6s remaining:    0.0s\n",
      "[CV] max_depth=10, min_samples_split=3 ...............................\n",
      "[CV] .. max_depth=10, min_samples_split=3, score=-3.374, total=   1.2s\n",
      "[Parallel(n_jobs=1)]: Done  81 out of  81 | elapsed:   47.7s remaining:    0.0s\n",
      "[CV] max_depth=10, min_samples_split=3 ...............................\n",
      "[CV] .. max_depth=10, min_samples_split=3, score=-4.692, total=   1.1s\n",
      "[Parallel(n_jobs=1)]: Done  82 out of  82 | elapsed:   48.9s remaining:    0.0s\n",
      "[CV] max_depth=10, min_samples_split=3 ...............................\n",
      "[CV] .. max_depth=10, min_samples_split=3, score=-7.744, total=   1.2s\n",
      "[Parallel(n_jobs=1)]: Done  83 out of  83 | elapsed:   50.1s remaining:    0.0s\n",
      "[CV] max_depth=10, min_samples_split=3 ...............................\n",
      "[CV] .. max_depth=10, min_samples_split=3, score=-5.361, total=   1.2s\n",
      "[Parallel(n_jobs=1)]: Done  84 out of  84 | elapsed:   51.3s remaining:    0.0s\n",
      "[CV] max_depth=10, min_samples_split=3 ...............................\n",
      "[CV] .. max_depth=10, min_samples_split=3, score=-5.502, total=   1.2s\n",
      "[Parallel(n_jobs=1)]: Done  85 out of  85 | elapsed:   52.5s remaining:    0.0s\n",
      "[CV] max_depth=10, min_samples_split=5 ...............................\n",
      "[CV] .. max_depth=10, min_samples_split=5, score=-3.392, total=   1.1s\n",
      "[Parallel(n_jobs=1)]: Done  86 out of  86 | elapsed:   53.6s remaining:    0.0s\n",
      "[CV] max_depth=10, min_samples_split=5 ...............................\n",
      "[CV] .. max_depth=10, min_samples_split=5, score=-4.711, total=   1.2s\n",
      "[Parallel(n_jobs=1)]: Done  87 out of  87 | elapsed:   54.8s remaining:    0.0s\n",
      "[CV] max_depth=10, min_samples_split=5 ...............................\n",
      "[CV] .. max_depth=10, min_samples_split=5, score=-7.753, total=   1.2s\n",
      "[Parallel(n_jobs=1)]: Done  88 out of  88 | elapsed:   56.0s remaining:    0.0s\n",
      "[CV] max_depth=10, min_samples_split=5 ...............................\n",
      "[CV] .. max_depth=10, min_samples_split=5, score=-5.422, total=   1.2s\n",
      "[Parallel(n_jobs=1)]: Done  89 out of  89 | elapsed:   57.1s remaining:    0.0s\n",
      "[CV] max_depth=10, min_samples_split=5 ...............................\n",
      "[CV] .. max_depth=10, min_samples_split=5, score=-5.759, total=   1.1s\n",
      "[Parallel(n_jobs=1)]: Done  90 out of  90 | elapsed:   58.2s remaining:    0.0s\n",
      "[CV] max_depth=10, min_samples_split=8 ...............................\n",
      "[CV] .. max_depth=10, min_samples_split=8, score=-3.332, total=   1.0s\n",
      "[Parallel(n_jobs=1)]: Done  91 out of  91 | elapsed:   59.2s remaining:    0.0s\n",
      "[CV] max_depth=10, min_samples_split=8 ...............................\n",
      "[CV] .. max_depth=10, min_samples_split=8, score=-4.717, total=   1.0s\n",
      "[Parallel(n_jobs=1)]: Done  92 out of  92 | elapsed:  1.0min remaining:    0.0s\n",
      "[CV] max_depth=10, min_samples_split=8 ...............................\n",
      "[CV] .. max_depth=10, min_samples_split=8, score=-7.736, total=   1.1s\n",
      "[Parallel(n_jobs=1)]: Done  93 out of  93 | elapsed:  1.0min remaining:    0.0s\n",
      "[CV] max_depth=10, min_samples_split=8 ...............................\n",
      "[CV] .. max_depth=10, min_samples_split=8, score=-5.443, total=   1.0s\n",
      "[Parallel(n_jobs=1)]: Done  94 out of  94 | elapsed:  1.0min remaining:    0.0s\n",
      "[CV] max_depth=10, min_samples_split=8 ...............................\n",
      "[CV] .. max_depth=10, min_samples_split=8, score=-5.867, total=   1.1s\n",
      "[Parallel(n_jobs=1)]: Done  95 out of  95 | elapsed:  1.1min remaining:    0.0s\n",
      "[CV] max_depth=10, min_samples_split=9 ...............................\n",
      "[CV] .. max_depth=10, min_samples_split=9, score=-3.330, total=   1.0s\n",
      "[Parallel(n_jobs=1)]: Done  96 out of  96 | elapsed:  1.1min remaining:    0.0s\n",
      "[CV] max_depth=10, min_samples_split=9 ...............................\n",
      "[CV] .. max_depth=10, min_samples_split=9, score=-4.723, total=   1.0s\n",
      "[Parallel(n_jobs=1)]: Done  97 out of  97 | elapsed:  1.1min remaining:    0.0s\n",
      "[CV] max_depth=10, min_samples_split=9 ...............................\n",
      "[CV] .. max_depth=10, min_samples_split=9, score=-7.730, total=   1.0s\n",
      "[Parallel(n_jobs=1)]: Done  98 out of  98 | elapsed:  1.1min remaining:    0.0s\n",
      "[CV] max_depth=10, min_samples_split=9 ...............................\n",
      "[CV] .. max_depth=10, min_samples_split=9, score=-5.391, total=   1.0s\n",
      "[Parallel(n_jobs=1)]: Done  99 out of  99 | elapsed:  1.1min remaining:    0.0s\n",
      "[CV] max_depth=10, min_samples_split=9 ...............................\n",
      "[CV] .. max_depth=10, min_samples_split=9, score=-5.825, total=   1.0s\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:  1.1min finished\n",
      "{'max_depth': 8, 'min_samples_split': 3} -5.246771815977212\n"
     ]
    }
   ],
   "source": [
    "# YOUR CODE HERE\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "params = {\"max_depth\":[2,4,6,8,10],'min_samples_split':[3,5,8,9]}\n",
    "\n",
    "mytree = MyDecisionTreeRegressor()\n",
    "grid_search = GridSearchCV(mytree, params, cv = 5, scoring = 'neg_root_mean_squared_error')\n",
    "grid_search.fit(X_train,y_train)\n",
    "print(grid_search.best_params_, grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 6 <a id=\"task6\"></a>  (2 points)\n",
    "\n",
    "Recall definition of bias and variance:\n",
    "$$\n",
    "\\text{Bias}^2 = \\mathbb{E}_{p(x, y)} \\left[  (f(x) - \\mathbb{E}_{\\mathbb{X}}a_{\\mathbb{X}}(x))^2 \\right] \\\\\n",
    "\\text{Variance} = \\mathbb{E}_{p(x, y)} \\left[  \\mathbb{V}_{\\mathbb{X}}( a_{\\mathbb{X}}(x))  \\right]\n",
    "$$\n",
    "\n",
    "We wil now use use the following algorithm to estimate bias and variance:\n",
    "\n",
    "1. Use bootsrap to create `n_iter` samples from the original dataset: $X_1, \\dots, X_{n_iter}$\n",
    "2. For each bootstrapped sample define out-of-bag (OOB) sample $Z_1, \\dots, Z_{n_iter}$, which contain all the observations, which did not appear in the corresponding boostraped sample\n",
    "3. Fit the model on $X_i$s and compute predictions on $Z_i$s\n",
    "4. For a given *object* $n$:\n",
    "     - bias^2: squared difference between true value $y_n$ and average prediction (average over the algorithms, for which $n$ was in OOB)\n",
    "     - variance: variance of the prediction (predictions of the algorithms, for which $n$ was in OOB)\n",
    "5. Average bias^2 and variance over all the points\n",
    "    \n",
    "**Implement `get_bias_variance` function, using the algorithm above**\n",
    "\n",
    "*Note:*  You can only use 1 loop (for bootsrap iterations). All other operations should be vectorized. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bias_variance(estimator, x, y, n_iter):\n",
    "    \"\"\" \n",
    "    Calculate bias and variance of the `estimator`. Using a given dataset and bootstrap with `n_iter` samples. \n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    x : ndarray, shape (n_samples, n_features)\n",
    "        The input samples.\n",
    "    y : ndarray, shape (n_samples, n_features)\n",
    "        The input samples.\n",
    "    n_iter: int\n",
    "        Number of samples in \n",
    "    Returns\n",
    "    -------\n",
    "    bias2 : float, \n",
    "        Estiamted squared bias\n",
    "    variance : float, \n",
    "        Estiamted variance\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    varr = np.array([])\n",
    "    bias2 = np.array([])\n",
    "    pred = np.empty((len(x), 1))\n",
    "    pred [:] = np.nan\n",
    "    for i in range(n_iter):\n",
    "\n",
    "        bootstraps_indexes = np.random.choice(len(y),len(y))\n",
    "        rng = np.arange(len(y))\n",
    "        no_brutstrap_indexes = rng[~np.isin(rng,bootstraps_indexes)]\n",
    "        \n",
    "        estimator.fit(x[bootstraps_indexes,:],y[bootstraps_indexes])\n",
    "        \n",
    "        y_pred = estimator.predict(x[no_brutstrap_indexes, :])\n",
    "        \n",
    "        tmp = np.empty(len(x))\n",
    "        tmp[:] = np.NaN\n",
    "        np.put(tmp, no_brutstrap_indexes, y_pred)\n",
    "        \n",
    "        pred = np.column_stack((pred, tmp))\n",
    "    pred = np.delete(pred, 1, axis = 1)\n",
    "    var = np.nanvar(pred, axis = 1)\n",
    "    bias = np.square(y - np.nanmean(pred, axis = 1))\n",
    "    return np.nanmean(bias), np.nanmean(var)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-14-f2edab789d4a>:41: RuntimeWarning: Degrees of freedom <= 0 for slice.\n",
      "  var = np.nanvar(pred, axis = 1)\n",
      "<ipython-input-14-f2edab789d4a>:42: RuntimeWarning: Mean of empty slice\n",
      "  bias = np.square(y - np.nanmean(pred, axis = 1))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(19.611659820165986, 7.994894716081735)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test\n",
    "estimator = MyDecisionTreeRegressor(max_depth=8, min_samples_split=15)\n",
    "\n",
    "get_bias_variance(estimator, X_train, y_train, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 7 <a id=\"task7\"></a>  (0.5 points)\n",
    "\n",
    "Compute bias and variance for the trees of different depths. Plot how bias and variance change as depth increases. \n",
    "\n",
    "Comment on what you observe, how does your result correspond to what we have discussed in class?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-14-f2edab789d4a>:41: RuntimeWarning: Degrees of freedom <= 0 for slice.\n",
      "  var = np.nanvar(pred, axis = 1)\n",
      "<ipython-input-14-f2edab789d4a>:42: RuntimeWarning: Mean of empty slice\n",
      "  bias = np.square(y - np.nanmean(pred, axis = 1))\n",
      "<ipython-input-14-f2edab789d4a>:41: RuntimeWarning: Degrees of freedom <= 0 for slice.\n",
      "  var = np.nanvar(pred, axis = 1)\n",
      "<ipython-input-14-f2edab789d4a>:42: RuntimeWarning: Mean of empty slice\n",
      "  bias = np.square(y - np.nanmean(pred, axis = 1))\n",
      "<ipython-input-14-f2edab789d4a>:41: RuntimeWarning: Degrees of freedom <= 0 for slice.\n",
      "  var = np.nanvar(pred, axis = 1)\n",
      "<ipython-input-14-f2edab789d4a>:42: RuntimeWarning: Mean of empty slice\n",
      "  bias = np.square(y - np.nanmean(pred, axis = 1))\n",
      "<ipython-input-14-f2edab789d4a>:41: RuntimeWarning: Degrees of freedom <= 0 for slice.\n",
      "  var = np.nanvar(pred, axis = 1)\n",
      "<ipython-input-14-f2edab789d4a>:42: RuntimeWarning: Mean of empty slice\n",
      "  bias = np.square(y - np.nanmean(pred, axis = 1))\n",
      "<ipython-input-14-f2edab789d4a>:41: RuntimeWarning: Degrees of freedom <= 0 for slice.\n",
      "  var = np.nanvar(pred, axis = 1)\n",
      "<ipython-input-14-f2edab789d4a>:42: RuntimeWarning: Mean of empty slice\n",
      "  bias = np.square(y - np.nanmean(pred, axis = 1))\n",
      "<ipython-input-14-f2edab789d4a>:41: RuntimeWarning: Degrees of freedom <= 0 for slice.\n",
      "  var = np.nanvar(pred, axis = 1)\n",
      "<ipython-input-14-f2edab789d4a>:42: RuntimeWarning: Mean of empty slice\n",
      "  bias = np.square(y - np.nanmean(pred, axis = 1))\n",
      "<ipython-input-14-f2edab789d4a>:41: RuntimeWarning: Degrees of freedom <= 0 for slice.\n",
      "  var = np.nanvar(pred, axis = 1)\n",
      "<ipython-input-14-f2edab789d4a>:42: RuntimeWarning: Mean of empty slice\n",
      "  bias = np.square(y - np.nanmean(pred, axis = 1))\n",
      "<ipython-input-14-f2edab789d4a>:41: RuntimeWarning: Degrees of freedom <= 0 for slice.\n",
      "  var = np.nanvar(pred, axis = 1)\n",
      "<ipython-input-14-f2edab789d4a>:42: RuntimeWarning: Mean of empty slice\n",
      "  bias = np.square(y - np.nanmean(pred, axis = 1))\n",
      "<ipython-input-14-f2edab789d4a>:41: RuntimeWarning: Degrees of freedom <= 0 for slice.\n",
      "  var = np.nanvar(pred, axis = 1)\n",
      "<ipython-input-14-f2edab789d4a>:42: RuntimeWarning: Mean of empty slice\n",
      "  bias = np.square(y - np.nanmean(pred, axis = 1))\n",
      "<ipython-input-14-f2edab789d4a>:41: RuntimeWarning: Degrees of freedom <= 0 for slice.\n",
      "  var = np.nanvar(pred, axis = 1)\n",
      "<ipython-input-14-f2edab789d4a>:42: RuntimeWarning: Mean of empty slice\n",
      "  bias = np.square(y - np.nanmean(pred, axis = 1))\n",
      "<ipython-input-14-f2edab789d4a>:41: RuntimeWarning: Degrees of freedom <= 0 for slice.\n",
      "  var = np.nanvar(pred, axis = 1)\n",
      "<ipython-input-14-f2edab789d4a>:42: RuntimeWarning: Mean of empty slice\n",
      "  bias = np.square(y - np.nanmean(pred, axis = 1))\n",
      "<ipython-input-14-f2edab789d4a>:41: RuntimeWarning: Degrees of freedom <= 0 for slice.\n",
      "  var = np.nanvar(pred, axis = 1)\n",
      "<ipython-input-14-f2edab789d4a>:42: RuntimeWarning: Mean of empty slice\n",
      "  bias = np.square(y - np.nanmean(pred, axis = 1))\n",
      "<ipython-input-14-f2edab789d4a>:41: RuntimeWarning: Degrees of freedom <= 0 for slice.\n",
      "  var = np.nanvar(pred, axis = 1)\n",
      "<ipython-input-14-f2edab789d4a>:42: RuntimeWarning: Mean of empty slice\n",
      "  bias = np.square(y - np.nanmean(pred, axis = 1))\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# YOUR CODE HERE\n",
    "x = []\n",
    "biases = []\n",
    "varss = []\n",
    "\n",
    "for i in range(2,15):\n",
    "    x.append(i)\n",
    "    est = MyDecisionTreeRegressor(max_depth=i, min_samples_split = 10)\n",
    "    bias,var = get_bias_variance(est, X_train, y_train, 8)\n",
    "    biases.append(bias)\n",
    "    varss.append(var)\n",
    "plt.plot(x,biases)\n",
    "plt.plot(x,varss)\n",
    "plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``` Как и ожидалось, bias2 убывает в зависимости от глубины дерева. Дисперсия же должна, вроде как возрастать, но не возрастает почти. Мне кажется, тут дела в том, что мало глубин деревьев изучил, и что на больших глубинах var повышается. ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 8 <a id=\"task8\"></a>  (0.5 points)\n",
    "\n",
    "Let's try to reduce variance with bagging. Use `sklearn.ensemble.BaggingRegressor` to get an ensemble and compute its bias and variance. \n",
    "\n",
    "Answer the following questions:\n",
    " - How bagging should affect bias and variance in theory?\n",
    " - How bias and variance change (if they change) compared to an individual tree in you experiments? \n",
    " - Do your results align with the theory? Why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-14-f2edab789d4a>:41: RuntimeWarning: Degrees of freedom <= 0 for slice.\n",
      "  var = np.nanvar(pred, axis = 1)\n",
      "<ipython-input-14-f2edab789d4a>:42: RuntimeWarning: Mean of empty slice\n",
      "  bias = np.square(y - np.nanmean(pred, axis = 1))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(14.914015112714406, 2.3633103437430556)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import BaggingRegressor\n",
    "\n",
    "bagging = BaggingRegressor(base_estimator=MyDecisionTreeRegressor(max_depth=8, min_samples_split=15), n_estimators=10, random_state=42)\n",
    "get_bias_variance(bagging, X_train, y_train, n_iter=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```В целом, с теорией всё сошлось```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2. More Ensembles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part we will be working with [Thyroid Disease Data Set](https://archive.ics.uci.edu/ml/datasets/thyroid+disease) to solve a classification task. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>on_thyroxine</th>\n",
       "      <th>query_on_thyroxine</th>\n",
       "      <th>on_antithyroid_medication</th>\n",
       "      <th>sick</th>\n",
       "      <th>pregnant</th>\n",
       "      <th>thyroid_surgery</th>\n",
       "      <th>I131_treatment</th>\n",
       "      <th>query_hypothyroid</th>\n",
       "      <th>...</th>\n",
       "      <th>T3</th>\n",
       "      <th>TT4_measured</th>\n",
       "      <th>TT4</th>\n",
       "      <th>T4U_measured</th>\n",
       "      <th>T4U</th>\n",
       "      <th>FTI_measured</th>\n",
       "      <th>FTI</th>\n",
       "      <th>TBG_measured</th>\n",
       "      <th>TBG</th>\n",
       "      <th>referral_source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>41.0</td>\n",
       "      <td>F</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>...</td>\n",
       "      <td>2.5</td>\n",
       "      <td>t</td>\n",
       "      <td>125.0</td>\n",
       "      <td>t</td>\n",
       "      <td>1.14</td>\n",
       "      <td>t</td>\n",
       "      <td>109.0</td>\n",
       "      <td>f</td>\n",
       "      <td>NaN</td>\n",
       "      <td>SVHC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>23.0</td>\n",
       "      <td>F</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>t</td>\n",
       "      <td>102.0</td>\n",
       "      <td>f</td>\n",
       "      <td>NaN</td>\n",
       "      <td>f</td>\n",
       "      <td>NaN</td>\n",
       "      <td>f</td>\n",
       "      <td>NaN</td>\n",
       "      <td>other</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>46.0</td>\n",
       "      <td>M</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>t</td>\n",
       "      <td>109.0</td>\n",
       "      <td>t</td>\n",
       "      <td>0.91</td>\n",
       "      <td>t</td>\n",
       "      <td>120.0</td>\n",
       "      <td>f</td>\n",
       "      <td>NaN</td>\n",
       "      <td>other</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>70.0</td>\n",
       "      <td>F</td>\n",
       "      <td>t</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>...</td>\n",
       "      <td>1.9</td>\n",
       "      <td>t</td>\n",
       "      <td>175.0</td>\n",
       "      <td>f</td>\n",
       "      <td>NaN</td>\n",
       "      <td>f</td>\n",
       "      <td>NaN</td>\n",
       "      <td>f</td>\n",
       "      <td>NaN</td>\n",
       "      <td>other</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>70.0</td>\n",
       "      <td>F</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>...</td>\n",
       "      <td>1.2</td>\n",
       "      <td>t</td>\n",
       "      <td>61.0</td>\n",
       "      <td>t</td>\n",
       "      <td>0.87</td>\n",
       "      <td>t</td>\n",
       "      <td>70.0</td>\n",
       "      <td>f</td>\n",
       "      <td>NaN</td>\n",
       "      <td>SVI</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    age sex on_thyroxine query_on_thyroxine on_antithyroid_medication sick  \\\n",
       "0  41.0   F            f                  f                         f    f   \n",
       "1  23.0   F            f                  f                         f    f   \n",
       "2  46.0   M            f                  f                         f    f   \n",
       "3  70.0   F            t                  f                         f    f   \n",
       "4  70.0   F            f                  f                         f    f   \n",
       "\n",
       "  pregnant thyroid_surgery I131_treatment query_hypothyroid  ...   T3  \\\n",
       "0        f               f              f                 f  ...  2.5   \n",
       "1        f               f              f                 f  ...  2.0   \n",
       "2        f               f              f                 f  ...  NaN   \n",
       "3        f               f              f                 f  ...  1.9   \n",
       "4        f               f              f                 f  ...  1.2   \n",
       "\n",
       "  TT4_measured    TT4 T4U_measured   T4U FTI_measured    FTI  TBG_measured  \\\n",
       "0            t  125.0            t  1.14            t  109.0             f   \n",
       "1            t  102.0            f   NaN            f    NaN             f   \n",
       "2            t  109.0            t  0.91            t  120.0             f   \n",
       "3            t  175.0            f   NaN            f    NaN             f   \n",
       "4            t   61.0            t  0.87            t   70.0             f   \n",
       "\n",
       "  TBG  referral_source  \n",
       "0 NaN             SVHC  \n",
       "1 NaN            other  \n",
       "2 NaN            other  \n",
       "3 NaN            other  \n",
       "4 NaN              SVI  \n",
       "\n",
       "[5 rows x 29 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "import pandas as pd\n",
    "df = pd.read_csv('thyroid_disease.csv')\n",
    "\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(df['Class'])\n",
    "X = df.drop('Class', axis=1)\n",
    "X.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1 <a id=\"task2_1\"></a> (1 point)\n",
    "\n",
    "Let's start with data preprocessing. \n",
    "\n",
    "0. Drop columns, which are not usefull (e.g. a lot of missing values). Motivate your choice. \n",
    "1. Split dataset into train and test\n",
    "2. You've probably noticed that we have both categorical and numerical columns. Here is what you need to do with them:\n",
    "    - Categorical: Fill missing values and apply one-hot-encoding\n",
    "    - Numeric: Fill missing values\n",
    "    \n",
    "Use `ColumnTranformer` to define a single transformer for all the columns in the dataset. It takes as input a list of tuples\n",
    "\n",
    "```\n",
    "ColumnTransformer([\n",
    "    ('name1', transorm1, column_names1),\n",
    "    ('name2', transorm2, column_names2)\n",
    "])\n",
    "```\n",
    "\n",
    "Pay attention to an argument `remainder='passthrough'`. [Here](https://scikit-learn.org/stable/modules/compose.html#column-transformer) you can find some examples of how to use column transformer. \n",
    "    \n",
    "Since we want to apply 2 transformations to categorical feature, it is very convenient to combine them into a `Pipeline`:\n",
    "\n",
    "```\n",
    "double_tranform = make_pipeline(\n",
    "                        transform_1,\n",
    "                        transform_2\n",
    "                        )\n",
    "```\n",
    "\n",
    "P.S. Choose your favourite way to fill missing values. \n",
    "\n",
    "*Hint* Categorical column usually have `dtype = 'object'`. This may help to obtain list of categorical and numerical columns on the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['TBG']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer, make_column_selector\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "\n",
    "# YOUR CODE HERE\n",
    "# define column_transformer \n",
    "columns = X.columns[X.isna().mean() > 0.25].to_list()\n",
    "print(columns)\n",
    "X = X.drop(columns = columns)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size = 0.3, random_state = 42)\n",
    "cat_prep = Pipeline(steps = [('fillna', SimpleImputer(strategy='most_frequent')),('encode', OneHotEncoder())])\n",
    "column_transformer = ColumnTransformer(transformers=[('num', SimpleImputer(strategy='most_frequent'), make_column_selector(dtype_include = 'float')),('cat',cat_prep,make_column_selector(dtype_include='object'))], remainder='passthrough')\n",
    "# Transform the data\n",
    "X_train = column_transformer.fit_transform(X_train)\n",
    "X_test = column_transformer.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2 <a id=\"task2_2\"></a> (0.7 points)\n",
    "\n",
    "Fit and compare 5 different models (use sklearn): Gradient Boosting, Random Forest, Decision Tree, SVM, Logitics Regression\n",
    "    \n",
    "* Choose one classification metric and justify your choice .\n",
    "* Compare the models using score on cross validation. Mind the class balance when choosing the cross validation. (You can read more about different CV strategies [here](https://scikit-learn.org/stable/modules/cross_validation.html#stratified-k-fold))\n",
    "* Which model has the best performance? Which models overfit or underfit?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gbc 0.9905527741934138 0.9302245488359278\n",
      "rfc 0.9905104850502707 0.8993050772221457\n",
      "dtc 0.9271131273724543 0.9182930002602134\n",
      "svc 0.960281304543717 0.5\n",
      "lr 0.9385601329592034 0.815937303883302\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\f.deryabin\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\f.deryabin\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\f.deryabin\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "\n",
    "gbc = GradientBoostingClassifier()\n",
    "rfc = RandomForestClassifier()\n",
    "dtc = DecisionTreeClassifier()\n",
    "svc = SVC()\n",
    "lr = LogisticRegression()\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold, cross_validate\n",
    "from sklearn.metrics import roc_auc_score\n",
    "cv = StratifiedKFold(n_splits= 3)\n",
    "score = {}\n",
    "models = [gbc, rfc,dtc,svc,lr]\n",
    "lables = ['GradientBoostingClassifier', 'RandomForestClassifier','DecisionTreeClassifier','SVC','LogisticRegression']\n",
    "# for i in range(len(lables)):\n",
    "#     score[lables[i]] = cross_validate(models[i], X_train, y_train, scoring='recall', cv= cv, return_estimator=True)\n",
    "# for key in score:\n",
    "#     print(key, score[key]['test_score'].mean(), recall_score(y_test, score[key]['estimator'][0].predict(X_test)))\n",
    "tmp = cross_validate(gbc, X_train, y_train, scoring='roc_auc', cv= cv, return_estimator=True)\n",
    "print('gbc', tmp['test_score'].mean(), roc_auc_score(y_test, tmp['estimator'][0].predict(X_test)))\n",
    "tmp = cross_validate(rfc, X_train, y_train, scoring='roc_auc', cv= cv, return_estimator=True)\n",
    "print('rfc', tmp['test_score'].mean(), roc_auc_score(y_test, tmp['estimator'][0].predict(X_test)))\n",
    "tmp = cross_validate(dtc, X_train, y_train, scoring='roc_auc', cv= cv, return_estimator=True)\n",
    "print('dtc', tmp['test_score'].mean(), roc_auc_score(y_test, tmp['estimator'][0].predict(X_test)))\n",
    "tmp = cross_validate(svc, X_train, y_train, scoring='roc_auc', cv= cv, return_estimator=True)\n",
    "print('svc', tmp['test_score'].mean(), roc_auc_score(y_test, tmp['estimator'][0].predict(X_test)))\n",
    "tmp = cross_validate(lr, X_train, y_train, scoring='roc_auc', cv= cv, return_estimator=True)\n",
    "print('lr', tmp['test_score'].mean(), roc_auc_score(y_test, tmp['estimator'][0].predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```Наибольшие отклонения у SVC. Она переобучилась. У остальных все показатели более или менее совпадают, причем находятся на довольно хорошем уровне, а значит модли хорошие```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3 <a id=\"task2_3\"></a> (0.5 points)\n",
    "\n",
    "More Gradient Boosting. Choose one of the tree popular boosting implementations (xgboost, lightgbm, catboost). Select hyperparameters (number of trees, learning rate, depth) on cross-validation and compare with the methods from the previous task. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```your comments here```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4 <a id=\"task2_4\"></a> (0.7 points)\n",
    "\n",
    "Now let's train more fancy ensembles:\n",
    "\n",
    "* Bagging with decision trees as base estimators\n",
    "* Bagging with gradient boosting (with large amount of trees, >100) as base estimators\n",
    "* [Voting classifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.VotingClassifier.html#sklearn.ensemble.VotingClassifier) \n",
    "* [Stacking Classifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.StackingClassifier.html#sklearn.ensemble.StackingClassifier) with Logistic Regression as a final model\n",
    "* [Stacking Classifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.StackingClassifier.html#sklearn.ensemble.StackingClassifier) with Gradeint Boosting as a final model\n",
    "\n",
    "\n",
    "If not stated in the task, feel free to tune / choose hyperparameters and base models.\n",
    "\n",
    "Answer the questions:\n",
    "* Which model has the best performance?\n",
    "* Does bagging reduce overfiting of the gradient boosting with large amount of trees? \n",
    "* What is the difference between voting and staking? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```your comments here```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 5 <a id=\"task2_5\"></a> (0.1 points)\n",
    "\n",
    "Report the test score for the best model, that you were able to train. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
