{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# HSE 2021: Mathematical Methods for Data Analysis\n",
    "\n",
    "## Homework 4\n",
    "\n",
    "**Warning 1**: You have 2 weeks for this assignemnt.  **it is better to start early (!)**\n",
    "\n",
    "**Warning 2**: it is critical to describe and explain what you are doing and why, use markdown cells\n",
    "\n",
    "\n",
    "### Contents\n",
    "\n",
    "#### Decision Trees - 7 points\n",
    "* [Task 1](#task1) (0.5 points)\n",
    "* [Task 2](#task2) (0.5 points)\n",
    "* [Task 3](#task3) (2 points)\n",
    "* [Task 4](#task4) (0.5 points)\n",
    "* [Task 5](#task5) (0.5 points)\n",
    "* [Task 6](#task6) (2 points)\n",
    "* [Task 7](#task7) (0.5 points)\n",
    "* [Task 8](#task8) (0.5 points)\n",
    "\n",
    "#### Ensembles - 3 points\n",
    "* [Task 1](#task2_1) (1 point)\n",
    "* [Task 2](#task2_2) (0.7 points)\n",
    "* [Task 3](#task2_3) (0.5 points)\n",
    "* [Task 4](#task2_4) (0.7 points)\n",
    "* [Task 5](#task2_5) (0.1 points)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (11, 5)\n",
    "%matplotlib inline"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Part 1. Decision Tree Regressor"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "In this task you will be implementing decision tree for the regression by hand."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Task 1 <a id=\"task1\"></a> (0.5 points)\n",
    "\n",
    "Here you should implement the function `H()` which calculates impurity criterion. We will be training regression tree, and will take mean absolute deviation as impurity criterion.\n",
    "\n",
    "* You cannot use loops\n",
    "* If `y` is empty, the function should return 0"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [],
   "source": [
    "def H(y):\n",
    "    \"\"\"\n",
    "    Calculate impurity criterion\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    y : np.array\n",
    "        array of objects target values in the node\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    H(R) : float\n",
    "        Impurity in the node (measuread by variance)\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    if len(y) == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return np.sum(np.power((y - np.mean(y)), 2)) / len(y) #variance"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [],
   "source": [
    "# Test the function\n",
    "assert np.allclose(H(np.array([4, 2, 2, 2])), 0.75)\n",
    "assert np.allclose(H(np.array([])), 0.0)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Task 2 <a id=\"task2\"></a>  (0.5 points)\n",
    "\n",
    "To find the best split in the node we need to calculate the cost function. Denote:\n",
    "- `R` all the object in the node\n",
    "- `j` index of the feature selected for the split\n",
    "- `t` threshold\n",
    "- `R_l` and `R_r` objects in the left and right child nodes correspondingly\n",
    "\n",
    "We get the following cost function:\n",
    "\n",
    "$$\n",
    "Q(R, j, t) =\\frac{|R_\\ell|}{|R|}H(R_\\ell) + \\frac{|R_r|}{|R|}H(R_r) \\to \\min_{j, t},\n",
    "$$\n",
    "\n",
    "Implement the function `Q`, which should calculate value of the cost function for a given feature and threshold."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [],
   "source": [
    "def Q(X, y, j, t):\n",
    "    \"\"\"\n",
    "    Calculate cost function\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : ndarray\n",
    "        array of objects in the node\n",
    "    y : ndarray\n",
    "        array of target values in the node\n",
    "    j : int\n",
    "        feature index (column in X)\n",
    "    t : float\n",
    "        threshold\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Q : float\n",
    "        Value of the cost function\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    x_col = X[:, j]\n",
    "\n",
    "    R_l = y[x_col <= t]\n",
    "    R_r = y[x_col > t]\n",
    "    tmp_r = H(R_r) / len(y)\n",
    "    tmp_l = H(R_l) / len(y)\n",
    "    Q = 0\n",
    "    return len(R_l) * tmp_l + len(R_r) * tmp_r\n",
    "    # return Q"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Task 3 <a id=\"task3\"></a>  (2 points)\n",
    "\n",
    "Now, let's implement `MyDecisionTreeRegressor` class. More specifically, you need to implement the following methods:\n",
    "\n",
    "- `best_split`\n",
    "- `grow_tree`\n",
    "- `get_prediction`\n",
    "\n",
    "Also, please add `min_samples_leaf` parameter to your class\n",
    "\n",
    "Read docstrings for more details. Do not forget to use function `Q` implemented above, when finding the `best_split`"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [],
   "source": [
    "class Node(object):\n",
    "    \"\"\"\n",
    "    Class for a decision tree node.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    right : Node() or None\n",
    "        Right child\n",
    "    right : Node() or None\n",
    "        Left child\n",
    "    threshold: float\n",
    "\n",
    "    column: int\n",
    "\n",
    "    depth: int\n",
    "\n",
    "    prediction: float\n",
    "        prediction of the target value in the node\n",
    "        (average values calculated on a train dataset)\n",
    "    is_terminal:bool\n",
    "        indicates whether it is a terminal node (leaf) or not\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.right = None\n",
    "        self.left = None\n",
    "        self.threshold = None\n",
    "        self.column = None\n",
    "        self.depth = None\n",
    "        self.is_terminal = False\n",
    "        self.prediction = None\n",
    "\n",
    "    def __repr__(self):\n",
    "        if self.is_terminal:\n",
    "            node_desc = 'Pred: {:.2f}'.format(self.prediction)\n",
    "        else:\n",
    "            node_desc = 'Col {}, t {:.2f}, Pred: {:.2f}'. \\\n",
    "            format(self.column, self.threshold, self.prediction)\n",
    "        return node_desc"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, RegressorMixin\n",
    "from sklearn.utils.validation import check_X_y, check_array, check_is_fitted\n",
    "\n",
    "class MyDecisionTreeRegressor(RegressorMixin, BaseEstimator):\n",
    "    \"\"\"\n",
    "    Class for a Decision Tree Regressor.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    max_depth : int\n",
    "        Max depth of a decision tree.\n",
    "    min_samples_split : int\n",
    "        Minimal number of samples (objects) in a node to make a split.\n",
    "    \"\"\"\n",
    "    def __init__(self, max_depth=3, min_samples_split=2, min_samples_leaf=1):\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.min_samples_leaf = min_samples_leaf\n",
    "\n",
    "    def best_split(self, X, y):\n",
    "        \"\"\"\n",
    "        Find the best split in terms of Q of data in a given decision tree node.\n",
    "        Try all features and thresholds.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : ndarray, shape (n_objects, n_features)\n",
    "            Objects in the parent node\n",
    "        y : ndarray, shape (n_objects, )\n",
    "            1D array with the object labels.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        best_split_column : int\n",
    "            Index of the best split column\n",
    "        best_threshold : float\n",
    "            The best split condition.\n",
    "        X_left : ndarray, shape (n_objects_l, n_features)\n",
    "            Objects in the left child\n",
    "        y_left : ndarray, shape (n_objects_l, )\n",
    "            Objects labels in the left child.\n",
    "        X_right : ndarray, shape (n_objects_r, n_features)\n",
    "            Objects in the right child\n",
    "        y_right : ndarray, shape (n_objects_r, )\n",
    "            Objects labels in the right child.\n",
    "        \"\"\"\n",
    "\n",
    "        # To store best split parameters\n",
    "        best_split_column = None\n",
    "        best_threshold = None\n",
    "        best_cost = H(y)\n",
    "        best_information_gain = -999\n",
    "\n",
    "        # Data impurity before the split\n",
    "        # impurity = 0.8 (example)\n",
    "\n",
    "        # For each column in X ...\n",
    "        for split_column in range(X.shape[1]):\n",
    "\n",
    "            # Select values of the column\n",
    "            x_col = X[:, split_column]\n",
    "            # x_col = [2.6, 1.3, 0.5, ...] (example)\n",
    "\n",
    "            # For each value in the column ...\n",
    "            for i_x in range(0, len(x_col)):\n",
    "\n",
    "                # Take the value as a threshold for a split\n",
    "                threshold = x_col[i_x]\n",
    "                # threshold = 1.3 (example)\n",
    "\n",
    "                # Make the split into right and left childs\n",
    "                information_gain = best_cost - Q(X, y, split_column, threshold)\n",
    "                # information_gain = 0.2 (example)\n",
    "\n",
    "                # Is this information_gain the best?\n",
    "                if information_gain > best_information_gain:\n",
    "                    best_split_column = split_column\n",
    "                    best_threshold = threshold\n",
    "                    best_information_gain = information_gain\n",
    "\n",
    "        # If no split available\n",
    "        if best_information_gain == -999:\n",
    "            return None, None, None, None, None, None\n",
    "\n",
    "        # Take the best split parameters and make this split\n",
    "        x_col = X[:, best_split_column]\n",
    "        X_left = X[x_col <= best_threshold, :]\n",
    "        y_left = y[x_col <= best_threshold]\n",
    "        X_right = X[x_col > best_threshold, :]\n",
    "        y_right = y[x_col > best_threshold]\n",
    "\n",
    "        return best_split_column, best_threshold, X_left, y_left, X_right, y_right\n",
    "\n",
    "    def is_terminal(self, node, y):\n",
    "        \"\"\"\n",
    "        Check terminality conditions based on `max_depth`,\n",
    "        `min_samples_split` parameters for a given node.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        node : Node,\n",
    "\n",
    "        y : ndarray, shape (n_objects, )\n",
    "            Object labels.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        Is_termial : bool\n",
    "            If True, node is terminal\n",
    "        \"\"\"\n",
    "        if node.depth >= self.max_depth:\n",
    "            return True\n",
    "        if len(y) < self.min_samples_split:\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "    def grow_tree(self, node, X, y):\n",
    "        \"\"\"\n",
    "        Reccurently grow the tree from the `node` using a `X` and `y` as a dataset:\n",
    "         - check terminality conditions\n",
    "         - find best split if node is not terminal\n",
    "         - add child nodes to the node\n",
    "         - call the function recursively for the added child nodes\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        node : Node() object\n",
    "            Current node of the decision tree.\n",
    "        X : ndarray, shape (n_objects, n_features)\n",
    "            Objects\n",
    "        y : ndarray, shape (n_objects)\n",
    "            Labels\n",
    "        \"\"\"\n",
    "\n",
    "        if self.is_terminal(node, y):\n",
    "            node.is_terminal =True\n",
    "            return\n",
    "\n",
    "        # Check termination conditions\n",
    "        if len(np.unique(y)) == 1:\n",
    "            node.is_terminal = True\n",
    "            return\n",
    "\n",
    "        # Make best split\n",
    "        split_column, threshold, X_left, y_left, X_right, y_right = self.best_split(X, y) # Make a split\n",
    "        # split_column = 2 (exmaple) column index of the split\n",
    "        # threshold = 2.74 (example) split_column > threshold\n",
    "\n",
    "        # Check additional termination conditions\n",
    "        if split_column is None:\n",
    "            node.is_terminal = True\n",
    "            return\n",
    "        if len(X_left) < self.min_samples_leaf or len(X_right) < self.min_samples_leaf:  # min_samples_leaf check\n",
    "            node.is_terminal = True\n",
    "            return\n",
    "\n",
    "\n",
    "        # Add split parameters into the current node\n",
    "        node.column = split_column\n",
    "        node.threshold = threshold\n",
    "\n",
    "        # Create a left child of the current node\n",
    "        node.left = Node()\n",
    "        node.left.depth = node.depth + 1\n",
    "        node.left.prediction = np.mean(y_left)\n",
    "\n",
    "        # Create a right child of the current node\n",
    "        node.right = Node()\n",
    "        node.right.depth = node.depth + 1\n",
    "        node.right.prediction = np.mean(y_right)\n",
    "\n",
    "        # Make splits for the left and right nodes\n",
    "        self.grow_tree(node.right, X_right, y_right)\n",
    "        self.grow_tree(node.left, X_left, y_left)\n",
    "\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Fit the Decision Tree Regressor.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : ndarray, shape (n_samples, n_features)\n",
    "            The input samples.\n",
    "        y : ndarray, shape (n_samples,) or (n_samples, n_outputs)\n",
    "            The target values.\n",
    "        Returns\n",
    "        -------\n",
    "        self : object\n",
    "            Returns self.\n",
    "        \"\"\"\n",
    "        X, y = check_X_y(X, y, accept_sparse=False)\n",
    "        self.is_fitted_ = True\n",
    "\n",
    "        # Initialize the tree (root node)\n",
    "        self.tree_ = Node()\n",
    "        self.tree_.depth = 0\n",
    "        self.tree_.prediction = np.mean(y)\n",
    "\n",
    "        # Grow the tree\n",
    "        self.grow_tree(self.tree_, X, y)\n",
    "        return self\n",
    "\n",
    "    def get_prediction(self, node, x):\n",
    "        \"\"\"\n",
    "        Get prediction for an object `x`\n",
    "            - Return prediction of the `node` if it is terminal\n",
    "            - Otherwise, recursively call the function to get\n",
    "            predictions of the proper child\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        node : Node() object\n",
    "            Current node of the decision tree.\n",
    "        x : ndarray, shape (n_features,)\n",
    "            Array of feature values of one object.\n",
    "        Returns\n",
    "        -------\n",
    "        y_pred : float\n",
    "            Prediction for an object x\n",
    "        \"\"\"\n",
    "        # YOUR CODE HERE\n",
    "\n",
    "        if node.is_terminal:\n",
    "            return node.prediction\n",
    "        if x[node.column] > node.threshold:\n",
    "            y_pred = self.get_prediction(node.right, x)\n",
    "        else:\n",
    "            y_pred = self.get_prediction(node.left, x)\n",
    "        return y_pred\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Get prediction for each object in X\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : ndarray, shape (n_samples, n_features)\n",
    "            The input samples.\n",
    "        Returns\n",
    "        -------\n",
    "        y : ndarray, shape (n_samples,)\n",
    "            Returns predictions.\n",
    "        \"\"\"\n",
    "        # Check input and that `fit` had been called\n",
    "        X = check_array(X, accept_sparse=False)\n",
    "        check_is_fitted(self, 'is_fitted_')\n",
    "\n",
    "        # Get predictions\n",
    "        y_predicted = []\n",
    "        for x in X:\n",
    "            y_curr = self.get_prediction(self.tree_, x)\n",
    "            y_predicted.append(y_curr)\n",
    "        return np.array(y_predicted)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\anaconda\\lib\\site-packages\\sklearn\\utils\\estimator_checks.py:3608: FutureWarning: As of scikit-learn 0.23, estimators should expose a n_features_in_ attribute, unless the 'no_validation' tag is True. This attribute should be equal to the number of features passed to the fit method. An error will be raised from version 1.0 (renaming of 0.25) when calling check_estimator(). See SLEP010: https://scikit-learn-enhancement-proposals.readthedocs.io/en/latest/slep010/proposal.html\n",
      "  warnings.warn(\n",
      "C:\\anaconda\\lib\\site-packages\\sklearn\\utils\\estimator_checks.py:3652: FutureWarning: As of scikit-learn 0.23, estimators should have a 'requires_y' tag set to the appropriate value. The default value of the tag is False. An error will be raised from version 1.0 when calling check_estimator() if the tag isn't properly set.\n",
      "  warnings.warn(warning_msg, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# check yourself\n",
    "from sklearn.utils.estimator_checks import check_estimator\n",
    "\n",
    "check_estimator(MyDecisionTreeRegressor())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Task 4 <a id=\"task4\"></a>  (0.5 points)\n",
    "\n",
    "Load boston dataset and split it on the train ($75\\%$) and test ($25\\%$). Fit Decision Tree of depth 1 and make the following plot:\n",
    "\n",
    "- Scatter plot of the traning points (selected for split feature on the x-axis, target variable on the y-axis)\n",
    "- Fitted model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\anaconda\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function load_boston is deprecated; `load_boston` is deprecated in 1.0 and will be removed in 1.2.\n",
      "\n",
      "    The Boston housing prices dataset has an ethical problem. You can refer to\n",
      "    the documentation of this function for further details.\n",
      "\n",
      "    The scikit-learn maintainers therefore strongly discourage the use of this\n",
      "    dataset unless the purpose of the code is to study and educate about\n",
      "    ethical issues in data science and machine learning.\n",
      "\n",
      "    In this special case, you can fetch the dataset from the original\n",
      "    source::\n",
      "\n",
      "        import pandas as pd\n",
      "        import numpy as np\n",
      "\n",
      "\n",
      "        data_url = \"http://lib.stat.cmu.edu/datasets/boston\"\n",
      "        raw_df = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22, header=None)\n",
      "        data = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])\n",
      "        target = raw_df.values[1::2, 2]\n",
      "\n",
      "    Alternative datasets include the California housing dataset (i.e.\n",
      "    :func:`~sklearn.datasets.fetch_california_housing`) and the Ames housing\n",
      "    dataset. You can load the datasets as follows::\n",
      "\n",
      "        from sklearn.datasets import fetch_california_housing\n",
      "        housing = fetch_california_housing()\n",
      "\n",
      "    for the California housing dataset and::\n",
      "\n",
      "        from sklearn.datasets import fetch_openml\n",
      "        housing = fetch_openml(name=\"house_prices\", as_frame=True)\n",
      "\n",
      "    for the Ames housing dataset.\n",
      "    \n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "data = load_boston()\n",
    "features_train, features_test, target_train, target_test = train_test_split(data.data, data.target, test_size = 0.25, random_state = 12345)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "В методе fit пришлось поменять <code>max_depth = 0</code>, иначе пришлось бы тут указывать глубину не 1, как в условии, а 2 или больше."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [
    {
     "data": {
      "text/plain": "MyDecisionTreeRegressor(max_depth=1)"
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_tree = MyDecisionTreeRegressor(max_depth=1)\n",
    "model_tree.fit(features_train, target_train)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 720x504 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_tree_predicted = model_tree.predict(features_train)\n",
    "tmp_col = features_train[:, model_tree.tree_.column]\n",
    "\n",
    "plt.gcf().set_size_inches(10, 7);\n",
    "\n",
    "plt.scatter(features_train[tmp_col <= model_tree.tree_.threshold][:, model_tree.tree_.column], target_train[tmp_col <= model_tree.tree_.threshold], color='red')\n",
    "\n",
    "plt.scatter(features_train[tmp_col > model_tree.tree_.threshold][:, model_tree.tree_.column], target_train[tmp_col > model_tree.tree_.threshold], color='green')\n",
    "\n",
    "plt.scatter(features_train[tmp_col <= model_tree.tree_.threshold][:, model_tree.tree_.column], model_tree_predicted[tmp_col <= model_tree.tree_.threshold], color ='black')\n",
    "\n",
    "plt.scatter(features_train[tmp_col > model_tree.tree_.threshold][:, model_tree.tree_.column], model_tree_predicted[tmp_col > model_tree.tree_.threshold], color ='yellow')\n",
    "\n",
    "plt.axvline(model_tree.tree_.threshold, color = 'blue');"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Task 5 <a id=\"task5\"></a>  (0.5 points)\n",
    "\n",
    "Keep working with boston dataset.\n",
    "- Use `GridSearchCV` to find the best hyperparameters among [`max_depth`, `min_samples_leaf`] on 5-Fold cross-validation\n",
    "- Train the model with the best set of hyperparameters on the whole train dataset.\n",
    "- Report `MAE` on test dataset and hyperparameters of the best estimator."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Лучшие параметры: {'max_depth': 5, 'min_samples_split': 5}\n",
    "\n",
    "Лучший счет: -4.610205735534387\n",
    "\n",
    "CPU times: total: 14min 47s\n",
    "Wall time: 14min 50s\n",
    "\n",
    "\n",
    "2 15\n",
    "2 10"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 15 candidates, totalling 75 fits\n",
      "[CV 1/5] END ..max_depth=1, min_samples_leaf=1;, score=-5.457 total time=   0.1s\n",
      "[CV 2/5] END ..max_depth=1, min_samples_leaf=1;, score=-5.343 total time=   0.1s\n",
      "[CV 3/5] END ..max_depth=1, min_samples_leaf=1;, score=-5.269 total time=   0.1s\n",
      "[CV 4/5] END ..max_depth=1, min_samples_leaf=1;, score=-5.043 total time=   0.1s\n",
      "[CV 5/5] END ..max_depth=1, min_samples_leaf=1;, score=-5.454 total time=   0.1s\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Input \u001B[1;32mIn [45]\u001B[0m, in \u001B[0;36m<cell line: 4>\u001B[1;34m()\u001B[0m\n\u001B[0;32m      2\u001B[0m model \u001B[38;5;241m=\u001B[39m MyDecisionTreeRegressor()\n\u001B[0;32m      3\u001B[0m grid \u001B[38;5;241m=\u001B[39m GridSearchCV(model, parameters, scoring\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mneg_mean_absolute_error\u001B[39m\u001B[38;5;124m'\u001B[39m, verbose\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m4\u001B[39m)\n\u001B[1;32m----> 4\u001B[0m \u001B[43mgrid\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfeatures_train\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtarget_train\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m      5\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mЛучшие параметры:\u001B[39m\u001B[38;5;124m'\u001B[39m, grid\u001B[38;5;241m.\u001B[39mbest_params_, \u001B[38;5;124m'\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m      6\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mЛучший счет:\u001B[39m\u001B[38;5;124m'\u001B[39m, grid\u001B[38;5;241m.\u001B[39mbest_score_, \u001B[38;5;124m'\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m'\u001B[39m)\n",
      "File \u001B[1;32mC:\\anaconda\\lib\\site-packages\\sklearn\\model_selection\\_search.py:891\u001B[0m, in \u001B[0;36mBaseSearchCV.fit\u001B[1;34m(self, X, y, groups, **fit_params)\u001B[0m\n\u001B[0;32m    885\u001B[0m     results \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_format_results(\n\u001B[0;32m    886\u001B[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001B[0;32m    887\u001B[0m     )\n\u001B[0;32m    889\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m results\n\u001B[1;32m--> 891\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_run_search\u001B[49m\u001B[43m(\u001B[49m\u001B[43mevaluate_candidates\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    893\u001B[0m \u001B[38;5;66;03m# multimetric is determined here because in the case of a callable\u001B[39;00m\n\u001B[0;32m    894\u001B[0m \u001B[38;5;66;03m# self.scoring the return type is only known after calling\u001B[39;00m\n\u001B[0;32m    895\u001B[0m first_test_score \u001B[38;5;241m=\u001B[39m all_out[\u001B[38;5;241m0\u001B[39m][\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtest_scores\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n",
      "File \u001B[1;32mC:\\anaconda\\lib\\site-packages\\sklearn\\model_selection\\_search.py:1392\u001B[0m, in \u001B[0;36mGridSearchCV._run_search\u001B[1;34m(self, evaluate_candidates)\u001B[0m\n\u001B[0;32m   1390\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_run_search\u001B[39m(\u001B[38;5;28mself\u001B[39m, evaluate_candidates):\n\u001B[0;32m   1391\u001B[0m     \u001B[38;5;124;03m\"\"\"Search all candidates in param_grid\"\"\"\u001B[39;00m\n\u001B[1;32m-> 1392\u001B[0m     \u001B[43mevaluate_candidates\u001B[49m\u001B[43m(\u001B[49m\u001B[43mParameterGrid\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mparam_grid\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mC:\\anaconda\\lib\\site-packages\\sklearn\\model_selection\\_search.py:838\u001B[0m, in \u001B[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001B[1;34m(candidate_params, cv, more_results)\u001B[0m\n\u001B[0;32m    830\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mverbose \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[0;32m    831\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\n\u001B[0;32m    832\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mFitting \u001B[39m\u001B[38;5;132;01m{0}\u001B[39;00m\u001B[38;5;124m folds for each of \u001B[39m\u001B[38;5;132;01m{1}\u001B[39;00m\u001B[38;5;124m candidates,\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    833\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m totalling \u001B[39m\u001B[38;5;132;01m{2}\u001B[39;00m\u001B[38;5;124m fits\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mformat(\n\u001B[0;32m    834\u001B[0m             n_splits, n_candidates, n_candidates \u001B[38;5;241m*\u001B[39m n_splits\n\u001B[0;32m    835\u001B[0m         )\n\u001B[0;32m    836\u001B[0m     )\n\u001B[1;32m--> 838\u001B[0m out \u001B[38;5;241m=\u001B[39m \u001B[43mparallel\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    839\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdelayed\u001B[49m\u001B[43m(\u001B[49m\u001B[43m_fit_and_score\u001B[49m\u001B[43m)\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    840\u001B[0m \u001B[43m        \u001B[49m\u001B[43mclone\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbase_estimator\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    841\u001B[0m \u001B[43m        \u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    842\u001B[0m \u001B[43m        \u001B[49m\u001B[43my\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    843\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtrain\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtrain\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    844\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtest\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtest\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    845\u001B[0m \u001B[43m        \u001B[49m\u001B[43mparameters\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mparameters\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    846\u001B[0m \u001B[43m        \u001B[49m\u001B[43msplit_progress\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43msplit_idx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mn_splits\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    847\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcandidate_progress\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mcand_idx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mn_candidates\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    848\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mfit_and_score_kwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    849\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    850\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43m(\u001B[49m\u001B[43mcand_idx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mparameters\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m(\u001B[49m\u001B[43msplit_idx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m(\u001B[49m\u001B[43mtrain\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtest\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mproduct\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    851\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43menumerate\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mcandidate_params\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43menumerate\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mcv\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msplit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgroups\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    852\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    853\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    855\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(out) \u001B[38;5;241m<\u001B[39m \u001B[38;5;241m1\u001B[39m:\n\u001B[0;32m    856\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[0;32m    857\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mNo fits were performed. \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    858\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mWas the CV iterator empty? \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    859\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mWere there no candidates?\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    860\u001B[0m     )\n",
      "File \u001B[1;32mC:\\anaconda\\lib\\site-packages\\joblib\\parallel.py:1046\u001B[0m, in \u001B[0;36mParallel.__call__\u001B[1;34m(self, iterable)\u001B[0m\n\u001B[0;32m   1043\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdispatch_one_batch(iterator):\n\u001B[0;32m   1044\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_iterating \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_original_iterator \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m-> 1046\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdispatch_one_batch\u001B[49m\u001B[43m(\u001B[49m\u001B[43miterator\u001B[49m\u001B[43m)\u001B[49m:\n\u001B[0;32m   1047\u001B[0m     \u001B[38;5;28;01mpass\u001B[39;00m\n\u001B[0;32m   1049\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m pre_dispatch \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mall\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01mor\u001B[39;00m n_jobs \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m1\u001B[39m:\n\u001B[0;32m   1050\u001B[0m     \u001B[38;5;66;03m# The iterable was consumed all at once by the above for loop.\u001B[39;00m\n\u001B[0;32m   1051\u001B[0m     \u001B[38;5;66;03m# No need to wait for async callbacks to trigger to\u001B[39;00m\n\u001B[0;32m   1052\u001B[0m     \u001B[38;5;66;03m# consumption.\u001B[39;00m\n",
      "File \u001B[1;32mC:\\anaconda\\lib\\site-packages\\joblib\\parallel.py:861\u001B[0m, in \u001B[0;36mParallel.dispatch_one_batch\u001B[1;34m(self, iterator)\u001B[0m\n\u001B[0;32m    859\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;01mFalse\u001B[39;00m\n\u001B[0;32m    860\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m--> 861\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_dispatch\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtasks\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    862\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m\n",
      "File \u001B[1;32mC:\\anaconda\\lib\\site-packages\\joblib\\parallel.py:779\u001B[0m, in \u001B[0;36mParallel._dispatch\u001B[1;34m(self, batch)\u001B[0m\n\u001B[0;32m    777\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_lock:\n\u001B[0;32m    778\u001B[0m     job_idx \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlen\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jobs)\n\u001B[1;32m--> 779\u001B[0m     job \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_backend\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mapply_async\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbatch\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcallback\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcb\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    780\u001B[0m     \u001B[38;5;66;03m# A job can complete so quickly than its callback is\u001B[39;00m\n\u001B[0;32m    781\u001B[0m     \u001B[38;5;66;03m# called before we get here, causing self._jobs to\u001B[39;00m\n\u001B[0;32m    782\u001B[0m     \u001B[38;5;66;03m# grow. To ensure correct results ordering, .insert is\u001B[39;00m\n\u001B[0;32m    783\u001B[0m     \u001B[38;5;66;03m# used (rather than .append) in the following line\u001B[39;00m\n\u001B[0;32m    784\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jobs\u001B[38;5;241m.\u001B[39minsert(job_idx, job)\n",
      "File \u001B[1;32mC:\\anaconda\\lib\\site-packages\\joblib\\_parallel_backends.py:208\u001B[0m, in \u001B[0;36mSequentialBackend.apply_async\u001B[1;34m(self, func, callback)\u001B[0m\n\u001B[0;32m    206\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mapply_async\u001B[39m(\u001B[38;5;28mself\u001B[39m, func, callback\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m):\n\u001B[0;32m    207\u001B[0m     \u001B[38;5;124;03m\"\"\"Schedule a func to be run\"\"\"\u001B[39;00m\n\u001B[1;32m--> 208\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[43mImmediateResult\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfunc\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    209\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m callback:\n\u001B[0;32m    210\u001B[0m         callback(result)\n",
      "File \u001B[1;32mC:\\anaconda\\lib\\site-packages\\joblib\\_parallel_backends.py:572\u001B[0m, in \u001B[0;36mImmediateResult.__init__\u001B[1;34m(self, batch)\u001B[0m\n\u001B[0;32m    569\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__init__\u001B[39m(\u001B[38;5;28mself\u001B[39m, batch):\n\u001B[0;32m    570\u001B[0m     \u001B[38;5;66;03m# Don't delay the application, to avoid keeping the input\u001B[39;00m\n\u001B[0;32m    571\u001B[0m     \u001B[38;5;66;03m# arguments in memory\u001B[39;00m\n\u001B[1;32m--> 572\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mresults \u001B[38;5;241m=\u001B[39m \u001B[43mbatch\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mC:\\anaconda\\lib\\site-packages\\joblib\\parallel.py:262\u001B[0m, in \u001B[0;36mBatchedCalls.__call__\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    258\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__call__\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m    259\u001B[0m     \u001B[38;5;66;03m# Set the default nested backend to self._backend but do not set the\u001B[39;00m\n\u001B[0;32m    260\u001B[0m     \u001B[38;5;66;03m# change the default number of processes to -1\u001B[39;00m\n\u001B[0;32m    261\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m parallel_backend(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backend, n_jobs\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_n_jobs):\n\u001B[1;32m--> 262\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m [func(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m    263\u001B[0m                 \u001B[38;5;28;01mfor\u001B[39;00m func, args, kwargs \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mitems]\n",
      "File \u001B[1;32mC:\\anaconda\\lib\\site-packages\\joblib\\parallel.py:262\u001B[0m, in \u001B[0;36m<listcomp>\u001B[1;34m(.0)\u001B[0m\n\u001B[0;32m    258\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__call__\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m    259\u001B[0m     \u001B[38;5;66;03m# Set the default nested backend to self._backend but do not set the\u001B[39;00m\n\u001B[0;32m    260\u001B[0m     \u001B[38;5;66;03m# change the default number of processes to -1\u001B[39;00m\n\u001B[0;32m    261\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m parallel_backend(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backend, n_jobs\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_n_jobs):\n\u001B[1;32m--> 262\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m [func(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m    263\u001B[0m                 \u001B[38;5;28;01mfor\u001B[39;00m func, args, kwargs \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mitems]\n",
      "File \u001B[1;32mC:\\anaconda\\lib\\site-packages\\sklearn\\utils\\fixes.py:216\u001B[0m, in \u001B[0;36m_FuncWrapper.__call__\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m    214\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__call__\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[0;32m    215\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m config_context(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconfig):\n\u001B[1;32m--> 216\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfunction(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32mC:\\anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:680\u001B[0m, in \u001B[0;36m_fit_and_score\u001B[1;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, split_progress, candidate_progress, error_score)\u001B[0m\n\u001B[0;32m    678\u001B[0m         estimator\u001B[38;5;241m.\u001B[39mfit(X_train, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mfit_params)\n\u001B[0;32m    679\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m--> 680\u001B[0m         estimator\u001B[38;5;241m.\u001B[39mfit(X_train, y_train, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mfit_params)\n\u001B[0;32m    682\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m:\n\u001B[0;32m    683\u001B[0m     \u001B[38;5;66;03m# Note fit time as time until error\u001B[39;00m\n\u001B[0;32m    684\u001B[0m     fit_time \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mtime() \u001B[38;5;241m-\u001B[39m start_time\n",
      "Input \u001B[1;32mIn [40]\u001B[0m, in \u001B[0;36mMyDecisionTreeRegressor.fit\u001B[1;34m(self, X, y)\u001B[0m\n\u001B[0;32m    198\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtree_\u001B[38;5;241m.\u001B[39mprediction \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mmean(y)\n\u001B[0;32m    200\u001B[0m \u001B[38;5;66;03m# Grow the tree\u001B[39;00m\n\u001B[1;32m--> 201\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgrow_tree\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtree_\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    202\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\n",
      "Input \u001B[1;32mIn [40]\u001B[0m, in \u001B[0;36mMyDecisionTreeRegressor.grow_tree\u001B[1;34m(self, node, X, y)\u001B[0m\n\u001B[0;32m    142\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m\n\u001B[0;32m    144\u001B[0m \u001B[38;5;66;03m# Make best split\u001B[39;00m\n\u001B[1;32m--> 145\u001B[0m split_column, threshold, X_left, y_left, X_right, y_right \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbest_split\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;66;03m# Make a split\u001B[39;00m\n\u001B[0;32m    146\u001B[0m \u001B[38;5;66;03m# split_column = 2 (exmaple) column index of the split\u001B[39;00m\n\u001B[0;32m    147\u001B[0m \u001B[38;5;66;03m# threshold = 2.74 (example) split_column > threshold\u001B[39;00m\n\u001B[0;32m    148\u001B[0m \n\u001B[0;32m    149\u001B[0m \u001B[38;5;66;03m# Check additional termination conditions\u001B[39;00m\n\u001B[0;32m    150\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m split_column \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "Input \u001B[1;32mIn [40]\u001B[0m, in \u001B[0;36mMyDecisionTreeRegressor.best_split\u001B[1;34m(self, X, y)\u001B[0m\n\u001B[0;32m     68\u001B[0m threshold \u001B[38;5;241m=\u001B[39m x_col[i_x]\n\u001B[0;32m     69\u001B[0m \u001B[38;5;66;03m# threshold = 1.3 (example)\u001B[39;00m\n\u001B[0;32m     70\u001B[0m \n\u001B[0;32m     71\u001B[0m \u001B[38;5;66;03m# Make the split into right and left childs\u001B[39;00m\n\u001B[1;32m---> 72\u001B[0m information_gain \u001B[38;5;241m=\u001B[39m best_cost \u001B[38;5;241m-\u001B[39m \u001B[43mQ\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msplit_column\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mthreshold\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     73\u001B[0m \u001B[38;5;66;03m# information_gain = 0.2 (example)\u001B[39;00m\n\u001B[0;32m     74\u001B[0m \n\u001B[0;32m     75\u001B[0m \u001B[38;5;66;03m# Is this information_gain the best?\u001B[39;00m\n\u001B[0;32m     76\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m information_gain \u001B[38;5;241m>\u001B[39m best_information_gain:\n",
      "Input \u001B[1;32mIn [38]\u001B[0m, in \u001B[0;36mQ\u001B[1;34m(X, y, j, t)\u001B[0m\n\u001B[0;32m     23\u001B[0m R_l \u001B[38;5;241m=\u001B[39m y[x_col \u001B[38;5;241m<\u001B[39m\u001B[38;5;241m=\u001B[39m t]\n\u001B[0;32m     24\u001B[0m R_r \u001B[38;5;241m=\u001B[39m y[x_col \u001B[38;5;241m>\u001B[39m t]\n\u001B[1;32m---> 25\u001B[0m tmp_r \u001B[38;5;241m=\u001B[39m \u001B[43mH\u001B[49m\u001B[43m(\u001B[49m\u001B[43mR_r\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;241m/\u001B[39m \u001B[38;5;28mlen\u001B[39m(y)\n\u001B[0;32m     26\u001B[0m tmp_l \u001B[38;5;241m=\u001B[39m H(R_l) \u001B[38;5;241m/\u001B[39m \u001B[38;5;28mlen\u001B[39m(y)\n\u001B[0;32m     27\u001B[0m Q \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m\n",
      "Input \u001B[1;32mIn [36]\u001B[0m, in \u001B[0;36mH\u001B[1;34m(y)\u001B[0m\n\u001B[0;32m     17\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;241m0\u001B[39m\n\u001B[0;32m     18\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m---> 19\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mnp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msum\u001B[49m\u001B[43m(\u001B[49m\u001B[43mnp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpower\u001B[49m\u001B[43m(\u001B[49m\u001B[43m(\u001B[49m\u001B[43my\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m-\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mnp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmean\u001B[49m\u001B[43m(\u001B[49m\u001B[43my\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m2\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;241m/\u001B[39m \u001B[38;5;28mlen\u001B[39m(y)\n",
      "File \u001B[1;32m<__array_function__ internals>:2\u001B[0m, in \u001B[0;36msum\u001B[1;34m(*args, **kwargs)\u001B[0m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "parameters = {\"max_depth\":range(1,4),'min_samples_leaf':range(1,6)}\n",
    "model = MyDecisionTreeRegressor()\n",
    "grid = GridSearchCV(model, parameters, scoring='neg_mean_absolute_error', verbose=4)\n",
    "grid.fit(features_train, target_train)\n",
    "print('Лучшие параметры:', grid.best_params_, '\\n')\n",
    "print('Лучший счет:', grid.best_score_, '\\n')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Task 6 <a id=\"task6\"></a>  (2 points)\n",
    "\n",
    "Recall definition of bias and variance:\n",
    "$$\n",
    "\\text{Bias}^2 = \\mathbb{E}_{p(x, y)} \\left[  (f(x) - \\mathbb{E}_{\\mathbb{X}}a_{\\mathbb{X}}(x))^2 \\right] \\\\\n",
    "\\text{Variance} = \\mathbb{E}_{p(x, y)} \\left[  \\mathbb{V}_{\\mathbb{X}}( a_{\\mathbb{X}}(x))  \\right]\n",
    "$$\n",
    "\n",
    "We wil now use use the following algorithm to estimate bias and variance:\n",
    "\n",
    "1. Use bootsrap to create `n_iter` samples from the original dataset: $X_1, \\dots, X_{n_iter}$\n",
    "2. For each bootstrapped sample define out-of-bag (OOB) sample $Z_1, \\dots, Z_{n_iter}$, which contain all the observations, which did not appear in the corresponding boostraped sample\n",
    "3. Fit the model on $X_i$s and compute predictions on $Z_i$s\n",
    "4. For a given *object* $n$:\n",
    "     - bias^2: squared difference between true value $y_n$ and average prediction (average over the algorithms, for which $n$ was in OOB)\n",
    "     - variance: variance of the prediction (predictions of the algorithms, for which $n$ was in OOB)\n",
    "5. Average bias^2 and variance over all the points\n",
    "\n",
    "**Implement `get_bias_variance` function, using the algorithm above**\n",
    "\n",
    "*Note:*  You can only use 1 loop (for bootsrap iterations). All other operations should be vectorized."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def get_bias_variance(estimator, x, y, n_iter):\n",
    "    \"\"\"\n",
    "    Calculate bias and variance of the `estimator`.\n",
    "    Using a given dataset and bootstrap with `n_iter` samples.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    x : ndarray, shape (n_samples, n_features)\n",
    "        The input samples.\n",
    "    y : ndarray, shape (n_samples, n_features)\n",
    "        The input samples.\n",
    "    n_iter: int\n",
    "        Number of samples in\n",
    "    Returns\n",
    "    -------\n",
    "    bias2 : float,\n",
    "        Estiamted squared bias\n",
    "    variance : float,\n",
    "        Estiamted variance\n",
    "    \"\"\"\n",
    "    np.random.seed(12345)\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    #TypeError: only integer scalar arrays can be converted to a scalar index\n",
    "    data_tmp = pd.DataFrame([], index=range(len(y)))\n",
    "    # for i in range(n_iter):\n",
    "    #     data_tmp[i] = np.nan\n",
    "\n",
    "    for i in range(n_iter):\n",
    "        bootstrap = np.random.choice(len(y), len(y))\n",
    "\n",
    "        OOB_indexes = np.arange(len(y))[~np.isin(np.arange(len(y)),bootstrap)]\n",
    "        OOB = x[OOB_indexes, :]\n",
    "\n",
    "        estimator.fit(x[bootstrap, :], y[bootstrap])\n",
    "        predictions = estimator.predict(OOB)\n",
    "\n",
    "        columns = pd.Series([np.nan] * len(y))\n",
    "        columns = np.array(columns)\n",
    "\n",
    "        np.put(columns, OOB_indexes, predictions)\n",
    "\n",
    "        # display(pd.Series(columns))\n",
    "\n",
    "        # display(pd.Series(columns))\n",
    "        data_tmp[i] = pd.Series(columns)\n",
    "\n",
    "\n",
    "        #Note: You can only use 1 loop (for bootsrap iterations). All other operations should be vectorized.Note: You can only use 1 loop (for bootsrap iterations). All other operations should be vectorized.\n",
    "        # for index in data_tmp.index:\n",
    "        #     if index in OOB_indexes:\n",
    "        #         data_tmp.loc[index, i] = predictions[index]\n",
    "        # display(data_tmp)\n",
    "        # my_iter = iter(predictions)\n",
    "        # print(OOB_indexes)\n",
    "        # for j in range(len(y)):\n",
    "        #     if j in OOB_indexes:\n",
    "        #         print('J', j)\n",
    "        #         index = list(OOB_indexes).index(j)\n",
    "        #         print('INDEX', index)\n",
    "        #         data_tmp.at[i, index] = next(my_iter)\n",
    "        # display(data_tmp)\n",
    "\n",
    "\n",
    "    return np.nanmean(np.square(y - np.nanmean(data_tmp, axis = 1))), np.nanmean(np.nanvar(data_tmp, axis=1))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%%time\n",
    "# Test\n",
    "estimator = MyDecisionTreeRegressor(max_depth=8, min_samples_split=15)\n",
    "\n",
    "get_bias_variance(estimator, features_train, target_train, 10)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Task 7 <a id=\"task7\"></a>  (0.5 points)\n",
    "\n",
    "Compute bias and variance for the trees with different min_samples_split. Plot how bias and variance change as min_samples_split increases.\n",
    "\n",
    "Comment on what you observe, how does your result correspond to theory?"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%%time\n",
    "np.random.seed(12345)\n",
    "\n",
    "bias_lst = []\n",
    "variance_lst = []\n",
    "\n",
    "for sample in range(0, 51, 10):\n",
    "    model = MyDecisionTreeRegressor(max_depth=10, min_samples_split=sample)\n",
    "    bias, variance = get_bias_variance(model, features_train, target_train, 10)\n",
    "    bias_lst.append(bias)\n",
    "    variance_lst.append(variance)\n",
    "plt.plot(range(0, 51, 10), bias_lst, label='bias')\n",
    "plt.plot(range(0, 51, 10), variance_lst, label='variance')\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "``` С увличением min_samples_split дисперсия уменьшается, что говорит о том, что данные становятся более кучными и модель становится лучше. bias же практически не изменяется, но в целом он и не должен, он бы убывал, если бы кол-во деревьев бы возрастало.```"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Task 8 <a id=\"task8\"></a>  (0.5 points)\n",
    "\n",
    "Let's try to reduce variance with bagging. Use `sklearn.ensemble.BaggingRegressor` to get an ensemble and compute its bias and variance.\n",
    "\n",
    "Answer the following questions:\n",
    " - How bagging should affect bias and variance in theory?\n",
    " - How bias and variance change (if they change) compared to an individual tree in you experiments?\n",
    " - Do your results align with the theory? Why?"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingRegressor\n",
    "\n",
    "model = BaggingRegressor(base_estimator=MyDecisionTreeRegressor(max_depth=8, min_samples_split=15), n_estimators=10, random_state=12345)\n",
    "\n",
    "get_bias_variance(model, features_train, target_train, 10)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "```your comments here```"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Part 2. More Ensembles"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "In this part we will be working with [Thyroid Disease Data Set](https://archive.ics.uci.edu/ml/datasets/thyroid+disease) to solve a classification task."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "df = pd.read_csv('thyroid_disease.csv')\n",
    "\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(df['Class'])\n",
    "X = df.drop('Class', axis=1)\n",
    "X.head(5)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Task 1 <a id=\"task2_1\"></a> (1 point)\n",
    "\n",
    "Let's start with data preprocessing.\n",
    "\n",
    "0. Drop columns, which are not usefull (e.g. a lot of missing values). Motivate your choice.\n",
    "1. Split dataset into train and test\n",
    "2. You've probably noticed that we have both categorical and numerical columns. Here is what you need to do with them:\n",
    "    - Categorical: Fill missing values and apply one-hot-encoding\n",
    "    - Numeric: Fill missing values\n",
    "\n",
    "Use `ColumnTranformer` to define a single transformer for all the columns in the dataset. It takes as input a list of tuples\n",
    "\n",
    "```\n",
    "ColumnTransformer([\n",
    "    ('name1', transform1, column_names1),\n",
    "    ('name2', transform2, column_names2)\n",
    "])\n",
    "```\n",
    "\n",
    "Pay attention to an argument `remainder='passthrough'`. [Here](https://scikit-learn.org/stable/modules/compose.html#column-transformer) you can find some examples of how to use column transformer.\n",
    "\n",
    "Since we want to apply 2 transformations to categorical feature, it is very convenient to combine them into a `Pipeline`:\n",
    "\n",
    "```\n",
    "double_tranform = make_pipeline(\n",
    "                        transform_1,\n",
    "                        transform_2\n",
    "                        )\n",
    "```\n",
    "\n",
    "P.S. Choose your favourite way to fill missing values.\n",
    "\n",
    "*Hint* Categorical column usually have `dtype = 'object'`. This may help to obtain list of categorical and numerical columns on the dataset."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "\n",
    "# YOUR CODE HERE\n",
    "# define column_transformer\n",
    "\n",
    "# Transform the data\n",
    "features_train = column_transformer.fit_transform(features_train)\n",
    "features_test = column_transformer.transform(features_test)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Task 2 <a id=\"task2_2\"></a> (0.7 points)\n",
    "\n",
    "Fit and compare 5 different models (use sklearn): Gradient Boosting, Random Forest, Decision Tree, SVM, Logitics Regression\n",
    "\n",
    "* Choose one classification metric and justify your choice .\n",
    "* Compare the models using score on cross validation. Mind the class balance when choosing the cross validation. (You can read more about different CV strategies [here](https://scikit-learn.org/stable/modules/cross_validation.html#stratified-k-fold))\n",
    "* Which model has the best performance? Which models overfit or underfit?"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "```your comments here```"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Task 3 <a id=\"task2_3\"></a> (0.5 points)\n",
    "\n",
    "More Gradient Boosting. You will have to implement one of the three popular boosting implementations (xgboost, lightgbm, catboost). Select hyperparameters (number of trees, learning rate, depth) on cross-validation and compare with the methods from the previous task.\n",
    "\n",
    "To get method that you have to implement, run cell below and input your name in Russian (for example, if you input Андрей, you will see that user with this name should implement xgboost)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def assign_method():\n",
    "    name = input()\n",
    "    methods = ['xgboost', 'lightgbm', 'catboost']\n",
    "    idx = sum([ord(x) for x in list(name)]) % 3\n",
    "    print('Реализуйте', methods[idx])\n",
    "\n",
    "assign_method()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "```your comments here```"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Task 4 <a id=\"task2_4\"></a> (0.7 points)\n",
    "\n",
    "Now let's train more fancy ensembles:\n",
    "\n",
    "* Bagging with decision trees as base estimators\n",
    "* Bagging with gradient boosting (with large amount of trees, >100) as base estimators\n",
    "* [Voting classifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.VotingClassifier.html#sklearn.ensemble.VotingClassifier)\n",
    "* [Stacking Classifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.StackingClassifier.html#sklearn.ensemble.StackingClassifier) with Logistic Regression as a final model\n",
    "* [Stacking Classifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.StackingClassifier.html#sklearn.ensemble.StackingClassifier) with Gradeint Boosting as a final model\n",
    "\n",
    "\n",
    "If not stated in the task, feel free to tune / choose hyperparameters and base models.\n",
    "\n",
    "Answer the questions:\n",
    "* Which model has the best performance?\n",
    "* Does bagging reduce overfiting of the gradient boosting with large amount of trees?\n",
    "* What is the difference between voting and staking?"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "```your comments here```"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Task 5 <a id=\"task2_5\"></a> (0.1 points)\n",
    "\n",
    "Report the test score for the best model, that you were able to train."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
