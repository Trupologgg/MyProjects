{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# HSE 2021: Mathematical Methods for Data Analysis\n",
    "\n",
    "## Homework 3\n",
    "\n",
    "**Warning 1**: some problems require (especially the lemmatization part) significant amount of time, so **it is better to start early (!)**\n",
    "\n",
    "**Warning 2**: it is critical to describe and explain what you are doing and why, use markdown cells"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Line magic function `%` not found.\n"
     ]
    }
   ],
   "source": [
    "from typing import Tuple, List\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "% matplotlib inline\n",
    "\n",
    "sns.set(style=\"darkgrid\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## PART 1: Logit model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We consider a binary classification problem. For prediction, we would like to use a logistic regression model. For regularization we add a combination of the $l_2$ and $l_1$ penalties (Elastic Net).\n",
    "\n",
    "Each object in the training dataset is indexed with $i$ and described by pair: features $x_i\\in\\mathbb{R}^{K}$ and binary labels $y_i$. The model parametrized with bias $w_0\\in\\mathbb{R}$ and weights $w\\in\\mathbb{R}^K$.\n",
    "\n",
    "The optimization problem with respect to the $w_0, w$ is the following (Elastic Net Loss):\n",
    "\n",
    "$$L(w, w_0) = \\frac{1}{N} \\sum_{i=1}^N \\ln(1+\\exp(-y_i(w^\\top x_i+w_0))) + \\gamma \\|w\\|_1 + \\beta \\|w\\|_2^2$$."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 1. [0.5 points]  Find the gradient of the Elastic Net loss and write its formulas (better in latex format)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### Put your markdown formulas here\n",
    "\n",
    "##### $$\\nabla_w L(w,w_0)= -\\frac{1}{N} \\sum_i^N \\frac{y_i x_iexp(-y_i (w^Tx_i+w_0))}{1+exp(-y_i (w^Tx_i+w_0))}  + \\gamma sign(w)+ 2 \\cdot \\beta \\cdot w$$\n",
    "\n",
    "##### $$\\nabla_{w_0} L(w,w_0)= -\\frac{1}{N} \\sum_i^N \\frac{y_iexp(-y_i (w^Tx_i+w_0))}{1+exp(-y_i (w^Tx_i+w_0))} $$\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 2. [0.25 points] Implement the Elastic Net loss (as a function)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def loss(X, y, w: List[float], w0: float, gamma=1., beta=1.) -> float:\n",
    "    return 1 / len(X) * np.sum(1 + np.exp(-y * (np.dot(X, np.transpose(w)) + w0))) + gamma * np.sum(\n",
    "        np.abs(w)) + beta * np.linalg.norm(w)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 3. [0.25 points] Implement the gradient (as a function)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# def get_grad(X, y, w: List[float], w0: float, gamma=1., beta=1.) -> Tuple[List[float], float]:\n",
    "#     grad_w0 = ((-y) / (1 + np.exp(np.multiply(y, (np.dot(w, X.T) + w0))))).mean()\n",
    "#     grad_w = ((np.dot((-y) / (1 + np.exp(np.multiply(y, (np.dot(w, X.T) + w0)))), X)) / len(y)) + gamma * np.sign(w) + 2 * beta * w\n",
    "#\n",
    "#     return grad_w, grad_w0\n",
    "\n",
    "\n",
    "# def get_grad(X, y, w: List[float], w0: float, gamma=1., beta=1.) -> Tuple[List[float], float]:\n",
    "#     grad_w0 = -1 / len(X) * np.sum(\n",
    "#         np.exp(-y * (np.dot(X, np.transpose(w)) + w0)) / (1 + np.exp(-y * (np.dot(X, np.transpose(w)) + w0))) * y)\n",
    "#     grad_w = -1 / len(X) * np.dot(np.exp(-y*(np.dot(X,np.transpose(w))+w0)) / (1 + np.exp(-y * (np.dot(X, np.transpose(w)) + w0))) * y, X) + gamma * np.sign(\n",
    "#         w) + 2 * beta * w\n",
    "#     return grad_w, grad_w0\n",
    "\n",
    "def tmp_grad(X, w, w0, y):\n",
    "    return (1 + np.exp(np.multiply(y, (np.dot(w, X.T) + w0))))\n",
    "\n",
    "\n",
    "def get_grad(X, y, w: List[float], w0: float, gamma=1., beta=1.) -> Tuple[List[float], float]:\n",
    "    grad_w0 = (y / tmp_grad(X, w, w0, y)).mean()\n",
    "    grad_w = ((np.dot((-y) / tmp_grad(X, w, w0, y), X)) / len(y)) + gamma * np.sign(w) + 2 * beta * w\n",
    "    return grad_w, -1 * grad_w0"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Check yourself"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "X = np.random.multivariate_normal(np.arange(5), np.eye(5), size=10)\n",
    "y = np.random.binomial(1, 0.42, size=10)\n",
    "w, w0 = np.random.normal(size=5), np.random.normal()\n",
    "\n",
    "grad_w, grad_w0 = get_grad(X, y, w, w0)\n",
    "print(grad_w)\n",
    "print(grad_w0)\n",
    "assert (np.allclose(grad_w,\n",
    "                    [-2.73262076, -1.87176281, 1.30051144, 2.53598941, -2.71198109],\n",
    "                    rtol=1e-2) & np.allclose(grad_w0,\n",
    "                                             -0.2078231418067844,\n",
    "                                             rtol=1e-2)\n",
    "        )"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "####  4. [1 point]  Implement gradient descent which works for both tol level and max_iter stop criteria and plot the decision boundary of the result"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The template provides basic sklearn API class. You are free to modify it in any convenient way."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, ClassifierMixin"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "import numpy as np\n",
    "\n",
    "class Logit(BaseEstimator, ClassifierMixin):\n",
    "\n",
    "    def __init__(self, beta=1.0, gamma=1.0, lr=1e-3, tolerance=1e-8, max_iter=1000, random_state=42):\n",
    "        self.threshold = 0.5 #по умолчанию если не менять пороги классификации\n",
    "        self.beta = beta\n",
    "        self.gamma = gamma\n",
    "        self.tolerance= tolerance\n",
    "        self.max_iter= max_iter\n",
    "        self.lr = lr\n",
    "        self.random_state = random_state\n",
    "        self.negative = -1\n",
    "        self.positive = 1\n",
    "        self.lh = []\n",
    "\n",
    "\n",
    "    def editing_loss(self):\n",
    "        self.lh.append(loss(X, y, self.w, self.w0, self.gamma, self.beta))\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        np.random.seed(self.random_state)\n",
    "        self.lh = []\n",
    "        self.w = np.random.sample(X.shape[1])\n",
    "        self.w0 = np.random.normal()\n",
    "\n",
    "        for iteration in range(self.max_iter):\n",
    "            print(iteration)\n",
    "            print()\n",
    "            grad_w, grad_w0 = get_grad(X, y, self.w, self.w0, self.gamma, self.beta)\n",
    "            self.w -= self.lr * grad_w\n",
    "            self.w0 -= self.lr * grad_w0\n",
    "            self.editing_loss()\n",
    "            if np.linalg.norm(-self.lr * grad_w) < self.tolerance:\n",
    "                break\n",
    "\n",
    "        # probabilities_valid = self.predict_proba(X)\n",
    "        # probabilities_one_valid = probabilities_valid[:, 1]\n",
    "        #\n",
    "        # fpr, tpr, thresholds = roc_curve(y, probabilities_one_valid)\n",
    "        # self.thresholds = thresholds\n",
    "        return self\n",
    "\n",
    "\n",
    "    def predict(self, X):\n",
    "        # probabilities_valid, useless = self.predict_proba(X)\n",
    "        # predict = list()\n",
    "        # predict = np.where(probabilities_valid <= self.threshold, self.negative, self.positive)\n",
    "        predict, useless = self.predict_proba(X)\n",
    "        predict  =  np.where(predict <= self.threshold, self.negative, self.positive)\n",
    "        return predict\n",
    "\n",
    "\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        return np.array([1 / (1 + np.exp(np.dot(X, self.w) + self.w0)),\\\n",
    "                         1 / (1 + np.exp(-np.dot(X, self.w) - self.w0))])\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "\n",
    "X, y = make_classification(n_samples=180, n_features=2, n_redundant=0, n_informative=2,\n",
    "                           random_state=42, n_clusters_per_class=1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# a function to plot the decision boundary\n",
    "def plot_decision_boundary(model, X, y):\n",
    "    fig = plt.figure()\n",
    "    X1min, X2min = X.min(axis=0)\n",
    "    X1max, X2max = X.max(axis=0)\n",
    "    x1, x2 = np.meshgrid(np.linspace(X1min, X1max, 200),\n",
    "                         np.linspace(X2min, X2max, 200))\n",
    "    ypred = model.predict(np.c_[x1.ravel(), x2.ravel()])\n",
    "    ypred = ypred.reshape(x1.shape)\n",
    "\n",
    "    plt.contourf(x1, x2, ypred, alpha=.4)\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model = Logit(0, 0)\n",
    "y[y == 0] = -1\n",
    "model.fit(X, y)\n",
    "plot_decision_boundary(model, X, y)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 5. [0.25 points] Plot loss diagram for the model, i.e. show the dependence of the loss function from the gradient descent steps"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(model.lh)\n",
    "plt.plot(model.lh)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## PART 2: Support Vector Machines"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 6. [2 point] Using the same dataset, train SVM Classifier from Sklearn.\n",
    "Investigate how different parameters influence the quality of the solution:\n",
    "+ Try several kernels: Linear, Polynomial, RBF (and others if you wish). Some Kernels have hypermeters: don't forget to try different.\n",
    "+ Regularization coefficient\n",
    "\n",
    "Show how these parameters affect accuracy, roc_auc and f1 score.\n",
    "Make plots for the dependencies between metrics and parameters.\n",
    "Try to formulate conclusions from the observations. How sensitive are kernels to hyperparameters? How sensitive is a solution to the regularization? Which kernel is prone to overfitting?"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "my_accuracy_linear = list()\n",
    "my_accuracy_poly = list()\n",
    "my_accuracy_rbf = list()\n",
    "\n",
    "my_auc_roc_linear = list()\n",
    "my_auc_roc_poly = list()\n",
    "my_auc_roc_rbf = list()\n",
    "\n",
    "my_f1_linear = list()\n",
    "my_f1_poly = list()\n",
    "my_f1_rbf = list()\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "\n",
    "def svm_kernels(features_train, features_valid, target_train, target_valid, kernel):\n",
    "    for iter in np.linspace(0.001, 3, 200):\n",
    "        model = SVC(kernel=kernel, C=iter)\n",
    "        model.fit(features_train, target_train)\n",
    "        predicted = model.predict(features_valid)\n",
    "        if kernel == 'linear':\n",
    "            my_accuracy_linear.append(accuracy_score(target_valid, predicted))\n",
    "            my_auc_roc_linear.append(roc_auc_score(target_valid, predicted))\n",
    "            my_f1_linear.append(f1_score(target_valid, predicted))\n",
    "        elif kernel == 'poly':\n",
    "            my_accuracy_poly.append(accuracy_score(target_valid, predicted))\n",
    "            my_auc_roc_poly.append(roc_auc_score(target_valid, predicted))\n",
    "            my_f1_poly.append(f1_score(target_valid, predicted))\n",
    "        else:\n",
    "            my_accuracy_rbf.append(accuracy_score(target_valid, predicted))\n",
    "            my_auc_roc_rbf.append(roc_auc_score(target_valid, predicted))\n",
    "            my_f1_rbf.append(f1_score(target_valid, predicted))\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "features_train, features_valid, target_train, target_valid = train_test_split(X, y, test_size=0.25, random_state=0)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "svm_kernels(features_train, features_valid, target_train, target_valid, 'linear')\n",
    "svm_kernels(features_train, features_valid, target_train, target_valid, 'poly')\n",
    "svm_kernels(features_train, features_valid, target_train, target_valid, 'rbf')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print((my_accuracy_linear))\n",
    "print((my_accuracy_poly))\n",
    "print((my_accuracy_rbf))\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.plot(my_accuracy_linear, label='linear kernel', color='red')\n",
    "plt.plot(my_accuracy_poly, label='poly kernel', color='blue')\n",
    "plt.plot(my_accuracy_rbf, label='rbf kernel', color='pink')\n",
    "plt.title(\"Accurance score\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "plt.plot(my_auc_roc_linear, label='linear kernel', color='red')\n",
    "plt.plot(my_auc_roc_poly, label='poly kernel', color='blue')\n",
    "plt.plot(my_auc_roc_rbf, label='rbf kernel', color='pink')\n",
    "plt.title(\"Roc score\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.plot(my_f1_linear, label='linear kernel', color='red')\n",
    "plt.plot(my_f1_poly, label='poly kernel', color='blue')\n",
    "plt.plot(my_f1_rbf, label='rbf kernel', color='pink')\n",
    "plt.title(\"F1 score\")\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## PART 3: Natural Language Processing"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 7. [1.75 point] Form the dataset\n",
    "\n",
    "We are going to form a dataset that we will use in the following tasks for binary and multiclass classification\n",
    "\n",
    "0. Choose **six** authors that you like (specify who you've chosen) and download the <a href=\"https://www.kaggle.com/d0rj3228/russian-literature?select=prose\">relevant data</a> from **prose** section\n",
    "1. Build your own dataset for these authors:\n",
    "    * divide each text into sentences such that we will have two columns: *sentence* and *target author*, each row will contain one sentence and one target\n",
    "    * drop sentences where N symbols in a sentence < 15\n",
    "    * fix random state and randomly choose sentences in the folowing proportion \"5k : 15k : 8k : 11k : 20k : 3k\" for the authors respectively\n",
    "\n",
    "    sample data may look like:\n",
    "\n",
    "    <center>\n",
    "    <table>\n",
    "        <tr>\n",
    "            <th> sentence </th>\n",
    "            <th> author </th>\n",
    "        </tr>\n",
    "        <tr><td> Несколько лет тому назад в одном из своих поместий жил старинный русской барин, Кирила Петрович Троекуров. </td><td> Пушкин </td><td>\n",
    "        <tr><td> Уже более недели приезжий господин жил в городе, разъезжая по вечеринкам и обедам и таким образом проводя, как говорится, очень приятно время. </td><td> Гоголь </td><td>\n",
    "        <tr><td> ... </td><td> ... </td><td>\n",
    "        <tr><td> Я жил недорослем, гоняя голубей и играя в чехарду с дворовыми мальчишками. </td><td> Пушкин </td><td>\n",
    "    </table>\n",
    "</center>\n",
    "\n",
    "2. Preprocess (tokenize and clean) the dataset\n",
    "    * tokenize, remove all stop words (nltk.corpus.stopwords), punctuation (string.punctuation) and numbers\n",
    "    * convert to lower case and apply either stemming or lemmatization of the words (on your choice)\n",
    "    * vectorize words using both **bag of words** and **tf-idf** (use sklearn)\n",
    "    * observe and describe the difference between vectorized output (what do numbers look like after transformations and what do they represent?)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "fold = 'C:/Users/Александр/DataspellProjects/HSE-ML/third_hw/datasets/prose/'\n",
    "my_authors = ['Gogol', 'Herzen', 'Turgenev', 'Tolstoy', 'Pushkin', 'Blok']"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import glob\n",
    "import pandas as pd\n",
    "\n",
    "target_author = []\n",
    "features_sent = []\n",
    "for author in my_authors:\n",
    "    tmp_sent = []\n",
    "    for path in glob.glob(fold + author + '/*.txt'):\n",
    "        text = []\n",
    "        with open(path, 'r', encoding=\"utf-8\") as file:\n",
    "            text = file.read()\n",
    "        text = text.replace('\\n', ' ').split('.')\n",
    "        text = [text[i] for i in range(len(text)) if len(text[i]) >= 15]\n",
    "        tmp_sent.extend(text)\n",
    "    features_sent.append(tmp_sent)\n",
    "for i in range(len(my_authors)):\n",
    "    print(my_authors[i], len(features_sent[i]))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "#\"5k : 15k : 8k : 11k : 20k : 3k\"\n",
    "\n",
    "tmp = 5 + 15 + 8 + 11 + 20 + 3\n",
    "k = 100 / tmp\n",
    "\n",
    "\n",
    "def randomly_choose(element, koef):\n",
    "    element = pd.Series(element)\n",
    "    return element.sample(frac=koef * k / 100, replace=True, random_state=12345)\n",
    "\n",
    "\n",
    "features_sent[0] = randomly_choose(features_sent[0], 5)\n",
    "features_sent[1] = randomly_choose(features_sent[1], 15)\n",
    "features_sent[2] = randomly_choose(features_sent[2], 8)\n",
    "features_sent[3] = randomly_choose(features_sent[3], 11)\n",
    "features_sent[4] = randomly_choose(features_sent[4], 20)\n",
    "features_sent[5] = randomly_choose(features_sent[5], 3)\n",
    "\n",
    "for i in range(len(my_authors)):\n",
    "    print(my_authors[i], len(features_sent[i]))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "\n",
    "random.seed(42)\n",
    "\n",
    "data = pd.DataFrame([], columns=['sentence', 'author'])\n",
    "\n",
    "# print(features_sent[0])\n",
    "print()\n",
    "\n",
    "\n",
    "def creating_data(i, sentence, author):\n",
    "    tmp_sent_for_each_author = sentence\n",
    "    tmp_author_for_each_author = list()\n",
    "    for i in range(len(tmp_sent_for_each_author)):\n",
    "        tmp_author_for_each_author.append(author)\n",
    "    tmp_data_for_each_author = pd.DataFrame()\n",
    "    tmp_data_for_each_author['sentence'] = list(tmp_sent_for_each_author)\n",
    "    tmp_data_for_each_author['author'] = tmp_author_for_each_author\n",
    "    return tmp_data_for_each_author\n",
    "\n",
    "\n",
    "for i in range(len(my_authors)):\n",
    "    data = pd.concat(\n",
    "        [data] + [creating_data(i, features_sent[i], my_authors[i])\n",
    "                  ])\n",
    "data = shuffle(data, random_state=12345)\n",
    "data = data.reset_index(drop=True)\n",
    "display(data)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import string # for work with strings\n",
    "import nltk   # Natural Language Toolkit"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# get russian stop words\n",
    "nltk.download('stopwords')\n",
    "stop_words = nltk.corpus.stopwords.words('russian')\n",
    "\n",
    "# example of stop words\n",
    "stop_words[:10]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "string.punctuation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "word_tokenizer = nltk.WordPunctTokenizer()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dates = [str(x) for x in np.arange(0, 5000)]\n",
    "def process_data(data):\n",
    "    words = []\n",
    "    for sentence in data['sentence']:\n",
    "        # collect nlabels of news\n",
    "        sentence_lower = sentence.lower() # convert words in a text to lower case\n",
    "        tokens = word_tokenizer.tokenize(sentence_lower) # splits the text into tokens (words)\n",
    "        # remove punct and stop words from tokens\n",
    "        tokens = [word for word in tokens if (word not in string.punctuation and word not in stop_words and word not in dates)]\n",
    "        words.append(tokens) # collect the text tokens\n",
    "    return words"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "words = process_data(data)\n",
    "display(words)\n",
    "print()\n",
    "words[:3]\n",
    "print(\"Tokens: \", words[:5])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "! pip install pymorphy2"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%%time\n",
    "import pymorphy2\n",
    "morph = pymorphy2.MorphAnalyzer()\n",
    "\n",
    "morph_word = []\n",
    "for aword in words:\n",
    "    # aword_norm = morph.parse(aword)[0].normal_form\n",
    "    morph_word.append([morph.parse(word)[0].normal_form for word in aword])\n",
    "    # print(\"Before: %s, After: %s\" % (aword, aword_norm))\n",
    "morph_word[:5]\n",
    "# import pymorphy2\n",
    "# morph = pymorphy2.MorphAnalyzer()\n",
    "#\n",
    "# morph_words = []\n",
    "# for words_lst in words:\n",
    "#     morph_words.append([morph.parse(word)[0].normal_form for word in words_lst])\n",
    "# morph_words[:3]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "upgrade_morph_sent = list()\n",
    "tmp_morph_sent = str()\n",
    "for word in morph_word:\n",
    "    tmp_morph_sent = ''\n",
    "    for morph_1 in word:\n",
    "        tmp_morph_sent += morph_1 + ' '\n",
    "    upgrade_morph_sent.append(tmp_morph_sent)\n",
    "data['morph_sentence'] = upgrade_morph_sent\n",
    "data.info()\n",
    "data.head()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# morph_sentences = []\n",
    "# for word in morph_word:\n",
    "#     morph_sentence = str()\n",
    "#     for morph_wordd in word:\n",
    "#         morph_sentence += morph_wordd + ' '\n",
    "#     morph_sentences.append(morph_sentence)\n",
    "# data['processed_sentence'] = morph_sentences\n",
    "# data.info()\n",
    "# data.head()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer(max_features = 50)\n",
    "vectorizer.fit(list(data['morph_sentence']))\n",
    "\n",
    "# The top 10 words\n",
    "vectorizer.get_feature_names()[:10]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "bag =  vectorizer.transform(data['morph_sentence'])\n",
    "bag.todense()[:5]\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#calc tf-idf\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, TfidfTransformer\n",
    "# Fit TF-IDF on train texts\n",
    "vectorizer = TfidfVectorizer(max_features = 100) # select the top 100 words\n",
    "vectorizer.fit(data['morph_sentence'])\n",
    "\n",
    "# The top 10 words\n",
    "vectorizer.get_feature_names()[:10]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "td_idf_sentences = vectorizer.transform(data['morph_sentence'])\n",
    "td_idf_sentences.todense()[:5]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "###  Binary classification\n",
    "\n",
    "#### 8. [2 point] Train model using Logistic Regression (your own) and SVC (SVM can be taken from sklearn)\n",
    "\n",
    "* choose *two* authors from the dataset that you have formed in the previous task\n",
    "* check the balance of the classes\n",
    "* divide the data into train and test samples with 0.7 split rate (don't forget to fix the random state)\n",
    "* using GridSearchCV - find the best parameters for the models (by F1 score) and use it in the next tasks\n",
    "* make several plots to address the dependence between F1 score and parameters\n",
    "* plot confusion matrix for train and test samples\n",
    "* compute some relevant metrics for test sample (useful to check the seminars 5 and 6, use sklearn)\n",
    "* make conclusions about the performance of your models\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# your code here\n",
    "two = ['Gogol']\n",
    "new_data = data.query('author in [\"Pushkin\", \"Herzen\"]')\n",
    "display(new_data)\n",
    "display(new_data['author'].value_counts(normalize=True))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "features_train, features_valid, target_train, target_valid = train_test_split(new_data['morph_sentence'], new_data['author'], stratify=new_data['author'], test_size=0.3, random_state=12345)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 9. [1 point] Analysing ROC AUC\n",
    "\n",
    "It is possible to control the proportion of statistical errors of different types using different thresholds for choosing a class. Plot ROC curves for Logistic Regression and SVC, show the threshold on ROC curve plots. Choose such a threshold that your models have no more than 30% of false positive errors rate. Pay attention to `thresholds` parameter in sklearn roc_curve"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# your code here"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Multiclass logit"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 10. [1 point] Take the One-VS-One classifier (use sklearn) and apply to Logit model (one you've made in the 4th task) in order to get multiclass linear classifier\n",
    "\n",
    "<a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.multiclass.OneVsOneClassifier.html\">OneVsOneClassifier</a>\n",
    "\n",
    "* use the data you got at the previous step for 6 authors\n",
    "* divide the data into train and test samples with 0.7 split rate\n",
    "* using GridSearchCV - find the best parameters for the models (by F1 score)\n",
    "* plot confusion matrix for train and test samples\n",
    "* compute all possible and relevant metrics for test sample (use sklearn)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# your code here"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
