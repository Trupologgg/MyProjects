{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HSE 2022: Mathematical Methods for Data Analysis\n",
    "\n",
    "## Homework 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import statsmodels.api as sm\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "sns.set(style=\"darkgrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data\n",
    "\n",
    "For this homework we use Dataset from seaborn on diamonds prices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = sns.load_dataset('diamonds')\n",
    "\n",
    "y = data.price\n",
    "x = data.drop(['price'], axis=1)\n",
    "columns = data.drop(['price'], axis=1).columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 0. [0.25 points] Encode categorical variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "       carat        cut color clarity  depth  table  price     x     y     z\n0       0.23      Ideal     E     SI2   61.5   55.0    326  3.95  3.98  2.43\n1       0.21    Premium     E     SI1   59.8   61.0    326  3.89  3.84  2.31\n2       0.23       Good     E     VS1   56.9   65.0    327  4.05  4.07  2.31\n3       0.29    Premium     I     VS2   62.4   58.0    334  4.20  4.23  2.63\n4       0.31       Good     J     SI2   63.3   58.0    335  4.34  4.35  2.75\n...      ...        ...   ...     ...    ...    ...    ...   ...   ...   ...\n53935   0.72      Ideal     D     SI1   60.8   57.0   2757  5.75  5.76  3.50\n53936   0.72       Good     D     SI1   63.1   55.0   2757  5.69  5.75  3.61\n53937   0.70  Very Good     D     SI1   62.8   60.0   2757  5.66  5.68  3.56\n53938   0.86    Premium     H     SI2   61.0   58.0   2757  6.15  6.12  3.74\n53939   0.75      Ideal     D     SI2   62.2   55.0   2757  5.83  5.87  3.64\n\n[53940 rows x 10 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>carat</th>\n      <th>cut</th>\n      <th>color</th>\n      <th>clarity</th>\n      <th>depth</th>\n      <th>table</th>\n      <th>price</th>\n      <th>x</th>\n      <th>y</th>\n      <th>z</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.23</td>\n      <td>Ideal</td>\n      <td>E</td>\n      <td>SI2</td>\n      <td>61.5</td>\n      <td>55.0</td>\n      <td>326</td>\n      <td>3.95</td>\n      <td>3.98</td>\n      <td>2.43</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.21</td>\n      <td>Premium</td>\n      <td>E</td>\n      <td>SI1</td>\n      <td>59.8</td>\n      <td>61.0</td>\n      <td>326</td>\n      <td>3.89</td>\n      <td>3.84</td>\n      <td>2.31</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.23</td>\n      <td>Good</td>\n      <td>E</td>\n      <td>VS1</td>\n      <td>56.9</td>\n      <td>65.0</td>\n      <td>327</td>\n      <td>4.05</td>\n      <td>4.07</td>\n      <td>2.31</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.29</td>\n      <td>Premium</td>\n      <td>I</td>\n      <td>VS2</td>\n      <td>62.4</td>\n      <td>58.0</td>\n      <td>334</td>\n      <td>4.20</td>\n      <td>4.23</td>\n      <td>2.63</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.31</td>\n      <td>Good</td>\n      <td>J</td>\n      <td>SI2</td>\n      <td>63.3</td>\n      <td>58.0</td>\n      <td>335</td>\n      <td>4.34</td>\n      <td>4.35</td>\n      <td>2.75</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>53935</th>\n      <td>0.72</td>\n      <td>Ideal</td>\n      <td>D</td>\n      <td>SI1</td>\n      <td>60.8</td>\n      <td>57.0</td>\n      <td>2757</td>\n      <td>5.75</td>\n      <td>5.76</td>\n      <td>3.50</td>\n    </tr>\n    <tr>\n      <th>53936</th>\n      <td>0.72</td>\n      <td>Good</td>\n      <td>D</td>\n      <td>SI1</td>\n      <td>63.1</td>\n      <td>55.0</td>\n      <td>2757</td>\n      <td>5.69</td>\n      <td>5.75</td>\n      <td>3.61</td>\n    </tr>\n    <tr>\n      <th>53937</th>\n      <td>0.70</td>\n      <td>Very Good</td>\n      <td>D</td>\n      <td>SI1</td>\n      <td>62.8</td>\n      <td>60.0</td>\n      <td>2757</td>\n      <td>5.66</td>\n      <td>5.68</td>\n      <td>3.56</td>\n    </tr>\n    <tr>\n      <th>53938</th>\n      <td>0.86</td>\n      <td>Premium</td>\n      <td>H</td>\n      <td>SI2</td>\n      <td>61.0</td>\n      <td>58.0</td>\n      <td>2757</td>\n      <td>6.15</td>\n      <td>6.12</td>\n      <td>3.74</td>\n    </tr>\n    <tr>\n      <th>53939</th>\n      <td>0.75</td>\n      <td>Ideal</td>\n      <td>D</td>\n      <td>SI2</td>\n      <td>62.2</td>\n      <td>55.0</td>\n      <td>2757</td>\n      <td>5.83</td>\n      <td>5.87</td>\n      <td>3.64</td>\n    </tr>\n  </tbody>\n</table>\n<p>53940 rows × 10 columns</p>\n</div>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "display(data)\n",
    "\n",
    "encoder = OrdinalEncoder()\n",
    "data_ordinal = pd.DataFrame(encoder.fit_transform(data),\n",
    "                            columns=data.columns)\n",
    "features = data_ordinal.drop('price', axis=1)\n",
    "target = data_ordinal['price']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. [0.25 points] Split the data into train and test sets with ratio 80:20 with random_state=17."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_train, features_test, target_train, target_test = train_test_split(features, target, test_size=0.2,\n",
    "                                                                            random_state=17)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. [1 point] Train models on train data using StatsModels library and apply it to the test set; use $RMSE$ and $R^2$ as the quality measure.\n",
    "\n",
    "* [`LinearRegression`](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html);\n",
    "* [`Ridge`](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html) with $\\alpha = 0.01$;\n",
    "* [`Lasso`](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lasso.html) with $\\alpha = 0.01$\n",
    "* [`ElasticNet`](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.ElasticNet.html) with $\\alpha = 0.01$, $l_{1}$_$ratio = 0.6$\n",
    "\n",
    "Don't forget to scale the data before training the models with StandardScaler!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "numerics = ['carat', 'depth', 'table', 'x', 'y', 'z']\n",
    "scaler.fit(features_train[numerics])\n",
    "features_train[numerics] = scaler.transform(features_train[numerics])\n",
    "features_test[numerics] = scaler.transform(features_test[numerics])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE = 776022.1336650986\n",
      "RMSE = 880.9211847067243\n",
      "R2 =  0.9148058965444855\n"
     ]
    }
   ],
   "source": [
    "features_train = sm.add_constant(features_train)\n",
    "features_test = sm.add_constant(features_test)\n",
    "\n",
    "linear_model = sm.OLS(target_train, features_train)\n",
    "linear_results = linear_model.fit()\n",
    "predicted_test = linear_results.predict(features_test)\n",
    "mse = mean_squared_error(y_true=target_test, y_pred=predicted_test)\n",
    "rmse = mse ** .5\n",
    "print(\"MSE =\", mse)\n",
    "print(\"RMSE =\", rmse)\n",
    "print(\"R2 = \", r2_score(y_true=target_test, y_pred=predicted_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE = 776022.1336650986\n",
      "RMSE = 880.9211847067243\n",
      "R2 =  0.9125959534506798\n"
     ]
    }
   ],
   "source": [
    "ridge_model = linear_model.fit_regularized(L1_wt=0, alpha=0.01)\n",
    "ridge_result = sm.regression.linear_model.OLSResults(linear_model, ridge_model.params,\n",
    "                                                     linear_model.normalized_cov_params)\n",
    "predicted_ridge = ridge_model.predict(features_test)\n",
    "mse = mean_squared_error(y_true=target_test, y_pred=predicted_test)\n",
    "rmse = mse ** .5\n",
    "print(\"MSE =\", mse)\n",
    "print(\"RMSE =\", rmse)\n",
    "print('R2 = ', r2_score(y_true=target_test, y_pred=predicted_ridge))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE = 777573.0826038037\n",
      "RMSE = 881.8010447962758\n",
      "R2 =  0.9146356285861295\n"
     ]
    }
   ],
   "source": [
    "lasso_model = linear_model.fit_regularized(L1_wt=1, alpha=0.01)\n",
    "lasso_result = sm.regression.linear_model.OLSResults(linear_model, lasso_model.params,\n",
    "                                                     linear_model.normalized_cov_params)\n",
    "predicted_lasso = lasso_model.predict(features_test)\n",
    "mse = mean_squared_error(y_true=target_test, y_pred=predicted_lasso)\n",
    "rmse = mse ** .5\n",
    "print(\"MSE =\", mse)\n",
    "print(\"RMSE =\", rmse)\n",
    "print('R2 = ', r2_score(y_true=target_test, y_pred=predicted_lasso))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE = 776022.1336650986\n",
      "RMSE = 880.9211847067243\n",
      "R2 =  0.9141784641073025\n"
     ]
    }
   ],
   "source": [
    "elastic_model = linear_model.fit_regularized(L1_wt=0.6, alpha=0.01)\n",
    "elastic_result = sm.regression.linear_model.OLSResults(linear_model, elastic_model.params,\n",
    "                                                       linear_model.normalized_cov_params)\n",
    "predicted_elastic = elastic_model.predict(features_test)\n",
    "mse = mean_squared_error(y_true=target_test, y_pred=predicted_test)\n",
    "rmse = mse ** .5\n",
    "print(\"MSE =\", mse)\n",
    "print(\"RMSE =\", rmse)\n",
    "print('R2 = ', r2_score(y_true=target_test, y_pred=predicted_elastic))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. [1 point] Explore the values of the parameters of the resulting models and compare the number of zero weights in them. Comment on the significance of the coefficients, overal model significance and other related factors from the results table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "<class 'statsmodels.iolib.summary2.Summary'>\n\"\"\"\n                  Results: Ordinary least squares\n====================================================================\nModel:              OLS              Adj. R-squared:     0.914      \nDependent Variable: price            AIC:                707856.7639\nDate:               2022-12-08 03:47 BIC:                707943.4887\nNo. Observations:   43152            Log-Likelihood:     -3.5392e+05\nDf Model:           9                F-statistic:        5.078e+04  \nDf Residuals:       43142            Prob (F-statistic): 0.00       \nR-squared:          0.914            Scale:              7.7894e+05 \n--------------------------------------------------------------------\n             Coef.    Std.Err.    t     P>|t|    [0.025     0.975]  \n--------------------------------------------------------------------\nconst       2624.2705  16.3907 160.1069 0.0000  2592.1444  2656.3967\ncarat       2699.0777  21.4817 125.6453 0.0000  2656.9731  2741.1823\ncut           52.8693   4.2725  12.3744 0.0000    44.4951    61.2434\ncolor       -190.5337   2.6168 -72.8112 0.0000  -195.6627  -185.4047\nclarity      214.5259   2.5507  84.1037 0.0000   209.5264   219.5253\ndepth        -46.6299   9.4944  -4.9113 0.0000   -65.2391   -28.0208\ntable       -121.5332   4.6368 -26.2106 0.0000  -130.6214  -112.4450\nx          -1372.6718  89.2151 -15.3861 0.0000 -1547.5351 -1197.8085\ny           1893.3603  87.5187  21.6338 0.0000  1721.8220  2064.8987\nz           -188.8980  66.9466  -2.8216 0.0048  -320.1145   -57.6814\n--------------------------------------------------------------------\nOmnibus:              7912.396      Durbin-Watson:         1.997    \nProb(Omnibus):        0.000         Jarque-Bera (JB):      51815.141\nSkew:                 0.724         Prob(JB):              0.000    \nKurtosis:             8.169         Condition No.:         152      \n====================================================================\n\n\"\"\"",
      "text/html": "<table class=\"simpletable\">\n<tr>\n        <td>Model:</td>               <td>OLS</td>         <td>Adj. R-squared:</td>      <td>0.914</td>   \n</tr>\n<tr>\n  <td>Dependent Variable:</td>       <td>price</td>             <td>AIC:</td>         <td>707856.7639</td>\n</tr>\n<tr>\n         <td>Date:</td>        <td>2022-12-08 03:47</td>        <td>BIC:</td>         <td>707943.4887</td>\n</tr>\n<tr>\n   <td>No. Observations:</td>        <td>43152</td>        <td>Log-Likelihood:</td>   <td>-3.5392e+05</td>\n</tr>\n<tr>\n       <td>Df Model:</td>              <td>9</td>           <td>F-statistic:</td>      <td>5.078e+04</td> \n</tr>\n<tr>\n     <td>Df Residuals:</td>          <td>43142</td>      <td>Prob (F-statistic):</td>    <td>0.00</td>    \n</tr>\n<tr>\n      <td>R-squared:</td>            <td>0.914</td>            <td>Scale:</td>        <td>7.7894e+05</td> \n</tr>\n</table>\n<table class=\"simpletable\">\n<tr>\n     <td></td>        <th>Coef.</th>   <th>Std.Err.</th>     <th>t</th>     <th>P>|t|</th>   <th>[0.025</th>     <th>0.975]</th>  \n</tr>\n<tr>\n  <th>const</th>    <td>2624.2705</td>  <td>16.3907</td> <td>160.1069</td> <td>0.0000</td>  <td>2592.1444</td>  <td>2656.3967</td>\n</tr>\n<tr>\n  <th>carat</th>    <td>2699.0777</td>  <td>21.4817</td> <td>125.6453</td> <td>0.0000</td>  <td>2656.9731</td>  <td>2741.1823</td>\n</tr>\n<tr>\n  <th>cut</th>       <td>52.8693</td>   <td>4.2725</td>   <td>12.3744</td> <td>0.0000</td>   <td>44.4951</td>    <td>61.2434</td> \n</tr>\n<tr>\n  <th>color</th>    <td>-190.5337</td>  <td>2.6168</td>  <td>-72.8112</td> <td>0.0000</td>  <td>-195.6627</td>  <td>-185.4047</td>\n</tr>\n<tr>\n  <th>clarity</th>  <td>214.5259</td>   <td>2.5507</td>   <td>84.1037</td> <td>0.0000</td>  <td>209.5264</td>   <td>219.5253</td> \n</tr>\n<tr>\n  <th>depth</th>    <td>-46.6299</td>   <td>9.4944</td>   <td>-4.9113</td> <td>0.0000</td>  <td>-65.2391</td>   <td>-28.0208</td> \n</tr>\n<tr>\n  <th>table</th>    <td>-121.5332</td>  <td>4.6368</td>  <td>-26.2106</td> <td>0.0000</td>  <td>-130.6214</td>  <td>-112.4450</td>\n</tr>\n<tr>\n  <th>x</th>       <td>-1372.6718</td>  <td>89.2151</td> <td>-15.3861</td> <td>0.0000</td> <td>-1547.5351</td> <td>-1197.8085</td>\n</tr>\n<tr>\n  <th>y</th>        <td>1893.3603</td>  <td>87.5187</td>  <td>21.6338</td> <td>0.0000</td>  <td>1721.8220</td>  <td>2064.8987</td>\n</tr>\n<tr>\n  <th>z</th>        <td>-188.8980</td>  <td>66.9466</td>  <td>-2.8216</td> <td>0.0048</td>  <td>-320.1145</td>  <td>-57.6814</td> \n</tr>\n</table>\n<table class=\"simpletable\">\n<tr>\n     <td>Omnibus:</td>    <td>7912.396</td>  <td>Durbin-Watson:</td>     <td>1.997</td>  \n</tr>\n<tr>\n  <td>Prob(Omnibus):</td>   <td>0.000</td>  <td>Jarque-Bera (JB):</td> <td>51815.141</td>\n</tr>\n<tr>\n       <td>Skew:</td>       <td>0.724</td>      <td>Prob(JB):</td>       <td>0.000</td>  \n</tr>\n<tr>\n     <td>Kurtosis:</td>     <td>8.169</td>   <td>Condition No.:</td>      <td>152</td>   \n</tr>\n</table>"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear_results.summary2()"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "На уровне значимости 0.05 нет признаков имеющие нулевые веса."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "array([], dtype=object)"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear_results.pvalues[linear_results.pvalues > 0.05].index.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "<class 'statsmodels.iolib.summary2.Summary'>\n\"\"\"\n                  Results: Ordinary least squares\n====================================================================\nModel:              OLS              Adj. R-squared:     0.911      \nDependent Variable: price            AIC:                709152.9296\nDate:               2022-12-08 03:47 BIC:                709239.6544\nNo. Observations:   43152            Log-Likelihood:     -3.5457e+05\nDf Model:           9                F-statistic:        4.913e+04  \nDf Residuals:       43142            Prob (F-statistic): 0.00       \nR-squared:          0.911            Scale:              8.0269e+05 \n---------------------------------------------------------------------\n            Coef.    Std.Err.     t      P>|t|     [0.025     0.975] \n---------------------------------------------------------------------\nconst     2238.4162   16.6388  134.5302  0.0000  2205.8039  2271.0285\ncarat     2165.6306   21.8068   99.3099  0.0000  2122.8888  2208.3723\ncut        117.0268    4.3371   26.9825  0.0000   108.5260   125.5277\ncolor     -161.3356    2.6564  -60.7343  0.0000  -166.5422  -156.1290\nclarity    246.8583    2.5893   95.3368  0.0000   241.7832   251.9334\ndepth      -56.4293    9.6380   -5.8549  0.0000   -75.3201   -37.5386\ntable     -128.3472    4.7070  -27.2675  0.0000  -137.5730  -119.1215\nx          169.0071   90.5651    1.8661  0.0620    -8.5022   346.5164\ny          500.0043   88.8430    5.6280  0.0000   325.8702   674.1383\nz          183.2733   67.9596    2.6968  0.0070    50.0712   316.4755\n--------------------------------------------------------------------\nOmnibus:              8038.335      Durbin-Watson:         1.996    \nProb(Omnibus):        0.000         Jarque-Bera (JB):      36234.753\nSkew:                 0.849         Prob(JB):              0.000    \nKurtosis:             7.156         Condition No.:         152      \n====================================================================\n\n\"\"\"",
      "text/html": "<table class=\"simpletable\">\n<tr>\n        <td>Model:</td>               <td>OLS</td>         <td>Adj. R-squared:</td>      <td>0.911</td>   \n</tr>\n<tr>\n  <td>Dependent Variable:</td>       <td>price</td>             <td>AIC:</td>         <td>709152.9296</td>\n</tr>\n<tr>\n         <td>Date:</td>        <td>2022-12-08 03:47</td>        <td>BIC:</td>         <td>709239.6544</td>\n</tr>\n<tr>\n   <td>No. Observations:</td>        <td>43152</td>        <td>Log-Likelihood:</td>   <td>-3.5457e+05</td>\n</tr>\n<tr>\n       <td>Df Model:</td>              <td>9</td>           <td>F-statistic:</td>      <td>4.913e+04</td> \n</tr>\n<tr>\n     <td>Df Residuals:</td>          <td>43142</td>      <td>Prob (F-statistic):</td>    <td>0.00</td>    \n</tr>\n<tr>\n      <td>R-squared:</td>            <td>0.911</td>            <td>Scale:</td>        <td>8.0269e+05</td> \n</tr>\n</table>\n<table class=\"simpletable\">\n<tr>\n     <td></td>       <th>Coef.</th>   <th>Std.Err.</th>     <th>t</th>     <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n</tr>\n<tr>\n  <th>const</th>   <td>2238.4162</td>  <td>16.6388</td> <td>134.5302</td> <td>0.0000</td> <td>2205.8039</td> <td>2271.0285</td>\n</tr>\n<tr>\n  <th>carat</th>   <td>2165.6306</td>  <td>21.8068</td>  <td>99.3099</td> <td>0.0000</td> <td>2122.8888</td> <td>2208.3723</td>\n</tr>\n<tr>\n  <th>cut</th>     <td>117.0268</td>   <td>4.3371</td>   <td>26.9825</td> <td>0.0000</td> <td>108.5260</td>  <td>125.5277</td> \n</tr>\n<tr>\n  <th>color</th>   <td>-161.3356</td>  <td>2.6564</td>  <td>-60.7343</td> <td>0.0000</td> <td>-166.5422</td> <td>-156.1290</td>\n</tr>\n<tr>\n  <th>clarity</th> <td>246.8583</td>   <td>2.5893</td>   <td>95.3368</td> <td>0.0000</td> <td>241.7832</td>  <td>251.9334</td> \n</tr>\n<tr>\n  <th>depth</th>   <td>-56.4293</td>   <td>9.6380</td>   <td>-5.8549</td> <td>0.0000</td> <td>-75.3201</td>  <td>-37.5386</td> \n</tr>\n<tr>\n  <th>table</th>   <td>-128.3472</td>  <td>4.7070</td>  <td>-27.2675</td> <td>0.0000</td> <td>-137.5730</td> <td>-119.1215</td>\n</tr>\n<tr>\n  <th>x</th>       <td>169.0071</td>   <td>90.5651</td>  <td>1.8661</td>  <td>0.0620</td>  <td>-8.5022</td>  <td>346.5164</td> \n</tr>\n<tr>\n  <th>y</th>       <td>500.0043</td>   <td>88.8430</td>  <td>5.6280</td>  <td>0.0000</td> <td>325.8702</td>  <td>674.1383</td> \n</tr>\n<tr>\n  <th>z</th>       <td>183.2733</td>   <td>67.9596</td>  <td>2.6968</td>  <td>0.0070</td>  <td>50.0712</td>  <td>316.4755</td> \n</tr>\n</table>\n<table class=\"simpletable\">\n<tr>\n     <td>Omnibus:</td>    <td>8038.335</td>  <td>Durbin-Watson:</td>     <td>1.996</td>  \n</tr>\n<tr>\n  <td>Prob(Omnibus):</td>   <td>0.000</td>  <td>Jarque-Bera (JB):</td> <td>36234.753</td>\n</tr>\n<tr>\n       <td>Skew:</td>       <td>0.849</td>      <td>Prob(JB):</td>       <td>0.000</td>  \n</tr>\n<tr>\n     <td>Kurtosis:</td>     <td>7.156</td>   <td>Condition No.:</td>      <td>152</td>   \n</tr>\n</table>"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ridge_result.summary2()"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "На уровне значимости 0.05 признак ['x'] имеет нулевой вес."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "array(['x'], dtype=object)"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_train.columns[ridge_result.pvalues > 0.05].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "<class 'statsmodels.iolib.summary2.Summary'>\n\"\"\"\n                  Results: Ordinary least squares\n====================================================================\nModel:              OLS              Adj. R-squared:     0.913      \nDependent Variable: price            AIC:                708261.0125\nDate:               2022-12-08 03:47 BIC:                708347.7374\nNo. Observations:   43152            Log-Likelihood:     -3.5412e+05\nDf Model:           9                F-statistic:        5.026e+04  \nDf Residuals:       43142            Prob (F-statistic): 0.00       \nR-squared:          0.913            Scale:              7.8627e+05 \n---------------------------------------------------------------------\n            Coef.    Std.Err.     t      P>|t|     [0.025     0.975] \n---------------------------------------------------------------------\nconst     2593.6833   16.4677  157.5013  0.0000  2561.4063  2625.9603\ncarat     2702.1952   21.5826  125.2026  0.0000  2659.8930  2744.4975\ncut         62.5648    4.2925   14.5752  0.0000    54.1514    70.9783\ncolor     -190.6321    2.6291  -72.5084  0.0000  -195.7852  -185.4790\nclarity    216.1488    2.5627   84.3440  0.0000   211.1259   221.1718\ndepth      -59.4398    9.5390   -6.2313  0.0000   -78.1363   -40.7433\ntable     -134.7719    4.6586  -28.9299  0.0000  -143.9028  -125.6410\nx          238.3825   89.6340    2.6595  0.0078    62.6982   414.0668\ny          239.0742   87.9296    2.7189  0.0066    66.7305   411.4179\nz         -145.8133   67.2609   -2.1679  0.0302  -277.6459   -13.9807\n--------------------------------------------------------------------\nOmnibus:              7859.417      Durbin-Watson:         1.996    \nProb(Omnibus):        0.000         Jarque-Bera (JB):      53481.710\nSkew:                 0.706         Prob(JB):              0.000    \nKurtosis:             8.268         Condition No.:         152      \n====================================================================\n\n\"\"\"",
      "text/html": "<table class=\"simpletable\">\n<tr>\n        <td>Model:</td>               <td>OLS</td>         <td>Adj. R-squared:</td>      <td>0.913</td>   \n</tr>\n<tr>\n  <td>Dependent Variable:</td>       <td>price</td>             <td>AIC:</td>         <td>708261.0125</td>\n</tr>\n<tr>\n         <td>Date:</td>        <td>2022-12-08 03:47</td>        <td>BIC:</td>         <td>708347.7374</td>\n</tr>\n<tr>\n   <td>No. Observations:</td>        <td>43152</td>        <td>Log-Likelihood:</td>   <td>-3.5412e+05</td>\n</tr>\n<tr>\n       <td>Df Model:</td>              <td>9</td>           <td>F-statistic:</td>      <td>5.026e+04</td> \n</tr>\n<tr>\n     <td>Df Residuals:</td>          <td>43142</td>      <td>Prob (F-statistic):</td>    <td>0.00</td>    \n</tr>\n<tr>\n      <td>R-squared:</td>            <td>0.913</td>            <td>Scale:</td>        <td>7.8627e+05</td> \n</tr>\n</table>\n<table class=\"simpletable\">\n<tr>\n     <td></td>       <th>Coef.</th>   <th>Std.Err.</th>     <th>t</th>     <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n</tr>\n<tr>\n  <th>const</th>   <td>2593.6833</td>  <td>16.4677</td> <td>157.5013</td> <td>0.0000</td> <td>2561.4063</td> <td>2625.9603</td>\n</tr>\n<tr>\n  <th>carat</th>   <td>2702.1952</td>  <td>21.5826</td> <td>125.2026</td> <td>0.0000</td> <td>2659.8930</td> <td>2744.4975</td>\n</tr>\n<tr>\n  <th>cut</th>      <td>62.5648</td>   <td>4.2925</td>   <td>14.5752</td> <td>0.0000</td>  <td>54.1514</td>   <td>70.9783</td> \n</tr>\n<tr>\n  <th>color</th>   <td>-190.6321</td>  <td>2.6291</td>  <td>-72.5084</td> <td>0.0000</td> <td>-195.7852</td> <td>-185.4790</td>\n</tr>\n<tr>\n  <th>clarity</th> <td>216.1488</td>   <td>2.5627</td>   <td>84.3440</td> <td>0.0000</td> <td>211.1259</td>  <td>221.1718</td> \n</tr>\n<tr>\n  <th>depth</th>   <td>-59.4398</td>   <td>9.5390</td>   <td>-6.2313</td> <td>0.0000</td> <td>-78.1363</td>  <td>-40.7433</td> \n</tr>\n<tr>\n  <th>table</th>   <td>-134.7719</td>  <td>4.6586</td>  <td>-28.9299</td> <td>0.0000</td> <td>-143.9028</td> <td>-125.6410</td>\n</tr>\n<tr>\n  <th>x</th>       <td>238.3825</td>   <td>89.6340</td>  <td>2.6595</td>  <td>0.0078</td>  <td>62.6982</td>  <td>414.0668</td> \n</tr>\n<tr>\n  <th>y</th>       <td>239.0742</td>   <td>87.9296</td>  <td>2.7189</td>  <td>0.0066</td>  <td>66.7305</td>  <td>411.4179</td> \n</tr>\n<tr>\n  <th>z</th>       <td>-145.8133</td>  <td>67.2609</td>  <td>-2.1679</td> <td>0.0302</td> <td>-277.6459</td> <td>-13.9807</td> \n</tr>\n</table>\n<table class=\"simpletable\">\n<tr>\n     <td>Omnibus:</td>    <td>7859.417</td>  <td>Durbin-Watson:</td>     <td>1.996</td>  \n</tr>\n<tr>\n  <td>Prob(Omnibus):</td>   <td>0.000</td>  <td>Jarque-Bera (JB):</td> <td>53481.710</td>\n</tr>\n<tr>\n       <td>Skew:</td>       <td>0.706</td>      <td>Prob(JB):</td>       <td>0.000</td>  \n</tr>\n<tr>\n     <td>Kurtosis:</td>     <td>8.268</td>   <td>Condition No.:</td>      <td>152</td>   \n</tr>\n</table>"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lasso_result.summary2()"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "На уровне значимости 0.05 нет нулевых признаков."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "array([], dtype=object)"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_train.columns[lasso_result.pvalues > 0.05].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "<class 'statsmodels.iolib.summary2.Summary'>\n\"\"\"\n                  Results: Ordinary least squares\n====================================================================\nModel:              OLS              Adj. R-squared:     0.913      \nDependent Variable: price            AIC:                708461.6585\nDate:               2022-12-08 03:47 BIC:                708548.3834\nNo. Observations:   43152            Log-Likelihood:     -3.5422e+05\nDf Model:           9                F-statistic:        5.000e+04  \nDf Residuals:       43142            Prob (F-statistic): 0.00       \nR-squared:          0.913            Scale:              7.8994e+05 \n---------------------------------------------------------------------\n            Coef.    Std.Err.     t      P>|t|     [0.025     0.975] \n---------------------------------------------------------------------\nconst     2436.0075   16.5060  147.5830  0.0000  2403.6553  2468.3596\ncarat     2459.2494   21.6328  113.6814  0.0000  2416.8486  2501.6501\ncut         87.3153    4.3025   20.2939  0.0000    78.8822    95.7483\ncolor     -177.7663    2.6352  -67.4577  0.0000  -182.9314  -172.6012\nclarity    229.5775    2.5687   89.3760  0.0000   224.5429   234.6122\ndepth      -45.6816    9.5612   -4.7778  0.0000   -64.4217   -26.9416\ntable     -132.9628    4.6694  -28.4753  0.0000  -142.1149  -123.8106\nx          396.1678   89.8426    4.4096  0.0000   220.0746   572.2610\ny          276.1604   88.1343    3.1334  0.0017   103.4155   448.9052\nz         -103.6976   67.4174   -1.5381  0.1240  -235.8371    28.4418\n--------------------------------------------------------------------\nOmnibus:              7996.346      Durbin-Watson:         1.996    \nProb(Omnibus):        0.000         Jarque-Bera (JB):      44977.191\nSkew:                 0.779         Prob(JB):              0.000    \nKurtosis:             7.753         Condition No.:         152      \n====================================================================\n\n\"\"\"",
      "text/html": "<table class=\"simpletable\">\n<tr>\n        <td>Model:</td>               <td>OLS</td>         <td>Adj. R-squared:</td>      <td>0.913</td>   \n</tr>\n<tr>\n  <td>Dependent Variable:</td>       <td>price</td>             <td>AIC:</td>         <td>708461.6585</td>\n</tr>\n<tr>\n         <td>Date:</td>        <td>2022-12-08 03:47</td>        <td>BIC:</td>         <td>708548.3834</td>\n</tr>\n<tr>\n   <td>No. Observations:</td>        <td>43152</td>        <td>Log-Likelihood:</td>   <td>-3.5422e+05</td>\n</tr>\n<tr>\n       <td>Df Model:</td>              <td>9</td>           <td>F-statistic:</td>      <td>5.000e+04</td> \n</tr>\n<tr>\n     <td>Df Residuals:</td>          <td>43142</td>      <td>Prob (F-statistic):</td>    <td>0.00</td>    \n</tr>\n<tr>\n      <td>R-squared:</td>            <td>0.913</td>            <td>Scale:</td>        <td>7.8994e+05</td> \n</tr>\n</table>\n<table class=\"simpletable\">\n<tr>\n     <td></td>       <th>Coef.</th>   <th>Std.Err.</th>     <th>t</th>     <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n</tr>\n<tr>\n  <th>const</th>   <td>2436.0075</td>  <td>16.5060</td> <td>147.5830</td> <td>0.0000</td> <td>2403.6553</td> <td>2468.3596</td>\n</tr>\n<tr>\n  <th>carat</th>   <td>2459.2494</td>  <td>21.6328</td> <td>113.6814</td> <td>0.0000</td> <td>2416.8486</td> <td>2501.6501</td>\n</tr>\n<tr>\n  <th>cut</th>      <td>87.3153</td>   <td>4.3025</td>   <td>20.2939</td> <td>0.0000</td>  <td>78.8822</td>   <td>95.7483</td> \n</tr>\n<tr>\n  <th>color</th>   <td>-177.7663</td>  <td>2.6352</td>  <td>-67.4577</td> <td>0.0000</td> <td>-182.9314</td> <td>-172.6012</td>\n</tr>\n<tr>\n  <th>clarity</th> <td>229.5775</td>   <td>2.5687</td>   <td>89.3760</td> <td>0.0000</td> <td>224.5429</td>  <td>234.6122</td> \n</tr>\n<tr>\n  <th>depth</th>   <td>-45.6816</td>   <td>9.5612</td>   <td>-4.7778</td> <td>0.0000</td> <td>-64.4217</td>  <td>-26.9416</td> \n</tr>\n<tr>\n  <th>table</th>   <td>-132.9628</td>  <td>4.6694</td>  <td>-28.4753</td> <td>0.0000</td> <td>-142.1149</td> <td>-123.8106</td>\n</tr>\n<tr>\n  <th>x</th>       <td>396.1678</td>   <td>89.8426</td>  <td>4.4096</td>  <td>0.0000</td> <td>220.0746</td>  <td>572.2610</td> \n</tr>\n<tr>\n  <th>y</th>       <td>276.1604</td>   <td>88.1343</td>  <td>3.1334</td>  <td>0.0017</td> <td>103.4155</td>  <td>448.9052</td> \n</tr>\n<tr>\n  <th>z</th>       <td>-103.6976</td>  <td>67.4174</td>  <td>-1.5381</td> <td>0.1240</td> <td>-235.8371</td>  <td>28.4418</td> \n</tr>\n</table>\n<table class=\"simpletable\">\n<tr>\n     <td>Omnibus:</td>    <td>7996.346</td>  <td>Durbin-Watson:</td>     <td>1.996</td>  \n</tr>\n<tr>\n  <td>Prob(Omnibus):</td>   <td>0.000</td>  <td>Jarque-Bera (JB):</td> <td>44977.191</td>\n</tr>\n<tr>\n       <td>Skew:</td>       <td>0.779</td>      <td>Prob(JB):</td>       <td>0.000</td>  \n</tr>\n<tr>\n     <td>Kurtosis:</td>     <td>7.753</td>   <td>Condition No.:</td>      <td>152</td>   \n</tr>\n</table>"
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "elastic_result.summary2()"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "На уровне значимости 0.05 есть нулевой признак ['y']."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "array(['z'], dtype=object)"
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_train.columns[elastic_result.pvalues > 0.05].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. [1 point] Implement one of the elimination algorithms that were described in the Seminar_4 (Elimination by P-value, Forward elimination, Backward elimination), make conclusions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def elimination(model, alpha):\n",
    "    if isinstance(model, sm.OLS):\n",
    "        features_tmp = model.exog\n",
    "        target_tmp = model.endog\n",
    "        max_iter = model.exog.shape[1]\n",
    "\n",
    "        dropping(alpha, features_tmp, max_iter, target_tmp)\n",
    "\n",
    "        return sm.OLS(target_tmp, features_tmp)\n",
    "\n",
    "    raise Exception('Not supported Model')\n",
    "\n",
    "\n",
    "def dropping(alpha, features_tmp, max_iter, target_tmp):\n",
    "    for i in range(max_iter):\n",
    "        results_tmp = sm.OLS(target_tmp, features_tmp).fit()\n",
    "        max_pvalue = results_tmp.pvalues.max()\n",
    "        if max_pvalue >= alpha:\n",
    "            col_to_drop = results_tmp.pvalues.index.values[results_tmp.pvalues.argmax()]\n",
    "            features_tmp.drop(labels=col_to_drop, axis=1, level=0, inplace=True)\n",
    "            continue\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "<class 'statsmodels.iolib.summary2.Summary'>\n\"\"\"\n                  Results: Ordinary least squares\n====================================================================\nModel:              OLS              Adj. R-squared:     0.914      \nDependent Variable: y                AIC:                707856.7639\nDate:               2022-12-08 03:47 BIC:                707943.4887\nNo. Observations:   43152            Log-Likelihood:     -3.5392e+05\nDf Model:           9                F-statistic:        5.078e+04  \nDf Residuals:       43142            Prob (F-statistic): 0.00       \nR-squared:          0.914            Scale:              7.7894e+05 \n---------------------------------------------------------------------\n         Coef.     Std.Err.     t      P>|t|     [0.025      0.975]  \n---------------------------------------------------------------------\nconst   2624.2705   16.3907  160.1069  0.0000   2592.1444   2656.3967\nx1      2699.0777   21.4817  125.6453  0.0000   2656.9731   2741.1823\nx2        52.8693    4.2725   12.3744  0.0000     44.4951     61.2434\nx3      -190.5337    2.6168  -72.8112  0.0000   -195.6627   -185.4047\nx4       214.5259    2.5507   84.1037  0.0000    209.5264    219.5253\nx5       -46.6299    9.4944   -4.9113  0.0000    -65.2391    -28.0208\nx6      -121.5332    4.6368  -26.2106  0.0000   -130.6214   -112.4450\nx7     -1372.6718   89.2151  -15.3861  0.0000  -1547.5351  -1197.8085\nx8      1893.3603   87.5187   21.6338  0.0000   1721.8220   2064.8987\nx9      -188.8980   66.9466   -2.8216  0.0048   -320.1145    -57.6814\n--------------------------------------------------------------------\nOmnibus:              7912.396      Durbin-Watson:         1.997    \nProb(Omnibus):        0.000         Jarque-Bera (JB):      51815.141\nSkew:                 0.724         Prob(JB):              0.000    \nKurtosis:             8.169         Condition No.:         152      \n====================================================================\n\n\"\"\"",
      "text/html": "<table class=\"simpletable\">\n<tr>\n        <td>Model:</td>               <td>OLS</td>         <td>Adj. R-squared:</td>      <td>0.914</td>   \n</tr>\n<tr>\n  <td>Dependent Variable:</td>         <td>y</td>               <td>AIC:</td>         <td>707856.7639</td>\n</tr>\n<tr>\n         <td>Date:</td>        <td>2022-12-08 03:47</td>        <td>BIC:</td>         <td>707943.4887</td>\n</tr>\n<tr>\n   <td>No. Observations:</td>        <td>43152</td>        <td>Log-Likelihood:</td>   <td>-3.5392e+05</td>\n</tr>\n<tr>\n       <td>Df Model:</td>              <td>9</td>           <td>F-statistic:</td>      <td>5.078e+04</td> \n</tr>\n<tr>\n     <td>Df Residuals:</td>          <td>43142</td>      <td>Prob (F-statistic):</td>    <td>0.00</td>    \n</tr>\n<tr>\n      <td>R-squared:</td>            <td>0.914</td>            <td>Scale:</td>        <td>7.7894e+05</td> \n</tr>\n</table>\n<table class=\"simpletable\">\n<tr>\n    <td></td>       <th>Coef.</th>   <th>Std.Err.</th>     <th>t</th>     <th>P>|t|</th>   <th>[0.025</th>     <th>0.975]</th>  \n</tr>\n<tr>\n  <th>const</th>  <td>2624.2705</td>  <td>16.3907</td> <td>160.1069</td> <td>0.0000</td>  <td>2592.1444</td>  <td>2656.3967</td>\n</tr>\n<tr>\n  <th>x1</th>     <td>2699.0777</td>  <td>21.4817</td> <td>125.6453</td> <td>0.0000</td>  <td>2656.9731</td>  <td>2741.1823</td>\n</tr>\n<tr>\n  <th>x2</th>      <td>52.8693</td>   <td>4.2725</td>   <td>12.3744</td> <td>0.0000</td>   <td>44.4951</td>    <td>61.2434</td> \n</tr>\n<tr>\n  <th>x3</th>     <td>-190.5337</td>  <td>2.6168</td>  <td>-72.8112</td> <td>0.0000</td>  <td>-195.6627</td>  <td>-185.4047</td>\n</tr>\n<tr>\n  <th>x4</th>     <td>214.5259</td>   <td>2.5507</td>   <td>84.1037</td> <td>0.0000</td>  <td>209.5264</td>   <td>219.5253</td> \n</tr>\n<tr>\n  <th>x5</th>     <td>-46.6299</td>   <td>9.4944</td>   <td>-4.9113</td> <td>0.0000</td>  <td>-65.2391</td>   <td>-28.0208</td> \n</tr>\n<tr>\n  <th>x6</th>     <td>-121.5332</td>  <td>4.6368</td>  <td>-26.2106</td> <td>0.0000</td>  <td>-130.6214</td>  <td>-112.4450</td>\n</tr>\n<tr>\n  <th>x7</th>    <td>-1372.6718</td>  <td>89.2151</td> <td>-15.3861</td> <td>0.0000</td> <td>-1547.5351</td> <td>-1197.8085</td>\n</tr>\n<tr>\n  <th>x8</th>     <td>1893.3603</td>  <td>87.5187</td>  <td>21.6338</td> <td>0.0000</td>  <td>1721.8220</td>  <td>2064.8987</td>\n</tr>\n<tr>\n  <th>x9</th>     <td>-188.8980</td>  <td>66.9466</td>  <td>-2.8216</td> <td>0.0048</td>  <td>-320.1145</td>  <td>-57.6814</td> \n</tr>\n</table>\n<table class=\"simpletable\">\n<tr>\n     <td>Omnibus:</td>    <td>7912.396</td>  <td>Durbin-Watson:</td>     <td>1.997</td>  \n</tr>\n<tr>\n  <td>Prob(Omnibus):</td>   <td>0.000</td>  <td>Jarque-Bera (JB):</td> <td>51815.141</td>\n</tr>\n<tr>\n       <td>Skew:</td>       <td>0.724</td>      <td>Prob(JB):</td>       <td>0.000</td>  \n</tr>\n<tr>\n     <td>Kurtosis:</td>     <td>8.169</td>   <td>Condition No.:</td>      <td>152</td>   \n</tr>\n</table>"
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "threshold = 0.05\n",
    "elimination_model = elimination(linear_model, threshold)\n",
    "elim_results = elimination_model.fit()\n",
    "elim_results.summary2()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. [1 point] Find the best (in terms of RMSE) $\\alpha$ for Lasso regression using cross-validation with 4 folds. You must select values from range $[10^{-4}, 10^{3}]$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "folds = 4"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best alpha = 1.9111\n",
      "CPU times: user 48.3 s, sys: 14.7 s, total: 1min 2s\n",
      "Wall time: 18.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "alphas = np.logspace(-4, 3, 1000)\n",
    "search = GridSearchCV(Ridge(), [{\"alpha\": alphas}], scoring=\"neg_mean_squared_error\", cv=folds)\n",
    "search.fit(features_train, target_train)\n",
    "print(f\"Best alpha = {search.best_params_['alpha']:.4f}\")\n",
    "# Best alpha = 2.0001\n",
    "# CPU times: total: 5min 13s\n",
    "# Wall time: 39.6 s\n",
    "# Best alpha = 1.9111\n",
    "# CPU times: total: 5min 23s\n",
    "# Wall time: 51.3 s"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Gradient descent\n",
    "\n",
    "#### 6. [3.5 points] Implement a Ridge regression model for the MSE loss function, trained by gradient descent.\n",
    "\n",
    "All calculations must be vectorized, and python loops can only be used for gradient descent iterations. As a stop criterion, you must use (simultaneously):\n",
    "\n",
    "* checking for the Absolute-value norm of the weight difference on two adjacent iterations (for example, less than some small number of the order of $10^{-6}$, set by the `tolerance` parameter);\n",
    "* reaching the maximum number of iterations (for example, 10000, set by the `max_iter` parameter).\n",
    "\n",
    "You need to implement:\n",
    "\n",
    "* Full gradient descent:\n",
    "\n",
    "$$\n",
    "w_{k + 1} = w_{k} - \\eta_{k} \\nabla_{w} Q(w_{k}).\n",
    "$$\n",
    "\n",
    "* Stochastic Gradient Descent:\n",
    "\n",
    "$$\n",
    "w_{k + 1} = w_{k} - \\eta_{k} \\nabla_{w} q_{i_{k}}(w_{k}).\n",
    "$$\n",
    "\n",
    "$\\nabla_{w} q_{i_{k}}(w_{k}) \\, $ is the estimate of the gradient over the batch of objects selected randomly.\n",
    "\n",
    "* Momentum method:\n",
    "\n",
    "$$\n",
    "h_0 = 0, \\\\\n",
    "h_{k + 1} = \\alpha h_{k} + \\eta_k \\nabla_{w} Q(w_{k}), \\\\\n",
    "w_{k + 1} = w_{k} - h_{k + 1}.\n",
    "$$\n",
    "\n",
    "* Adagrad method:\n",
    "\n",
    "$$\n",
    "G_0 = 0, \\\\\n",
    "G_{k + 1} = G_{k} + (\\nabla_{w} Q(w_{k+1}))^2, \\\\\n",
    "w_{k + 1} = w_{k} - \\eta * \\frac{\\nabla_{w} Q(w_{k+1})}{\\sqrt{G_{k+1} + \\epsilon}}.\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "To make sure that the optimization process really converges, we will use the `loss_history` class attribute. After calling the `fit` method, it should contain the values of the loss function for all iterations, starting from the first one (before the first step on the anti-gradient).\n",
    "\n",
    "You need to initialize the weights with a random vector from normal distribution. The following is a template class that needs to contain the code implementing all variations of the models."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator\n",
    "\n",
    "\n",
    "class LinReg(BaseEstimator):\n",
    "    def __init__(self, delta=1.0, gd_type='Momentum',\n",
    "                 tolerance=1e-4, max_iter=1000, w0=None, eta=1e-2, alpha=1e-3):\n",
    "        \"\"\"\n",
    "        gd_type: str\n",
    "            'GradientDescent', 'StochasticDescent', 'Momentum', 'Adagrad'\n",
    "        delta: float\n",
    "            proportion of object in a batch (for stochastic GD)\n",
    "        tolerance: float\n",
    "            for stopping gradient descent\n",
    "        max_iter: int\n",
    "            maximum number of steps in gradient descent\n",
    "        w0: np.array of shape (d)\n",
    "            init weights\n",
    "        eta: float\n",
    "            learning rate\n",
    "        alpha: float\n",
    "            momentum coefficient\n",
    "        reg_cf: float\n",
    "            regularization coefficient\n",
    "        epsilon: float\n",
    "            numerical stability\n",
    "        \"\"\"\n",
    "        np.random.seed(10)\n",
    "        self.delta = delta\n",
    "        self.gd_type = gd_type\n",
    "        self.tolerance = tolerance\n",
    "        self.max_iter = max_iter\n",
    "        self.w0 = w0\n",
    "        self.alpha = alpha\n",
    "        self.w = None\n",
    "        self.eta = eta\n",
    "        self.loss_history = None  # list of loss function values at each training iteration\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        X: np.array of shape (l, d)\n",
    "        y: np.array of shape (l)\n",
    "        ---\n",
    "        output: self\n",
    "        \"\"\"\n",
    "\n",
    "        supported_types = ['Momentum', 'GradientDescent', 'StochasticDescent']\n",
    "\n",
    "        if self.gd_type in supported_types:\n",
    "            self.loss_history = []\n",
    "            if self.w0 is None:\n",
    "                self.w0 = np.zeros(X.shape[1])\n",
    "            self.w = np.array(self.w0)\n",
    "            cur_w = np.array(self.w)\n",
    "            h = np.zeros(X.shape[1])\n",
    "\n",
    "            for i in range(0, self.max_iter):\n",
    "                if self.gd_type == supported_types[1]:\n",
    "                    self.w -= self.gradient_descent(X, y)\n",
    "                elif self.gd_type == supported_types[2]:\n",
    "                    self.stochastic_descent(X, y)\n",
    "                else:\n",
    "                    self.momentum_descent(X, h, y)\n",
    "                self.loss_history.append(self.calc_loss(X, y))\n",
    "                if np.linalg.norm(self.w - cur_w) < self.tolerance:\n",
    "                    break\n",
    "                cur_w = np.array(self.w)\n",
    "\n",
    "            return self\n",
    "        raise Exception('Unknown type')\n",
    "\n",
    "    def momentum_descent(self, X, h, y):\n",
    "        indexes = np.random.choice(X.shape[0], int(X.shape[0] * self.delta))\n",
    "        h = self.alpha * h + self.gradient_descent(np.take(X, indexes, axis=0), np.take(y, indexes))\n",
    "        self.w -= h\n",
    "\n",
    "    def stochastic_descent(self, X, y):\n",
    "        indexes = np.random.choice(X.shape[0], int(X.shape[0] * self.delta))\n",
    "        self.w -= self.gradient_descent(np.take(X, indexes, axis=0), np.take(y, indexes))\n",
    "\n",
    "    def gradient_descent(self, X, y):\n",
    "        return self.eta * self.calc_gradient(X, y)\n",
    "\n",
    "    def adagard_descent(self, X, y):\n",
    "        pass\n",
    "\n",
    "    def predict(self, X):\n",
    "        if self.w is None:\n",
    "            raise Exception('Not trained yet')\n",
    "\n",
    "        return np.dot(X, self.w)\n",
    "\n",
    "    def calc_gradient(self, X, y):\n",
    "        \"\"\"\n",
    "        X: np.array of shape (l, d) (l can be equal to 1 if stochastic)\n",
    "        y: np.array of shape (l)\n",
    "        ---\n",
    "        output: np.array of shape (d)\n",
    "        \"\"\"\n",
    "        return 2 * np.dot(X.T, np.dot(X, self.w) - y) / y.shape[0]\n",
    "\n",
    "    def calc_loss(self, X, y):\n",
    "        \"\"\"\n",
    "        X: np.array of shape (l, d)\n",
    "        y: np.array of shape (l)\n",
    "        ---\n",
    "        output: float\n",
    "        \"\"\"\n",
    "        return np.mean((self.predict(X) - y) ** 2)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 7. [1 points] Train and validate \"hand-written\" models on the same data, and compare the quality with the Sklearn or StatsModels methods. Investigate the effect of the `max_iter` and `alpha` parameters on the optimization process. Is it consistent with your expectations?"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "features_train_last = features_train\n",
    "features_test_last = features_test\n",
    "target_train_last = target_train\n",
    "target_test_last = target_test"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [],
   "source": [
    "w0 = np.random.uniform(-1, 1, features_train_last.shape[1])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "gradient = LinReg(gd_type='GradientDescent', w0=w0, eta=0.5, tolerance=1e-6, max_iter=100).fit(features_train_last, target_train_last)\n",
    "stoch = LinReg(gd_type='StochasticDescent', w0=w0, eta=0.5, tolerance=1e-6, max_iter=100).fit(features_train_last, target_train_last)\n",
    "momentum = LinReg(gd_type='Momentum', eta=0.5, w0=w0, tolerance=1e-6, max_iter=100, alpha=0.1).fit(features_train_last, target_train_last)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear regression:\n",
      "RMSE : 882.4742\n",
      "R2   : 0.9137\n",
      "RMSE test: 880.9212\n",
      "R2 test: 0.9148\n"
     ]
    }
   ],
   "source": [
    "def display_info_linear():\n",
    "    predicted_test_last, predicted_train = calculations()\n",
    "    print('Linear regression:')\n",
    "    print(f'RMSE : {mean_squared_error(target_train, predicted_train, squared=False):.4f}')\n",
    "    print(f'R2   : {r2_score(target_train, predicted_train):.4f}')\n",
    "    print(f'RMSE test: {mean_squared_error(target_test, predicted_test_last, squared=False):.4f}')\n",
    "    print(f'R2 test: {r2_score(target_test, predicted_test_last):.4f}')\n",
    "\n",
    "\n",
    "def calculations():\n",
    "    predicted_train = linear_results.predict(features_train)\n",
    "    predicted_test_last = linear_results.predict(features_test)\n",
    "    return predicted_test_last, predicted_train\n",
    "\n",
    "\n",
    "display_info_linear()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8. [1 points] Plot graphs (on the same picture) of the dependence of the loss function value on the iteration number for Full GD, SGD, Momentum and Adagrad. Draw conclusions about the rate of convergence of various modifications of gradient descent.\n",
    "\n",
    "Don't forget about what *beautiful* graphics should look like!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
