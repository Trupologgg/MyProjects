{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# HSE 2022: Mathematical Methods for Data Analysis\n",
    "\n",
    "## Homework 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 518,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-26T16:48:20.566549Z",
     "start_time": "2020-09-26T16:48:19.893995Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sklearn\n",
    "from sklearn import datasets\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.regression.linear_model import OLSResults\n",
    "from math import sqrt\n",
    "import random\n",
    "import sys\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "sns.set(style=\"darkgrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Data\n",
    "\n",
    "For this homework we use Dataset from seaborn on diamonds prices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 519,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "data = sns.load_dataset('diamonds')\n",
    "\n",
    "y = data.price\n",
    "x = data.drop(['price'], axis=1)\n",
    "columns = data.drop(['price'], axis=1).columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Linear regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### 0. [0.25 points] Encode categorical variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 520,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "data_ohe = pd.get_dummies(data, drop_first=True)\n",
    "x = data_ohe.drop('price', axis=1)\n",
    "y = data_ohe['price']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### 1. [0.25 points] Split the data into train and test sets with ratio 80:20 with random_state=17."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 521,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x, y, random_state=17, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### 2. [1 point] Train models on train data using StatsModels library and apply it to the test set; use $RMSE$ and $R^2$ as the quality measure.\n",
    "\n",
    "* [`LinearRegression`](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html);\n",
    "* [`Ridge`](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html) with $\\alpha = 0.01$;\n",
    "* [`Lasso`](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lasso.html) with $\\alpha = 0.01$\n",
    "* [`ElasticNet`](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.ElasticNet.html) with $\\alpha = 0.01$, $l_{1}$_$ratio = 0.6$\n",
    "\n",
    "Don't forget to scale the data before training the models with StandardScaler!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 522,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "columns_to_scalar = ['carat', 'depth', 'table', 'x', 'y', 'z']\n",
    "scaler.fit(x_train[columns_to_scalar])\n",
    "x_train[columns_to_scalar] = scaler.transform(x_train[columns_to_scalar])\n",
    "x_test[columns_to_scalar] = scaler.transform(x_test[columns_to_scalar])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 523,
   "outputs": [],
   "source": [
    "features_train_constant = sm.add_constant(x_train)\n",
    "features_test_constant = sm.add_constant(x_test)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 524,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE = 1322478.130934603\n",
      "RMSE = 1149.990491671389\n",
      "R2 =  0.9183333802164863\n"
     ]
    }
   ],
   "source": [
    "model_ols = sm.OLS(y_train, features_train_constant)\n",
    "linear_for_summary = model_ols.fit()\n",
    "predicted_linear = linear_for_summary.predict(features_test_constant)\n",
    "mse = mean_squared_error(y_test, predicted_linear)\n",
    "sqrt_mse = mse ** .5\n",
    "r2 = r2_score(y_test, predicted_linear)\n",
    "print(\"MSE =\", mse)\n",
    "print(\"RMSE =\", sqrt_mse)\n",
    "print(\"R2 = \", r2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 525,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE = 1499815.5963935482\n",
      "RMSE = 1224.6695866206314\n",
      "R2 =  0.9073823096269312\n"
     ]
    }
   ],
   "source": [
    "ridge_model = model_ols.fit_regularized(L1_wt=0, alpha=0.01)\n",
    "ridge_for_summary = sm.regression.linear_model.OLSResults(model_ols, ridge_model.params,\n",
    "                                                          model_ols.normalized_cov_params)\n",
    "predicted_ridge = ridge_model.predict(features_test_constant)\n",
    "mse = mean_squared_error(y_true=y_test, y_pred=predicted_ridge)\n",
    "sqrt_mse = mse ** .5\n",
    "r2 = r2_score(y_test, predicted_ridge)\n",
    "print(\"MSE =\", mse)\n",
    "print(\"RMSE =\", sqrt_mse)\n",
    "print('R2 = ', r2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 526,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE = 1499815.5963935482\n",
      "RMSE = 1224.6695866206314\n",
      "R2 =  0.9073823096269312\n"
     ]
    }
   ],
   "source": [
    "lasso_model = model_ols.fit_regularized(L1_wt=1, alpha=0.01)\n",
    "lasso_for_summary = sm.regression.linear_model.OLSResults(model_ols, lasso_model.params,\n",
    "                                                          model_ols.normalized_cov_params)\n",
    "predicted_lasso = ridge_model.predict(features_test_constant)\n",
    "mse = mean_squared_error(y_test, predicted_lasso)\n",
    "sqrt_mse = mse ** .5\n",
    "r2 = r2_score(y_test, predicted_lasso)\n",
    "print(\"MSE =\", mse)\n",
    "print(\"RMSE =\", sqrt_mse)\n",
    "print('R2 = ', r2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 527,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE = 1385150.6626689616\n",
      "RMSE = 1176.9242382876484\n",
      "R2 =  0.9144631810046453\n"
     ]
    }
   ],
   "source": [
    "elastic_model = model_ols.fit_regularized(L1_wt=0.6, alpha=0.01)\n",
    "elastic_for_summary = sm.regression.linear_model.OLSResults(model_ols, elastic_model.params,\n",
    "                                                            model_ols.normalized_cov_params)\n",
    "predicted_elastic = elastic_model.predict(features_test_constant)\n",
    "mse = mean_squared_error(y_test, predicted_elastic)\n",
    "sqrt_mse = mse ** .5\n",
    "r2 = r2_score(y_test, predicted_elastic)\n",
    "print(\"MSE =\", mse)\n",
    "print(\"RMSE =\", sqrt_mse)\n",
    "print('R2 = ', r2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### 3. [1 point] Explore the values of the parameters of the resulting models and compare the number of zero weights in them. Comment on the significance of the coefficients, overal model significance and other related factors from the results table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 528,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "<class 'statsmodels.iolib.summary.Summary'>\n\"\"\"\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                  price   R-squared:                       0.920\nModel:                            OLS   Adj. R-squared:                  0.920\nMethod:                 Least Squares   F-statistic:                 2.160e+04\nDate:                Sun, 16 Oct 2022   Prob (F-statistic):               0.00\nTime:                        23:49:23   Log-Likelihood:            -3.6439e+05\nNo. Observations:               43152   AIC:                         7.288e+05\nDf Residuals:                   43128   BIC:                         7.290e+05\nDf Model:                          23                                         \nCovariance Type:            nonrobust                                         \n=================================================================================\n                    coef    std err          t      P>|t|      [0.025      0.975]\n---------------------------------------------------------------------------------\nconst          5998.1755     34.906    171.839      0.000    5929.759    6066.592\ncarat          5386.7101     26.223    205.422      0.000    5335.313    5438.107\ndepth           -97.9285      7.156    -13.685      0.000    -111.954     -83.903\ntable           -61.6945      7.248     -8.512      0.000     -75.900     -47.489\nx             -1186.2280     40.719    -29.132      0.000   -1266.038   -1106.418\ny                 2.8156     24.711      0.114      0.909     -45.618      51.249\nz               -31.7705     24.767     -1.283      0.200     -80.314      16.773\ncut_Premium     -75.6406     16.251     -4.654      0.000    -107.493     -43.788\ncut_Very Good   -97.8485     15.842     -6.177      0.000    -128.899     -66.798\ncut_Good       -250.7740     22.467    -11.162      0.000    -294.810    -206.738\ncut_Fair       -824.7797     36.981    -22.303      0.000    -897.263    -752.296\ncolor_E        -221.6556     19.881    -11.149      0.000    -260.622    -182.689\ncolor_F        -287.3614     20.122    -14.281      0.000    -326.800    -247.922\ncolor_G        -486.4749     19.710    -24.682      0.000    -525.106    -447.843\ncolor_H       -1001.0497     20.942    -47.800      0.000   -1042.097    -960.002\ncolor_I       -1485.1637     23.508    -63.177      0.000   -1531.240   -1439.087\ncolor_J       -2368.2801     29.008    -81.642      0.000   -2425.137   -2311.424\nclarity_VVS1   -343.8856     36.255     -9.485      0.000    -414.947    -272.824\nclarity_VVS2   -413.4255     34.682    -11.920      0.000    -481.404    -345.447\nclarity_VS1    -782.7279     33.063    -23.674      0.000    -847.533    -717.923\nclarity_VS2   -1084.1024     32.286    -33.578      0.000   -1147.383   -1020.822\nclarity_SI1   -1689.7140     32.549    -51.913      0.000   -1753.510   -1625.918\nclarity_SI2   -2659.0586     33.852    -78.550      0.000   -2725.408   -2592.709\nclarity_I1    -5317.8522     56.897    -93.464      0.000   -5429.372   -5206.332\n==============================================================================\nOmnibus:                    11910.550   Durbin-Watson:                   2.000\nProb(Omnibus):                  0.000   Jarque-Bera (JB):           377007.939\nSkew:                           0.693   Prob(JB):                         0.00\nKurtosis:                      17.414   Cond. No.                         32.8\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\"\"\"",
      "text/html": "<table class=\"simpletable\">\n<caption>OLS Regression Results</caption>\n<tr>\n  <th>Dep. Variable:</th>          <td>price</td>      <th>  R-squared:         </th>  <td>   0.920</td>  \n</tr>\n<tr>\n  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th>  <td>   0.920</td>  \n</tr>\n<tr>\n  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th>  <td>2.160e+04</td> \n</tr>\n<tr>\n  <th>Date:</th>             <td>Sun, 16 Oct 2022</td> <th>  Prob (F-statistic):</th>   <td>  0.00</td>   \n</tr>\n<tr>\n  <th>Time:</th>                 <td>23:49:23</td>     <th>  Log-Likelihood:    </th> <td>-3.6439e+05</td>\n</tr>\n<tr>\n  <th>No. Observations:</th>      <td> 43152</td>      <th>  AIC:               </th>  <td>7.288e+05</td> \n</tr>\n<tr>\n  <th>Df Residuals:</th>          <td> 43128</td>      <th>  BIC:               </th>  <td>7.290e+05</td> \n</tr>\n<tr>\n  <th>Df Model:</th>              <td>    23</td>      <th>                     </th>      <td> </td>     \n</tr>\n<tr>\n  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>      <td> </td>     \n</tr>\n</table>\n<table class=\"simpletable\">\n<tr>\n        <td></td>           <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n</tr>\n<tr>\n  <th>const</th>         <td> 5998.1755</td> <td>   34.906</td> <td>  171.839</td> <td> 0.000</td> <td> 5929.759</td> <td> 6066.592</td>\n</tr>\n<tr>\n  <th>carat</th>         <td> 5386.7101</td> <td>   26.223</td> <td>  205.422</td> <td> 0.000</td> <td> 5335.313</td> <td> 5438.107</td>\n</tr>\n<tr>\n  <th>depth</th>         <td>  -97.9285</td> <td>    7.156</td> <td>  -13.685</td> <td> 0.000</td> <td> -111.954</td> <td>  -83.903</td>\n</tr>\n<tr>\n  <th>table</th>         <td>  -61.6945</td> <td>    7.248</td> <td>   -8.512</td> <td> 0.000</td> <td>  -75.900</td> <td>  -47.489</td>\n</tr>\n<tr>\n  <th>x</th>             <td>-1186.2280</td> <td>   40.719</td> <td>  -29.132</td> <td> 0.000</td> <td>-1266.038</td> <td>-1106.418</td>\n</tr>\n<tr>\n  <th>y</th>             <td>    2.8156</td> <td>   24.711</td> <td>    0.114</td> <td> 0.909</td> <td>  -45.618</td> <td>   51.249</td>\n</tr>\n<tr>\n  <th>z</th>             <td>  -31.7705</td> <td>   24.767</td> <td>   -1.283</td> <td> 0.200</td> <td>  -80.314</td> <td>   16.773</td>\n</tr>\n<tr>\n  <th>cut_Premium</th>   <td>  -75.6406</td> <td>   16.251</td> <td>   -4.654</td> <td> 0.000</td> <td> -107.493</td> <td>  -43.788</td>\n</tr>\n<tr>\n  <th>cut_Very Good</th> <td>  -97.8485</td> <td>   15.842</td> <td>   -6.177</td> <td> 0.000</td> <td> -128.899</td> <td>  -66.798</td>\n</tr>\n<tr>\n  <th>cut_Good</th>      <td> -250.7740</td> <td>   22.467</td> <td>  -11.162</td> <td> 0.000</td> <td> -294.810</td> <td> -206.738</td>\n</tr>\n<tr>\n  <th>cut_Fair</th>      <td> -824.7797</td> <td>   36.981</td> <td>  -22.303</td> <td> 0.000</td> <td> -897.263</td> <td> -752.296</td>\n</tr>\n<tr>\n  <th>color_E</th>       <td> -221.6556</td> <td>   19.881</td> <td>  -11.149</td> <td> 0.000</td> <td> -260.622</td> <td> -182.689</td>\n</tr>\n<tr>\n  <th>color_F</th>       <td> -287.3614</td> <td>   20.122</td> <td>  -14.281</td> <td> 0.000</td> <td> -326.800</td> <td> -247.922</td>\n</tr>\n<tr>\n  <th>color_G</th>       <td> -486.4749</td> <td>   19.710</td> <td>  -24.682</td> <td> 0.000</td> <td> -525.106</td> <td> -447.843</td>\n</tr>\n<tr>\n  <th>color_H</th>       <td>-1001.0497</td> <td>   20.942</td> <td>  -47.800</td> <td> 0.000</td> <td>-1042.097</td> <td> -960.002</td>\n</tr>\n<tr>\n  <th>color_I</th>       <td>-1485.1637</td> <td>   23.508</td> <td>  -63.177</td> <td> 0.000</td> <td>-1531.240</td> <td>-1439.087</td>\n</tr>\n<tr>\n  <th>color_J</th>       <td>-2368.2801</td> <td>   29.008</td> <td>  -81.642</td> <td> 0.000</td> <td>-2425.137</td> <td>-2311.424</td>\n</tr>\n<tr>\n  <th>clarity_VVS1</th>  <td> -343.8856</td> <td>   36.255</td> <td>   -9.485</td> <td> 0.000</td> <td> -414.947</td> <td> -272.824</td>\n</tr>\n<tr>\n  <th>clarity_VVS2</th>  <td> -413.4255</td> <td>   34.682</td> <td>  -11.920</td> <td> 0.000</td> <td> -481.404</td> <td> -345.447</td>\n</tr>\n<tr>\n  <th>clarity_VS1</th>   <td> -782.7279</td> <td>   33.063</td> <td>  -23.674</td> <td> 0.000</td> <td> -847.533</td> <td> -717.923</td>\n</tr>\n<tr>\n  <th>clarity_VS2</th>   <td>-1084.1024</td> <td>   32.286</td> <td>  -33.578</td> <td> 0.000</td> <td>-1147.383</td> <td>-1020.822</td>\n</tr>\n<tr>\n  <th>clarity_SI1</th>   <td>-1689.7140</td> <td>   32.549</td> <td>  -51.913</td> <td> 0.000</td> <td>-1753.510</td> <td>-1625.918</td>\n</tr>\n<tr>\n  <th>clarity_SI2</th>   <td>-2659.0586</td> <td>   33.852</td> <td>  -78.550</td> <td> 0.000</td> <td>-2725.408</td> <td>-2592.709</td>\n</tr>\n<tr>\n  <th>clarity_I1</th>    <td>-5317.8522</td> <td>   56.897</td> <td>  -93.464</td> <td> 0.000</td> <td>-5429.372</td> <td>-5206.332</td>\n</tr>\n</table>\n<table class=\"simpletable\">\n<tr>\n  <th>Omnibus:</th>       <td>11910.550</td> <th>  Durbin-Watson:     </th>  <td>   2.000</td> \n</tr>\n<tr>\n  <th>Prob(Omnibus):</th>  <td> 0.000</td>   <th>  Jarque-Bera (JB):  </th> <td>377007.939</td>\n</tr>\n<tr>\n  <th>Skew:</th>           <td> 0.693</td>   <th>  Prob(JB):          </th>  <td>    0.00</td> \n</tr>\n<tr>\n  <th>Kurtosis:</th>       <td>17.414</td>   <th>  Cond. No.          </th>  <td>    32.8</td> \n</tr>\n</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['y' 'z']\n"
     ]
    }
   ],
   "source": [
    "display(linear_for_summary.summary())\n",
    "print(linear_for_summary.pvalues[linear_for_summary.pvalues > 0.05].index.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "На уровне значимости 0.05 есть два признака, которые имеют нулевые веса. Это признаки ['y'], ['z']."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 529,
   "outputs": [
    {
     "data": {
      "text/plain": "<class 'statsmodels.iolib.summary.Summary'>\n\"\"\"\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                  price   R-squared:                       0.907\nModel:                            OLS   Adj. R-squared:                  0.907\nMethod:                 Least Squares   F-statistic:                 1.838e+04\nDate:                Sun, 16 Oct 2022   Prob (F-statistic):               0.00\nTime:                        23:49:23   Log-Likelihood:            -3.6758e+05\nNo. Observations:               43152   AIC:                         7.352e+05\nDf Residuals:                   43128   BIC:                         7.354e+05\nDf Model:                          23                                         \nCovariance Type:            nonrobust                                         \n=================================================================================\n                    coef    std err          t      P>|t|      [0.025      0.975]\n---------------------------------------------------------------------------------\nconst          3943.1458     37.585    104.914      0.000    3869.479    4016.812\ncarat          4297.5148     28.235    152.205      0.000    4242.173    4352.856\ndepth          -104.5765      7.705    -13.573      0.000    -119.678     -89.475\ntable          -130.3229      7.804    -16.700      0.000    -145.619    -115.027\nx              -258.1331     43.844     -5.888      0.000    -344.068    -172.198\ny                15.4047     26.607      0.579      0.563     -36.746      67.556\nz               -42.8857     26.668     -1.608      0.108     -95.155       9.384\ncut_Premium      43.2635     17.498      2.472      0.013       8.967      77.561\ncut_Very Good    47.3255     17.058      2.774      0.006      13.892      80.759\ncut_Good       -108.9764     24.192     -4.505      0.000    -156.392     -61.560\ncut_Fair       -610.2820     39.819    -15.326      0.000    -688.328    -532.236\ncolor_E         248.2811     21.407     11.598      0.000     206.324     290.238\ncolor_F         226.9876     21.666     10.477      0.000     184.522     269.453\ncolor_G         110.4881     21.222      5.206      0.000      68.892     152.084\ncolor_H        -376.4216     22.550    -16.693      0.000    -420.619    -332.224\ncolor_I        -745.9151     25.312    -29.469      0.000    -795.528    -696.303\ncolor_J       -1394.8162     31.234    -44.656      0.000   -1456.036   -1333.596\nclarity_VVS1    917.0207     39.038     23.491      0.000     840.506     993.536\nclarity_VVS2    911.6279     37.344     24.412      0.000     838.433     984.823\nclarity_VS1     565.2382     35.601     15.877      0.000     495.460     635.017\nclarity_VS2     344.6802     34.764      9.915      0.000     276.543     412.818\nclarity_SI1    -226.8150     35.047     -6.472      0.000    -295.507    -158.123\nclarity_SI2   -1046.5241     36.450    -28.712      0.000   -1117.966    -975.082\nclarity_I1    -2116.4966     61.264    -34.547      0.000   -2236.575   -1996.418\n==============================================================================\nOmnibus:                    15835.711   Durbin-Watson:                   2.002\nProb(Omnibus):                  0.000   Jarque-Bera (JB):           297019.622\nSkew:                           1.290   Prob(JB):                         0.00\nKurtosis:                      15.591   Cond. No.                         32.8\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\"\"\"",
      "text/html": "<table class=\"simpletable\">\n<caption>OLS Regression Results</caption>\n<tr>\n  <th>Dep. Variable:</th>          <td>price</td>      <th>  R-squared:         </th>  <td>   0.907</td>  \n</tr>\n<tr>\n  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th>  <td>   0.907</td>  \n</tr>\n<tr>\n  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th>  <td>1.838e+04</td> \n</tr>\n<tr>\n  <th>Date:</th>             <td>Sun, 16 Oct 2022</td> <th>  Prob (F-statistic):</th>   <td>  0.00</td>   \n</tr>\n<tr>\n  <th>Time:</th>                 <td>23:49:23</td>     <th>  Log-Likelihood:    </th> <td>-3.6758e+05</td>\n</tr>\n<tr>\n  <th>No. Observations:</th>      <td> 43152</td>      <th>  AIC:               </th>  <td>7.352e+05</td> \n</tr>\n<tr>\n  <th>Df Residuals:</th>          <td> 43128</td>      <th>  BIC:               </th>  <td>7.354e+05</td> \n</tr>\n<tr>\n  <th>Df Model:</th>              <td>    23</td>      <th>                     </th>      <td> </td>     \n</tr>\n<tr>\n  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>      <td> </td>     \n</tr>\n</table>\n<table class=\"simpletable\">\n<tr>\n        <td></td>           <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n</tr>\n<tr>\n  <th>const</th>         <td> 3943.1458</td> <td>   37.585</td> <td>  104.914</td> <td> 0.000</td> <td> 3869.479</td> <td> 4016.812</td>\n</tr>\n<tr>\n  <th>carat</th>         <td> 4297.5148</td> <td>   28.235</td> <td>  152.205</td> <td> 0.000</td> <td> 4242.173</td> <td> 4352.856</td>\n</tr>\n<tr>\n  <th>depth</th>         <td> -104.5765</td> <td>    7.705</td> <td>  -13.573</td> <td> 0.000</td> <td> -119.678</td> <td>  -89.475</td>\n</tr>\n<tr>\n  <th>table</th>         <td> -130.3229</td> <td>    7.804</td> <td>  -16.700</td> <td> 0.000</td> <td> -145.619</td> <td> -115.027</td>\n</tr>\n<tr>\n  <th>x</th>             <td> -258.1331</td> <td>   43.844</td> <td>   -5.888</td> <td> 0.000</td> <td> -344.068</td> <td> -172.198</td>\n</tr>\n<tr>\n  <th>y</th>             <td>   15.4047</td> <td>   26.607</td> <td>    0.579</td> <td> 0.563</td> <td>  -36.746</td> <td>   67.556</td>\n</tr>\n<tr>\n  <th>z</th>             <td>  -42.8857</td> <td>   26.668</td> <td>   -1.608</td> <td> 0.108</td> <td>  -95.155</td> <td>    9.384</td>\n</tr>\n<tr>\n  <th>cut_Premium</th>   <td>   43.2635</td> <td>   17.498</td> <td>    2.472</td> <td> 0.013</td> <td>    8.967</td> <td>   77.561</td>\n</tr>\n<tr>\n  <th>cut_Very Good</th> <td>   47.3255</td> <td>   17.058</td> <td>    2.774</td> <td> 0.006</td> <td>   13.892</td> <td>   80.759</td>\n</tr>\n<tr>\n  <th>cut_Good</th>      <td> -108.9764</td> <td>   24.192</td> <td>   -4.505</td> <td> 0.000</td> <td> -156.392</td> <td>  -61.560</td>\n</tr>\n<tr>\n  <th>cut_Fair</th>      <td> -610.2820</td> <td>   39.819</td> <td>  -15.326</td> <td> 0.000</td> <td> -688.328</td> <td> -532.236</td>\n</tr>\n<tr>\n  <th>color_E</th>       <td>  248.2811</td> <td>   21.407</td> <td>   11.598</td> <td> 0.000</td> <td>  206.324</td> <td>  290.238</td>\n</tr>\n<tr>\n  <th>color_F</th>       <td>  226.9876</td> <td>   21.666</td> <td>   10.477</td> <td> 0.000</td> <td>  184.522</td> <td>  269.453</td>\n</tr>\n<tr>\n  <th>color_G</th>       <td>  110.4881</td> <td>   21.222</td> <td>    5.206</td> <td> 0.000</td> <td>   68.892</td> <td>  152.084</td>\n</tr>\n<tr>\n  <th>color_H</th>       <td> -376.4216</td> <td>   22.550</td> <td>  -16.693</td> <td> 0.000</td> <td> -420.619</td> <td> -332.224</td>\n</tr>\n<tr>\n  <th>color_I</th>       <td> -745.9151</td> <td>   25.312</td> <td>  -29.469</td> <td> 0.000</td> <td> -795.528</td> <td> -696.303</td>\n</tr>\n<tr>\n  <th>color_J</th>       <td>-1394.8162</td> <td>   31.234</td> <td>  -44.656</td> <td> 0.000</td> <td>-1456.036</td> <td>-1333.596</td>\n</tr>\n<tr>\n  <th>clarity_VVS1</th>  <td>  917.0207</td> <td>   39.038</td> <td>   23.491</td> <td> 0.000</td> <td>  840.506</td> <td>  993.536</td>\n</tr>\n<tr>\n  <th>clarity_VVS2</th>  <td>  911.6279</td> <td>   37.344</td> <td>   24.412</td> <td> 0.000</td> <td>  838.433</td> <td>  984.823</td>\n</tr>\n<tr>\n  <th>clarity_VS1</th>   <td>  565.2382</td> <td>   35.601</td> <td>   15.877</td> <td> 0.000</td> <td>  495.460</td> <td>  635.017</td>\n</tr>\n<tr>\n  <th>clarity_VS2</th>   <td>  344.6802</td> <td>   34.764</td> <td>    9.915</td> <td> 0.000</td> <td>  276.543</td> <td>  412.818</td>\n</tr>\n<tr>\n  <th>clarity_SI1</th>   <td> -226.8150</td> <td>   35.047</td> <td>   -6.472</td> <td> 0.000</td> <td> -295.507</td> <td> -158.123</td>\n</tr>\n<tr>\n  <th>clarity_SI2</th>   <td>-1046.5241</td> <td>   36.450</td> <td>  -28.712</td> <td> 0.000</td> <td>-1117.966</td> <td> -975.082</td>\n</tr>\n<tr>\n  <th>clarity_I1</th>    <td>-2116.4966</td> <td>   61.264</td> <td>  -34.547</td> <td> 0.000</td> <td>-2236.575</td> <td>-1996.418</td>\n</tr>\n</table>\n<table class=\"simpletable\">\n<tr>\n  <th>Omnibus:</th>       <td>15835.711</td> <th>  Durbin-Watson:     </th>  <td>   2.002</td> \n</tr>\n<tr>\n  <th>Prob(Omnibus):</th>  <td> 0.000</td>   <th>  Jarque-Bera (JB):  </th> <td>297019.622</td>\n</tr>\n<tr>\n  <th>Skew:</th>           <td> 1.290</td>   <th>  Prob(JB):          </th>  <td>    0.00</td> \n</tr>\n<tr>\n  <th>Kurtosis:</th>       <td>15.591</td>   <th>  Cond. No.          </th>  <td>    32.8</td> \n</tr>\n</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['y' 'z']\n"
     ]
    }
   ],
   "source": [
    "display(ridge_for_summary.summary())\n",
    "print(features_train_constant.columns[ridge_for_summary.pvalues > 0.05].values)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "На уровне значимости 0.05 есть два признака, которые имеют нулевые веса. Это признаки ['y'], ['z'].\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 530,
   "outputs": [
    {
     "data": {
      "text/plain": "<class 'statsmodels.iolib.summary.Summary'>\n\"\"\"\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                  price   R-squared:                       0.920\nModel:                            OLS   Adj. R-squared:                  0.920\nMethod:                 Least Squares   F-statistic:                 2.147e+04\nDate:                Sun, 16 Oct 2022   Prob (F-statistic):               0.00\nTime:                        23:49:23   Log-Likelihood:            -3.6452e+05\nNo. Observations:               43152   AIC:                         7.291e+05\nDf Residuals:                   43128   BIC:                         7.293e+05\nDf Model:                          23                                         \nCovariance Type:            nonrobust                                         \n=================================================================================\n                    coef    std err          t      P>|t|      [0.025      0.975]\n---------------------------------------------------------------------------------\nconst          5516.7760     35.010    157.578      0.000    5448.156    5585.396\ncarat          5290.9319     26.301    201.170      0.000    5239.382    5342.482\ndepth           -98.7304      7.177    -13.756      0.000    -112.798     -84.663\ntable           -64.5031      7.269     -8.873      0.000     -78.751     -50.255\nx             -1128.8524     40.840    -27.641      0.000   -1208.900   -1048.805\ny                15.2731     24.784      0.616      0.538     -33.305      63.851\nz               -22.4373     24.841     -0.903      0.366     -71.126      26.251\ncut_Premium     -89.3686     16.300     -5.483      0.000    -121.316     -57.421\ncut_Very Good  -108.6368     15.889     -6.837      0.000    -139.780     -77.494\ncut_Good       -265.2476     22.534    -11.771      0.000    -309.415    -221.080\ncut_Fair       -837.3102     37.091    -22.574      0.000    -910.010    -764.611\ncolor_E        -212.1345     19.940    -10.639      0.000    -251.217    -173.052\ncolor_F        -263.9123     20.182    -13.077      0.000    -303.469    -224.356\ncolor_G        -451.1805     19.769    -22.823      0.000    -489.927    -412.434\ncolor_H        -970.9316     21.005    -46.224      0.000   -1012.101    -929.762\ncolor_I       -1452.5364     23.578    -61.605      0.000   -1498.750   -1406.323\ncolor_J       -2332.6923     29.095    -80.176      0.000   -2389.718   -2275.666\nclarity_VVS1    111.7518     36.363      3.073      0.002      40.479     183.025\nclarity_VVS2     45.2394     34.786      1.301      0.193     -22.941     113.420\nclarity_VS1    -322.8553     33.162     -9.736      0.000    -387.853    -257.857\nclarity_VS2    -619.0582     32.382    -19.117      0.000    -682.528    -555.589\nclarity_SI1   -1221.5874     32.646    -37.420      0.000   -1285.573   -1157.601\nclarity_SI2   -2180.0448     33.953    -64.209      0.000   -2246.592   -2113.497\nclarity_I1    -4823.8177     57.067    -84.529      0.000   -4935.670   -4711.965\n==============================================================================\nOmnibus:                    12978.589   Durbin-Watson:                   1.999\nProb(Omnibus):                  0.000   Jarque-Bera (JB):           376787.537\nSkew:                           0.843   Prob(JB):                         0.00\nKurtosis:                      17.378   Cond. No.                         32.8\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\"\"\"",
      "text/html": "<table class=\"simpletable\">\n<caption>OLS Regression Results</caption>\n<tr>\n  <th>Dep. Variable:</th>          <td>price</td>      <th>  R-squared:         </th>  <td>   0.920</td>  \n</tr>\n<tr>\n  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th>  <td>   0.920</td>  \n</tr>\n<tr>\n  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th>  <td>2.147e+04</td> \n</tr>\n<tr>\n  <th>Date:</th>             <td>Sun, 16 Oct 2022</td> <th>  Prob (F-statistic):</th>   <td>  0.00</td>   \n</tr>\n<tr>\n  <th>Time:</th>                 <td>23:49:23</td>     <th>  Log-Likelihood:    </th> <td>-3.6452e+05</td>\n</tr>\n<tr>\n  <th>No. Observations:</th>      <td> 43152</td>      <th>  AIC:               </th>  <td>7.291e+05</td> \n</tr>\n<tr>\n  <th>Df Residuals:</th>          <td> 43128</td>      <th>  BIC:               </th>  <td>7.293e+05</td> \n</tr>\n<tr>\n  <th>Df Model:</th>              <td>    23</td>      <th>                     </th>      <td> </td>     \n</tr>\n<tr>\n  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>      <td> </td>     \n</tr>\n</table>\n<table class=\"simpletable\">\n<tr>\n        <td></td>           <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n</tr>\n<tr>\n  <th>const</th>         <td> 5516.7760</td> <td>   35.010</td> <td>  157.578</td> <td> 0.000</td> <td> 5448.156</td> <td> 5585.396</td>\n</tr>\n<tr>\n  <th>carat</th>         <td> 5290.9319</td> <td>   26.301</td> <td>  201.170</td> <td> 0.000</td> <td> 5239.382</td> <td> 5342.482</td>\n</tr>\n<tr>\n  <th>depth</th>         <td>  -98.7304</td> <td>    7.177</td> <td>  -13.756</td> <td> 0.000</td> <td> -112.798</td> <td>  -84.663</td>\n</tr>\n<tr>\n  <th>table</th>         <td>  -64.5031</td> <td>    7.269</td> <td>   -8.873</td> <td> 0.000</td> <td>  -78.751</td> <td>  -50.255</td>\n</tr>\n<tr>\n  <th>x</th>             <td>-1128.8524</td> <td>   40.840</td> <td>  -27.641</td> <td> 0.000</td> <td>-1208.900</td> <td>-1048.805</td>\n</tr>\n<tr>\n  <th>y</th>             <td>   15.2731</td> <td>   24.784</td> <td>    0.616</td> <td> 0.538</td> <td>  -33.305</td> <td>   63.851</td>\n</tr>\n<tr>\n  <th>z</th>             <td>  -22.4373</td> <td>   24.841</td> <td>   -0.903</td> <td> 0.366</td> <td>  -71.126</td> <td>   26.251</td>\n</tr>\n<tr>\n  <th>cut_Premium</th>   <td>  -89.3686</td> <td>   16.300</td> <td>   -5.483</td> <td> 0.000</td> <td> -121.316</td> <td>  -57.421</td>\n</tr>\n<tr>\n  <th>cut_Very Good</th> <td> -108.6368</td> <td>   15.889</td> <td>   -6.837</td> <td> 0.000</td> <td> -139.780</td> <td>  -77.494</td>\n</tr>\n<tr>\n  <th>cut_Good</th>      <td> -265.2476</td> <td>   22.534</td> <td>  -11.771</td> <td> 0.000</td> <td> -309.415</td> <td> -221.080</td>\n</tr>\n<tr>\n  <th>cut_Fair</th>      <td> -837.3102</td> <td>   37.091</td> <td>  -22.574</td> <td> 0.000</td> <td> -910.010</td> <td> -764.611</td>\n</tr>\n<tr>\n  <th>color_E</th>       <td> -212.1345</td> <td>   19.940</td> <td>  -10.639</td> <td> 0.000</td> <td> -251.217</td> <td> -173.052</td>\n</tr>\n<tr>\n  <th>color_F</th>       <td> -263.9123</td> <td>   20.182</td> <td>  -13.077</td> <td> 0.000</td> <td> -303.469</td> <td> -224.356</td>\n</tr>\n<tr>\n  <th>color_G</th>       <td> -451.1805</td> <td>   19.769</td> <td>  -22.823</td> <td> 0.000</td> <td> -489.927</td> <td> -412.434</td>\n</tr>\n<tr>\n  <th>color_H</th>       <td> -970.9316</td> <td>   21.005</td> <td>  -46.224</td> <td> 0.000</td> <td>-1012.101</td> <td> -929.762</td>\n</tr>\n<tr>\n  <th>color_I</th>       <td>-1452.5364</td> <td>   23.578</td> <td>  -61.605</td> <td> 0.000</td> <td>-1498.750</td> <td>-1406.323</td>\n</tr>\n<tr>\n  <th>color_J</th>       <td>-2332.6923</td> <td>   29.095</td> <td>  -80.176</td> <td> 0.000</td> <td>-2389.718</td> <td>-2275.666</td>\n</tr>\n<tr>\n  <th>clarity_VVS1</th>  <td>  111.7518</td> <td>   36.363</td> <td>    3.073</td> <td> 0.002</td> <td>   40.479</td> <td>  183.025</td>\n</tr>\n<tr>\n  <th>clarity_VVS2</th>  <td>   45.2394</td> <td>   34.786</td> <td>    1.301</td> <td> 0.193</td> <td>  -22.941</td> <td>  113.420</td>\n</tr>\n<tr>\n  <th>clarity_VS1</th>   <td> -322.8553</td> <td>   33.162</td> <td>   -9.736</td> <td> 0.000</td> <td> -387.853</td> <td> -257.857</td>\n</tr>\n<tr>\n  <th>clarity_VS2</th>   <td> -619.0582</td> <td>   32.382</td> <td>  -19.117</td> <td> 0.000</td> <td> -682.528</td> <td> -555.589</td>\n</tr>\n<tr>\n  <th>clarity_SI1</th>   <td>-1221.5874</td> <td>   32.646</td> <td>  -37.420</td> <td> 0.000</td> <td>-1285.573</td> <td>-1157.601</td>\n</tr>\n<tr>\n  <th>clarity_SI2</th>   <td>-2180.0448</td> <td>   33.953</td> <td>  -64.209</td> <td> 0.000</td> <td>-2246.592</td> <td>-2113.497</td>\n</tr>\n<tr>\n  <th>clarity_I1</th>    <td>-4823.8177</td> <td>   57.067</td> <td>  -84.529</td> <td> 0.000</td> <td>-4935.670</td> <td>-4711.965</td>\n</tr>\n</table>\n<table class=\"simpletable\">\n<tr>\n  <th>Omnibus:</th>       <td>12978.589</td> <th>  Durbin-Watson:     </th>  <td>   1.999</td> \n</tr>\n<tr>\n  <th>Prob(Omnibus):</th>  <td> 0.000</td>   <th>  Jarque-Bera (JB):  </th> <td>376787.537</td>\n</tr>\n<tr>\n  <th>Skew:</th>           <td> 0.843</td>   <th>  Prob(JB):          </th>  <td>    0.00</td> \n</tr>\n<tr>\n  <th>Kurtosis:</th>       <td>17.378</td>   <th>  Cond. No.          </th>  <td>    32.8</td> \n</tr>\n</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['y' 'z' 'clarity_VVS2']\n"
     ]
    }
   ],
   "source": [
    "display(lasso_for_summary.summary())\n",
    "print(features_train_constant.columns[lasso_for_summary.pvalues > 0.05].values)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "На уровне значимости 0.05 есть три признака, которые имеют нулевые веса. Это признаки ['y'], ['z'], ['claruty_VVS2']."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 531,
   "outputs": [
    {
     "data": {
      "text/plain": "<class 'statsmodels.iolib.summary.Summary'>\n\"\"\"\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                  price   R-squared:                       0.915\nModel:                            OLS   Adj. R-squared:                  0.915\nMethod:                 Least Squares   F-statistic:                 2.022e+04\nDate:                Sun, 16 Oct 2022   Prob (F-statistic):               0.00\nTime:                        23:49:23   Log-Likelihood:            -3.6571e+05\nNo. Observations:               43152   AIC:                         7.315e+05\nDf Residuals:                   43128   BIC:                         7.317e+05\nDf Model:                          23                                         \nCovariance Type:            nonrobust                                         \n=================================================================================\n                    coef    std err          t      P>|t|      [0.025      0.975]\n---------------------------------------------------------------------------------\nconst          4559.9255     35.984    126.720      0.000    4489.396    4630.455\ncarat          4824.8898     27.033    178.483      0.000    4771.905    4877.875\ndepth          -104.5162      7.377    -14.168      0.000    -118.975     -90.058\ntable           -99.3859      7.472    -13.302      0.000    -114.030     -84.741\nx              -697.7107     41.977    -16.621      0.000    -779.986    -615.435\ny                -4.5791     25.474     -0.180      0.857     -54.509      45.351\nz               -43.1341     25.532     -1.689      0.091     -93.178       6.909\ncut_Premium     -27.3443     16.753     -1.632      0.103     -60.181       5.492\ncut_Very Good   -31.8043     16.331     -1.947      0.051     -63.814       0.205\ncut_Good       -188.1689     23.161     -8.124      0.000    -233.566    -142.772\ncut_Fair       -743.5142     38.124    -19.503      0.000    -818.237    -668.791\ncolor_E          46.9627     20.495      2.291      0.022       6.792      87.133\ncolor_F          15.1966     20.743      0.733      0.464     -25.461      55.854\ncolor_G        -135.4776     20.319     -6.668      0.000    -175.303     -95.653\ncolor_H        -642.3429     21.589    -29.753      0.000    -684.658    -600.027\ncolor_I       -1068.3808     24.234    -44.085      0.000   -1115.881   -1020.881\ncolor_J       -1831.0152     29.904    -61.229      0.000   -1889.628   -1772.402\nclarity_VVS1    678.1770     37.375     18.145      0.000     604.920     751.434\nclarity_VVS2    639.5165     35.754     17.887      0.000     569.438     709.595\nclarity_VS1     274.2197     34.085      8.045      0.000     207.413     341.027\nclarity_VS2      14.2698     33.283      0.429      0.668     -50.966      79.506\nclarity_SI1    -572.2643     33.554    -17.055      0.000    -638.031    -506.497\nclarity_SI2   -1461.4776     34.897    -41.879      0.000   -1529.877   -1393.078\nclarity_I1    -3167.5999     58.655    -54.004      0.000   -3282.565   -3052.635\n==============================================================================\nOmnibus:                    14825.528   Durbin-Watson:                   2.002\nProb(Omnibus):                  0.000   Jarque-Bera (JB):           377176.717\nSkew:                           1.088   Prob(JB):                         0.00\nKurtosis:                      17.319   Cond. No.                         32.8\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\"\"\"",
      "text/html": "<table class=\"simpletable\">\n<caption>OLS Regression Results</caption>\n<tr>\n  <th>Dep. Variable:</th>          <td>price</td>      <th>  R-squared:         </th>  <td>   0.915</td>  \n</tr>\n<tr>\n  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th>  <td>   0.915</td>  \n</tr>\n<tr>\n  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th>  <td>2.022e+04</td> \n</tr>\n<tr>\n  <th>Date:</th>             <td>Sun, 16 Oct 2022</td> <th>  Prob (F-statistic):</th>   <td>  0.00</td>   \n</tr>\n<tr>\n  <th>Time:</th>                 <td>23:49:23</td>     <th>  Log-Likelihood:    </th> <td>-3.6571e+05</td>\n</tr>\n<tr>\n  <th>No. Observations:</th>      <td> 43152</td>      <th>  AIC:               </th>  <td>7.315e+05</td> \n</tr>\n<tr>\n  <th>Df Residuals:</th>          <td> 43128</td>      <th>  BIC:               </th>  <td>7.317e+05</td> \n</tr>\n<tr>\n  <th>Df Model:</th>              <td>    23</td>      <th>                     </th>      <td> </td>     \n</tr>\n<tr>\n  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>      <td> </td>     \n</tr>\n</table>\n<table class=\"simpletable\">\n<tr>\n        <td></td>           <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n</tr>\n<tr>\n  <th>const</th>         <td> 4559.9255</td> <td>   35.984</td> <td>  126.720</td> <td> 0.000</td> <td> 4489.396</td> <td> 4630.455</td>\n</tr>\n<tr>\n  <th>carat</th>         <td> 4824.8898</td> <td>   27.033</td> <td>  178.483</td> <td> 0.000</td> <td> 4771.905</td> <td> 4877.875</td>\n</tr>\n<tr>\n  <th>depth</th>         <td> -104.5162</td> <td>    7.377</td> <td>  -14.168</td> <td> 0.000</td> <td> -118.975</td> <td>  -90.058</td>\n</tr>\n<tr>\n  <th>table</th>         <td>  -99.3859</td> <td>    7.472</td> <td>  -13.302</td> <td> 0.000</td> <td> -114.030</td> <td>  -84.741</td>\n</tr>\n<tr>\n  <th>x</th>             <td> -697.7107</td> <td>   41.977</td> <td>  -16.621</td> <td> 0.000</td> <td> -779.986</td> <td> -615.435</td>\n</tr>\n<tr>\n  <th>y</th>             <td>   -4.5791</td> <td>   25.474</td> <td>   -0.180</td> <td> 0.857</td> <td>  -54.509</td> <td>   45.351</td>\n</tr>\n<tr>\n  <th>z</th>             <td>  -43.1341</td> <td>   25.532</td> <td>   -1.689</td> <td> 0.091</td> <td>  -93.178</td> <td>    6.909</td>\n</tr>\n<tr>\n  <th>cut_Premium</th>   <td>  -27.3443</td> <td>   16.753</td> <td>   -1.632</td> <td> 0.103</td> <td>  -60.181</td> <td>    5.492</td>\n</tr>\n<tr>\n  <th>cut_Very Good</th> <td>  -31.8043</td> <td>   16.331</td> <td>   -1.947</td> <td> 0.051</td> <td>  -63.814</td> <td>    0.205</td>\n</tr>\n<tr>\n  <th>cut_Good</th>      <td> -188.1689</td> <td>   23.161</td> <td>   -8.124</td> <td> 0.000</td> <td> -233.566</td> <td> -142.772</td>\n</tr>\n<tr>\n  <th>cut_Fair</th>      <td> -743.5142</td> <td>   38.124</td> <td>  -19.503</td> <td> 0.000</td> <td> -818.237</td> <td> -668.791</td>\n</tr>\n<tr>\n  <th>color_E</th>       <td>   46.9627</td> <td>   20.495</td> <td>    2.291</td> <td> 0.022</td> <td>    6.792</td> <td>   87.133</td>\n</tr>\n<tr>\n  <th>color_F</th>       <td>   15.1966</td> <td>   20.743</td> <td>    0.733</td> <td> 0.464</td> <td>  -25.461</td> <td>   55.854</td>\n</tr>\n<tr>\n  <th>color_G</th>       <td> -135.4776</td> <td>   20.319</td> <td>   -6.668</td> <td> 0.000</td> <td> -175.303</td> <td>  -95.653</td>\n</tr>\n<tr>\n  <th>color_H</th>       <td> -642.3429</td> <td>   21.589</td> <td>  -29.753</td> <td> 0.000</td> <td> -684.658</td> <td> -600.027</td>\n</tr>\n<tr>\n  <th>color_I</th>       <td>-1068.3808</td> <td>   24.234</td> <td>  -44.085</td> <td> 0.000</td> <td>-1115.881</td> <td>-1020.881</td>\n</tr>\n<tr>\n  <th>color_J</th>       <td>-1831.0152</td> <td>   29.904</td> <td>  -61.229</td> <td> 0.000</td> <td>-1889.628</td> <td>-1772.402</td>\n</tr>\n<tr>\n  <th>clarity_VVS1</th>  <td>  678.1770</td> <td>   37.375</td> <td>   18.145</td> <td> 0.000</td> <td>  604.920</td> <td>  751.434</td>\n</tr>\n<tr>\n  <th>clarity_VVS2</th>  <td>  639.5165</td> <td>   35.754</td> <td>   17.887</td> <td> 0.000</td> <td>  569.438</td> <td>  709.595</td>\n</tr>\n<tr>\n  <th>clarity_VS1</th>   <td>  274.2197</td> <td>   34.085</td> <td>    8.045</td> <td> 0.000</td> <td>  207.413</td> <td>  341.027</td>\n</tr>\n<tr>\n  <th>clarity_VS2</th>   <td>   14.2698</td> <td>   33.283</td> <td>    0.429</td> <td> 0.668</td> <td>  -50.966</td> <td>   79.506</td>\n</tr>\n<tr>\n  <th>clarity_SI1</th>   <td> -572.2643</td> <td>   33.554</td> <td>  -17.055</td> <td> 0.000</td> <td> -638.031</td> <td> -506.497</td>\n</tr>\n<tr>\n  <th>clarity_SI2</th>   <td>-1461.4776</td> <td>   34.897</td> <td>  -41.879</td> <td> 0.000</td> <td>-1529.877</td> <td>-1393.078</td>\n</tr>\n<tr>\n  <th>clarity_I1</th>    <td>-3167.5999</td> <td>   58.655</td> <td>  -54.004</td> <td> 0.000</td> <td>-3282.565</td> <td>-3052.635</td>\n</tr>\n</table>\n<table class=\"simpletable\">\n<tr>\n  <th>Omnibus:</th>       <td>14825.528</td> <th>  Durbin-Watson:     </th>  <td>   2.002</td> \n</tr>\n<tr>\n  <th>Prob(Omnibus):</th>  <td> 0.000</td>   <th>  Jarque-Bera (JB):  </th> <td>377176.717</td>\n</tr>\n<tr>\n  <th>Skew:</th>           <td> 1.088</td>   <th>  Prob(JB):          </th>  <td>    0.00</td> \n</tr>\n<tr>\n  <th>Kurtosis:</th>       <td>17.319</td>   <th>  Cond. No.          </th>  <td>    32.8</td> \n</tr>\n</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['y' 'z' 'cut_Premium' 'cut_Very Good' 'color_F' 'clarity_VS2']\n"
     ]
    }
   ],
   "source": [
    "display(elastic_for_summary.summary())\n",
    "print(features_train_constant.columns[elastic_for_summary.pvalues > 0.05].values)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "На уровне значимости 0.05 есть шесть признака, которые имеют нулевые веса. Это признаки ['y'], ['z'], ['cut_Premium'], ['cut_Very Good'], ['color_F'], ['clarity_VS2'].\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### 4. [1 point] Implement one of the elimination algorithms that were described in the Seminar_4 (Elimination by P-value, Forward elimination, Backward elimination), make conclusions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 532,
   "outputs": [],
   "source": [
    "class GroupOfOSLModels(Exception):\n",
    "    pass"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 533,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from pandas import DataFrame\n",
    "\n",
    "\n",
    "def eliminate_pval(model_ols, alpha):\n",
    "    if not isinstance(model_ols, sm.OLS):\n",
    "        raise GroupOfOSLModels('Only OLS eliminate')\n",
    "\n",
    "    x, y = crutch(model_ols)\n",
    "    iterations = model_ols.exog.shape[1]\n",
    "\n",
    "    x = columns_to_drop(alpha, iterations, x, y)\n",
    "\n",
    "    return sm.OLS(y, x)\n",
    "\n",
    "\n",
    "def columns_to_drop(alpha, iterations, x, y):\n",
    "    for i in range(iterations):\n",
    "        res = sm.OLS(y, x).fit()\n",
    "        max_pvalue = res.pvalues.max()\n",
    "        if max_pvalue < alpha:\n",
    "            break\n",
    "        columns_dropping = res.pvalues.index.values[res.pvalues.argmax()]\n",
    "        x = x.drop(labels=columns_dropping, axis=1, level=0)\n",
    "    return x\n",
    "\n",
    "\n",
    "def crutch(model):\n",
    "    x = DataFrame(data=model.exog, columns=[model.exog_names])\n",
    "    y = DataFrame(data=model.endog, columns=[model.endog_names])\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 534,
   "outputs": [
    {
     "data": {
      "text/plain": "<class 'statsmodels.iolib.summary2.Summary'>\n\"\"\"\n                    Results: Ordinary least squares\n=======================================================================\nModel:               OLS               Adj. R-squared:      0.920      \nDependent Variable:  price             AIC:                 728832.0274\nDate:                2022-10-16 23:49  BIC:                 729022.8221\nNo. Observations:    43152             Log-Likelihood:      -3.6439e+05\nDf Model:            21                F-statistic:         2.366e+04  \nDf Residuals:        43130             Prob (F-statistic):  0.00       \nR-squared:           0.920             Scale:               1.2662e+06 \n-----------------------------------------------------------------------\n                Coef.    Std.Err.    t     P>|t|    [0.025     0.975]  \n-----------------------------------------------------------------------\nconst          5997.9955  34.9049 171.8381 0.0000  5929.5812  6066.4098\ncarat          5386.4129  26.1997 205.5905 0.0000  5335.0609  5437.7648\ndepth          -101.6578   6.5305 -15.5665 0.0000  -114.4577   -88.8578\ntable           -61.5921   7.2458  -8.5004 0.0000   -75.7940   -47.3901\nx             -1214.1301  26.2216 -46.3026 0.0000 -1265.5250 -1162.7352\ncut_Premium     -75.2719  16.2344  -4.6366 0.0000  -107.0915   -43.4522\ncut_Very Good   -98.3208  15.8288  -6.2115 0.0000  -129.3456   -67.2961\ncut_Good       -250.7595  22.4646 -11.1624 0.0000  -294.7906  -206.7284\ncut_Fair       -824.8827  36.9688 -22.3129 0.0000  -897.3423  -752.4231\ncolor_E        -221.8187  19.8803 -11.1577 0.0000  -260.7844  -182.8530\ncolor_F        -287.3519  20.1216 -14.2808 0.0000  -326.7907  -247.9132\ncolor_G        -486.3671  19.7095 -24.6768 0.0000  -524.9981  -447.7361\ncolor_H       -1000.9724  20.9420 -47.7973 0.0000 -1042.0192  -959.9256\ncolor_I       -1485.1121  23.5079 -63.1751 0.0000 -1531.1880 -1439.0362\ncolor_J       -2368.3533  29.0079 -81.6450 0.0000 -2425.2094 -2311.4972\nclarity_VVS1   -343.7572  36.2549  -9.4817 0.0000  -414.8174  -272.6969\nclarity_VVS2   -413.2817  34.6819 -11.9164 0.0000  -481.2588  -345.3046\nclarity_VS1    -782.7723  33.0631 -23.6751 0.0000  -847.5766  -717.9680\nclarity_VS2   -1083.9008  32.2849 -33.5730 0.0000 -1147.1799 -1020.6218\nclarity_SI1   -1689.4645  32.5476 -51.9075 0.0000 -1753.2584 -1625.6706\nclarity_SI2   -2658.7763  33.8506 -78.5445 0.0000 -2725.1241 -2592.4286\nclarity_I1    -5316.6456  56.8833 -93.4659 0.0000 -5428.1379 -5205.1534\n-----------------------------------------------------------------------\nOmnibus:             11910.897       Durbin-Watson:          2.000     \nProb(Omnibus):       0.000           Jarque-Bera (JB):       377224.072\nSkew:                0.693           Prob(JB):               0.000     \nKurtosis:            17.418          Condition No.:          24        \n=======================================================================\n\n\"\"\"",
      "text/html": "<table class=\"simpletable\">\n<tr>\n        <td>Model:</td>               <td>OLS</td>         <td>Adj. R-squared:</td>      <td>0.920</td>   \n</tr>\n<tr>\n  <td>Dependent Variable:</td>       <td>price</td>             <td>AIC:</td>         <td>728832.0274</td>\n</tr>\n<tr>\n         <td>Date:</td>        <td>2022-10-16 23:49</td>        <td>BIC:</td>         <td>729022.8221</td>\n</tr>\n<tr>\n   <td>No. Observations:</td>        <td>43152</td>        <td>Log-Likelihood:</td>   <td>-3.6439e+05</td>\n</tr>\n<tr>\n       <td>Df Model:</td>             <td>21</td>           <td>F-statistic:</td>      <td>2.366e+04</td> \n</tr>\n<tr>\n     <td>Df Residuals:</td>          <td>43130</td>      <td>Prob (F-statistic):</td>    <td>0.00</td>    \n</tr>\n<tr>\n      <td>R-squared:</td>            <td>0.920</td>            <td>Scale:</td>        <td>1.2662e+06</td> \n</tr>\n</table>\n<table class=\"simpletable\">\n<tr>\n        <td></td>           <th>Coef.</th>   <th>Std.Err.</th>     <th>t</th>     <th>P>|t|</th>   <th>[0.025</th>     <th>0.975]</th>  \n</tr>\n<tr>\n  <th>const</th>          <td>5997.9955</td>  <td>34.9049</td> <td>171.8381</td> <td>0.0000</td>  <td>5929.5812</td>  <td>6066.4098</td>\n</tr>\n<tr>\n  <th>carat</th>          <td>5386.4129</td>  <td>26.1997</td> <td>205.5905</td> <td>0.0000</td>  <td>5335.0609</td>  <td>5437.7648</td>\n</tr>\n<tr>\n  <th>depth</th>          <td>-101.6578</td>  <td>6.5305</td>  <td>-15.5665</td> <td>0.0000</td>  <td>-114.4577</td>  <td>-88.8578</td> \n</tr>\n<tr>\n  <th>table</th>          <td>-61.5921</td>   <td>7.2458</td>   <td>-8.5004</td> <td>0.0000</td>  <td>-75.7940</td>   <td>-47.3901</td> \n</tr>\n<tr>\n  <th>x</th>             <td>-1214.1301</td>  <td>26.2216</td> <td>-46.3026</td> <td>0.0000</td> <td>-1265.5250</td> <td>-1162.7352</td>\n</tr>\n<tr>\n  <th>cut_Premium</th>    <td>-75.2719</td>   <td>16.2344</td>  <td>-4.6366</td> <td>0.0000</td>  <td>-107.0915</td>  <td>-43.4522</td> \n</tr>\n<tr>\n  <th>cut_Very Good</th>  <td>-98.3208</td>   <td>15.8288</td>  <td>-6.2115</td> <td>0.0000</td>  <td>-129.3456</td>  <td>-67.2961</td> \n</tr>\n<tr>\n  <th>cut_Good</th>       <td>-250.7595</td>  <td>22.4646</td> <td>-11.1624</td> <td>0.0000</td>  <td>-294.7906</td>  <td>-206.7284</td>\n</tr>\n<tr>\n  <th>cut_Fair</th>       <td>-824.8827</td>  <td>36.9688</td> <td>-22.3129</td> <td>0.0000</td>  <td>-897.3423</td>  <td>-752.4231</td>\n</tr>\n<tr>\n  <th>color_E</th>        <td>-221.8187</td>  <td>19.8803</td> <td>-11.1577</td> <td>0.0000</td>  <td>-260.7844</td>  <td>-182.8530</td>\n</tr>\n<tr>\n  <th>color_F</th>        <td>-287.3519</td>  <td>20.1216</td> <td>-14.2808</td> <td>0.0000</td>  <td>-326.7907</td>  <td>-247.9132</td>\n</tr>\n<tr>\n  <th>color_G</th>        <td>-486.3671</td>  <td>19.7095</td> <td>-24.6768</td> <td>0.0000</td>  <td>-524.9981</td>  <td>-447.7361</td>\n</tr>\n<tr>\n  <th>color_H</th>       <td>-1000.9724</td>  <td>20.9420</td> <td>-47.7973</td> <td>0.0000</td> <td>-1042.0192</td>  <td>-959.9256</td>\n</tr>\n<tr>\n  <th>color_I</th>       <td>-1485.1121</td>  <td>23.5079</td> <td>-63.1751</td> <td>0.0000</td> <td>-1531.1880</td> <td>-1439.0362</td>\n</tr>\n<tr>\n  <th>color_J</th>       <td>-2368.3533</td>  <td>29.0079</td> <td>-81.6450</td> <td>0.0000</td> <td>-2425.2094</td> <td>-2311.4972</td>\n</tr>\n<tr>\n  <th>clarity_VVS1</th>   <td>-343.7572</td>  <td>36.2549</td>  <td>-9.4817</td> <td>0.0000</td>  <td>-414.8174</td>  <td>-272.6969</td>\n</tr>\n<tr>\n  <th>clarity_VVS2</th>   <td>-413.2817</td>  <td>34.6819</td> <td>-11.9164</td> <td>0.0000</td>  <td>-481.2588</td>  <td>-345.3046</td>\n</tr>\n<tr>\n  <th>clarity_VS1</th>    <td>-782.7723</td>  <td>33.0631</td> <td>-23.6751</td> <td>0.0000</td>  <td>-847.5766</td>  <td>-717.9680</td>\n</tr>\n<tr>\n  <th>clarity_VS2</th>   <td>-1083.9008</td>  <td>32.2849</td> <td>-33.5730</td> <td>0.0000</td> <td>-1147.1799</td> <td>-1020.6218</td>\n</tr>\n<tr>\n  <th>clarity_SI1</th>   <td>-1689.4645</td>  <td>32.5476</td> <td>-51.9075</td> <td>0.0000</td> <td>-1753.2584</td> <td>-1625.6706</td>\n</tr>\n<tr>\n  <th>clarity_SI2</th>   <td>-2658.7763</td>  <td>33.8506</td> <td>-78.5445</td> <td>0.0000</td> <td>-2725.1241</td> <td>-2592.4286</td>\n</tr>\n<tr>\n  <th>clarity_I1</th>    <td>-5316.6456</td>  <td>56.8833</td> <td>-93.4659</td> <td>0.0000</td> <td>-5428.1379</td> <td>-5205.1534</td>\n</tr>\n</table>\n<table class=\"simpletable\">\n<tr>\n     <td>Omnibus:</td>    <td>11910.897</td>  <td>Durbin-Watson:</td>      <td>2.000</td>  \n</tr>\n<tr>\n  <td>Prob(Omnibus):</td>   <td>0.000</td>   <td>Jarque-Bera (JB):</td> <td>377224.072</td>\n</tr>\n<tr>\n       <td>Skew:</td>       <td>0.693</td>       <td>Prob(JB):</td>        <td>0.000</td>  \n</tr>\n<tr>\n     <td>Kurtosis:</td>    <td>17.418</td>    <td>Condition No.:</td>       <td>24</td>    \n</tr>\n</table>"
     },
     "execution_count": 534,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "threshold = 0.05\n",
    "elimination_model = eliminate_pval(model_ols, threshold)\n",
    "elim_results = elimination_model.fit()\n",
    "elim_results.summary2()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Сработало! Порогов выше 0.05 больше нет!"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### 5. [1 point] Find the best (in terms of RMSE) $\\alpha$ for Lasso regression using cross-validation with 4 folds. You must select values from range $[10^{-4}, 10^{3}]$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 535,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best alpha =  13.681576279674704\n"
     ]
    }
   ],
   "source": [
    "fold = 4\n",
    "alphas = np.logspace(-4, 3, 1000)\n",
    "search = GridSearchCV(Ridge(), [{\"alpha\": alphas}], scoring=\"neg_mean_squared_error\", cv=fold)\n",
    "search.fit(features_train_constant, y_train)\n",
    "print(\"Best alpha = \", search.best_params_['alpha'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Gradient descent\n",
    "\n",
    "#### 6. [3.5 points] Implement a Ridge regression model for the MSE loss function, trained by gradient descent.\n",
    "\n",
    "All calculations must be vectorized, and python loops can only be used for gradient descent iterations. As a stop criterion, you must use (simultaneously):\n",
    "\n",
    "* checking for the Absolute-value norm of the weight difference on two adjacent iterations (for example, less than some small number of the order of $10^{-6}$, set by the `tolerance` parameter);\n",
    "* reaching the maximum number of iterations (for example, 10000, set by the `max_iter` parameter).\n",
    "\n",
    "You need to implement:\n",
    "\n",
    "* Full gradient descent:\n",
    "\n",
    "$$\n",
    "w_{k + 1} = w_{k} - \\eta_{k} \\nabla_{w} Q(w_{k}).\n",
    "$$\n",
    "\n",
    "* Stochastic Gradient Descent:\n",
    "\n",
    "$$\n",
    "w_{k + 1} = w_{k} - \\eta_{k} \\nabla_{w} q_{i_{k}}(w_{k}).\n",
    "$$\n",
    "\n",
    "$\\nabla_{w} q_{i_{k}}(w_{k}) \\, $ is the estimate of the gradient over the batch of objects selected randomly.\n",
    "\n",
    "* Momentum method:\n",
    "\n",
    "$$\n",
    "h_0 = 0, \\\\\n",
    "h_{k + 1} = \\alpha h_{k} + \\eta_k \\nabla_{w} Q(w_{k}), \\\\\n",
    "w_{k + 1} = w_{k} - h_{k + 1}.\n",
    "$$\n",
    "\n",
    "* Adagrad method:\n",
    "\n",
    "$$\n",
    "G_0 = 0, \\\\\n",
    "G_{k + 1} = G_{k} + (\\nabla_{w} Q(w_{k+1}))^2, \\\\\n",
    "w_{k + 1} = w_{k} - \\eta * \\frac{\\nabla_{w} Q(w_{k+1})}{\\sqrt{G_{k+1} + \\epsilon}}.\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "To make sure that the optimization process really converges, we will use the `loss_history` class attribute. After calling the `fit` method, it should contain the values of the loss function for all iterations, starting from the first one (before the first step on the anti-gradient).\n",
    "\n",
    "You need to initialize the weights with a random vector from normal distribution. The following is a template class that needs to contain the code implementing all variations of the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 536,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator\n",
    "\n",
    "\n",
    "class LinReg(BaseEstimator):\n",
    "    def __init__(self, delta=1.0, gd_type='Momentum',\n",
    "                 tolerance=1e-4, max_iter=1000, w0=None, eta=1e-2, alpha=1e-3):\n",
    "        self.gd_type = gd_type\n",
    "        self.delta = delta\n",
    "        self.tolerance = tolerance\n",
    "        self.max_iter = max_iter\n",
    "        self.w0 = w0\n",
    "        self.alpha = alpha\n",
    "        self.w = None\n",
    "        self.eta = eta\n",
    "        self.loss_history = None\n",
    "\n",
    "    support = ['GradientDescent', 'StochasticDescent', 'Momentum']\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        w_tmp, h_tmp = self.filling_missed_values(X)\n",
    "\n",
    "        for i in range(0, self.max_iter):\n",
    "            if self.gd_type == self.support[0]:\n",
    "                self.gradient(X, y)\n",
    "            elif self.gd_type == self.support[1]:\n",
    "                self.stoch(X, y)\n",
    "            else:\n",
    "                self.momentum(X, h_tmp, y)\n",
    "            self.loss_history.append(self.calc_loss(X, y))\n",
    "            if np.linalg.norm(self.w - w_tmp) < self.tolerance:\n",
    "                break\n",
    "            w_tmp = np.array(self.w)\n",
    "\n",
    "        return self\n",
    "\n",
    "    def momentum(self, X, h, y):\n",
    "        indexes = np.random.choice(X.shape[0], int(X.shape[0] * self.delta))\n",
    "        h = self.alpha * h + self.eta * self.calc_gradient(np.take(X, indexes, axis=0), np.take(y, indexes))\n",
    "        self.w -= h\n",
    "\n",
    "    def stoch(self, X, y):\n",
    "        indexes = np.random.choice(X.shape[0], int(X.shape[0] * self.delta))\n",
    "        self.gradient(np.take(X, indexes, axis=0), np.take(y, indexes))\n",
    "\n",
    "    def gradient(self, X, y):\n",
    "        self.w -= self.eta * self.calc_gradient(X, y)\n",
    "\n",
    "    def filling_missed_values(self, X):\n",
    "        if self.gd_type not in self.support:\n",
    "            raise GroupOfOSLModels('Only OLS model')\n",
    "        np.random.seed(0)\n",
    "        self.loss_history = []\n",
    "        if self.w0 is None:\n",
    "            self.w0 = np.zeros(X.shape[1])\n",
    "        self.w = np.array(self.w0)\n",
    "        cur_w = np.array(self.w)\n",
    "        h = np.zeros(X.shape[1])\n",
    "        return cur_w, h\n",
    "\n",
    "    def predict(self, X):\n",
    "        if self.w is None:\n",
    "            raise Exception('Not trained yet')\n",
    "        return self.deep_calc_grad(X)\n",
    "\n",
    "    def calc_gradient(self, X, y):\n",
    "        return 2 * np.dot(X.T, self.deep_calc_grad(X) - y) / y.shape[0]\n",
    "\n",
    "    def deep_calc_grad(self, X):\n",
    "        return np.dot(X, self.w)\n",
    "\n",
    "    def calc_loss(self, X, y):\n",
    "\n",
    "        return np.mean(self.deep_calc(X, y))\n",
    "\n",
    "    def deep_calc(self, X, y):\n",
    "        return (self.predict(X) - y) ** 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### 7. [1 points] Train and validate \"hand-written\" models on the same data, and compare the quality with the Sklearn or StatsModels methods. Investigate the effect of the `max_iter` and `alpha` parameters on the optimization process. Is it consistent with your expectations?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 536,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 536,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### 8. [1 points] Plot graphs (on the same picture) of the dependence of the loss function value on the iteration number for Full GD, SGD, Momentum and Adagrad. Draw conclusions about the rate of convergence of various modifications of gradient descent.\n",
    "\n",
    "Don't forget about what *beautiful* graphics should look like!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 537,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# your code here \n",
    "#╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 537,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}