{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# HSE 2022: Mathematical Methods for Data Analysis\n",
    "\n",
    "## Homework 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sklearn\n",
    "from sklearn import datasets\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.regression.linear_model import OLSResults\n",
    "from math import sqrt\n",
    "import random\n",
    "import sys\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "sns.set(style=\"darkgrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Data\n",
    "\n",
    "For this homework we use Dataset from seaborn on diamonds prices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "data = sns.load_dataset('diamonds')\n",
    "\n",
    "y = data.price\n",
    "x = data.drop(['price'], axis=1)\n",
    "columns = data.drop(['price'], axis=1).columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Linear regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### 0. [0.25 points] Encode categorical variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>carat</th>\n",
       "      <th>cut</th>\n",
       "      <th>color</th>\n",
       "      <th>clarity</th>\n",
       "      <th>depth</th>\n",
       "      <th>table</th>\n",
       "      <th>price</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>z</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.23</td>\n",
       "      <td>Ideal</td>\n",
       "      <td>E</td>\n",
       "      <td>SI2</td>\n",
       "      <td>61.5</td>\n",
       "      <td>55.0</td>\n",
       "      <td>326</td>\n",
       "      <td>3.95</td>\n",
       "      <td>3.98</td>\n",
       "      <td>2.43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.21</td>\n",
       "      <td>Premium</td>\n",
       "      <td>E</td>\n",
       "      <td>SI1</td>\n",
       "      <td>59.8</td>\n",
       "      <td>61.0</td>\n",
       "      <td>326</td>\n",
       "      <td>3.89</td>\n",
       "      <td>3.84</td>\n",
       "      <td>2.31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.23</td>\n",
       "      <td>Good</td>\n",
       "      <td>E</td>\n",
       "      <td>VS1</td>\n",
       "      <td>56.9</td>\n",
       "      <td>65.0</td>\n",
       "      <td>327</td>\n",
       "      <td>4.05</td>\n",
       "      <td>4.07</td>\n",
       "      <td>2.31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.29</td>\n",
       "      <td>Premium</td>\n",
       "      <td>I</td>\n",
       "      <td>VS2</td>\n",
       "      <td>62.4</td>\n",
       "      <td>58.0</td>\n",
       "      <td>334</td>\n",
       "      <td>4.20</td>\n",
       "      <td>4.23</td>\n",
       "      <td>2.63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.31</td>\n",
       "      <td>Good</td>\n",
       "      <td>J</td>\n",
       "      <td>SI2</td>\n",
       "      <td>63.3</td>\n",
       "      <td>58.0</td>\n",
       "      <td>335</td>\n",
       "      <td>4.34</td>\n",
       "      <td>4.35</td>\n",
       "      <td>2.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53935</th>\n",
       "      <td>0.72</td>\n",
       "      <td>Ideal</td>\n",
       "      <td>D</td>\n",
       "      <td>SI1</td>\n",
       "      <td>60.8</td>\n",
       "      <td>57.0</td>\n",
       "      <td>2757</td>\n",
       "      <td>5.75</td>\n",
       "      <td>5.76</td>\n",
       "      <td>3.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53936</th>\n",
       "      <td>0.72</td>\n",
       "      <td>Good</td>\n",
       "      <td>D</td>\n",
       "      <td>SI1</td>\n",
       "      <td>63.1</td>\n",
       "      <td>55.0</td>\n",
       "      <td>2757</td>\n",
       "      <td>5.69</td>\n",
       "      <td>5.75</td>\n",
       "      <td>3.61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53937</th>\n",
       "      <td>0.70</td>\n",
       "      <td>Very Good</td>\n",
       "      <td>D</td>\n",
       "      <td>SI1</td>\n",
       "      <td>62.8</td>\n",
       "      <td>60.0</td>\n",
       "      <td>2757</td>\n",
       "      <td>5.66</td>\n",
       "      <td>5.68</td>\n",
       "      <td>3.56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53938</th>\n",
       "      <td>0.86</td>\n",
       "      <td>Premium</td>\n",
       "      <td>H</td>\n",
       "      <td>SI2</td>\n",
       "      <td>61.0</td>\n",
       "      <td>58.0</td>\n",
       "      <td>2757</td>\n",
       "      <td>6.15</td>\n",
       "      <td>6.12</td>\n",
       "      <td>3.74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53939</th>\n",
       "      <td>0.75</td>\n",
       "      <td>Ideal</td>\n",
       "      <td>D</td>\n",
       "      <td>SI2</td>\n",
       "      <td>62.2</td>\n",
       "      <td>55.0</td>\n",
       "      <td>2757</td>\n",
       "      <td>5.83</td>\n",
       "      <td>5.87</td>\n",
       "      <td>3.64</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>53940 rows Ã— 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       carat        cut color clarity  depth  table  price     x     y     z\n",
       "0       0.23      Ideal     E     SI2   61.5   55.0    326  3.95  3.98  2.43\n",
       "1       0.21    Premium     E     SI1   59.8   61.0    326  3.89  3.84  2.31\n",
       "2       0.23       Good     E     VS1   56.9   65.0    327  4.05  4.07  2.31\n",
       "3       0.29    Premium     I     VS2   62.4   58.0    334  4.20  4.23  2.63\n",
       "4       0.31       Good     J     SI2   63.3   58.0    335  4.34  4.35  2.75\n",
       "...      ...        ...   ...     ...    ...    ...    ...   ...   ...   ...\n",
       "53935   0.72      Ideal     D     SI1   60.8   57.0   2757  5.75  5.76  3.50\n",
       "53936   0.72       Good     D     SI1   63.1   55.0   2757  5.69  5.75  3.61\n",
       "53937   0.70  Very Good     D     SI1   62.8   60.0   2757  5.66  5.68  3.56\n",
       "53938   0.86    Premium     H     SI2   61.0   58.0   2757  6.15  6.12  3.74\n",
       "53939   0.75      Ideal     D     SI2   62.2   55.0   2757  5.83  5.87  3.64\n",
       "\n",
       "[53940 rows x 10 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "\n",
    "display(data)\n",
    "\n",
    "encoder = OrdinalEncoder()\n",
    "data_ordinal = pd.DataFrame(encoder.fit_transform(data),\n",
    "                            columns=data.columns)\n",
    "features = data_ordinal.drop('price', axis=1)\n",
    "target = data_ordinal['price']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### 1. [0.25 points] Split the data into train and test sets with ratio 80:20 with random_state=17."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "features_train, features_test, target_train, target_test = train_test_split(features, target, test_size=0.2, random_state=17)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### 2. [1 point] Train models on train data using StatsModels library and apply it to the test set; use $RMSE$ and $R^2$ as the quality measure.\n",
    "\n",
    "* [`LinearRegression`](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html);\n",
    "* [`Ridge`](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html) with $\\alpha = 0.01$;\n",
    "* [`Lasso`](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lasso.html) with $\\alpha = 0.01$\n",
    "* [`ElasticNet`](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.ElasticNet.html) with $\\alpha = 0.01$, $l_{1}$_$ratio = 0.6$\n",
    "\n",
    "Don't forget to scale the data before training the models with StandardScaler!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "numerics = ['carat', 'depth', 'table', 'x', 'y', 'z']\n",
    "scaler.fit(features_train[numerics])\n",
    "features_train[numerics] = scaler.transform(features_train[numerics])\n",
    "features_test[numerics] = scaler.transform(features_test[numerics])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# def show_rmse(model):\n",
    "#     y_pred = model.predict(X_test)\n",
    "#     y_train_pred = model.predict(X_train)\n",
    "#\n",
    "#     print(\"Train RMSE = %.4f\" % rmse(y_train, y_train_pred))\n",
    "#     print(\"Test RMSE = %.4f\" % rmse(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE = 776022.1336650985\n",
      "RMSE = 880.9211847067241\n",
      "R2 =  0.9148058965444855\n"
     ]
    }
   ],
   "source": [
    "features_train = sm.add_constant(features_train)\n",
    "features_test = sm.add_constant(features_test)\n",
    "\n",
    "linear_model = sm.OLS(target_train, features_train)\n",
    "linear_results = linear_model.fit()\n",
    "predicted_test = linear_results.predict(features_test)\n",
    "mse = mean_squared_error(y_true=target_test, y_pred=predicted_test)\n",
    "print(\"MSE =\", mse)\n",
    "print(\"RMSE =\", mse ** .5)\n",
    "print('R2 = ', r2_score(y_true=target_test, y_pred=predicted_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE = 796152.2211403317\n",
      "RMSE = 892.2736245907596\n",
      "R2 =  0.9125959534506798\n"
     ]
    }
   ],
   "source": [
    "ridge_model = linear_model.fit_regularized(L1_wt=0, alpha=0.01)\n",
    "ridge_result = sm.regression.linear_model.OLSResults(linear_model, ridge_model.params, linear_model.normalized_cov_params)\n",
    "predicted_ridge = ridge_model.predict(features_test)\n",
    "mse = mean_squared_error(y_true=target_test, y_pred=predicted_ridge)\n",
    "print(\"MSE =\", mse)\n",
    "print(\"RMSE =\", mse ** .5)\n",
    "print('R2 = ', r2_score(y_true=target_test, y_pred=predicted_ridge))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE = 777573.0826038036\n",
      "RMSE = 881.8010447962758\n",
      "R2 =  0.9146356285861295\n"
     ]
    }
   ],
   "source": [
    "lasso_model = linear_model.fit_regularized(L1_wt=1, alpha=0.01)\n",
    "lasso_result = sm.regression.linear_model.OLSResults(linear_model, lasso_model.params, linear_model.normalized_cov_params)\n",
    "predicted_lasso = lasso_model.predict(features_test)\n",
    "mse = mean_squared_error(y_true=target_test, y_pred=predicted_lasso)\n",
    "print(\"MSE =\", mse)\n",
    "print(\"RMSE =\", mse ** .5)\n",
    "print('R2 = ', r2_score(y_true=target_test, y_pred=predicted_lasso))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE = 781737.335056795\n",
      "RMSE = 884.1591118440136\n",
      "R2 =  0.9141784641073027\n"
     ]
    }
   ],
   "source": [
    "elastic_model = linear_model.fit_regularized(L1_wt=0.6, alpha=0.01)\n",
    "elastic_result = sm.regression.linear_model.OLSResults(linear_model, elastic_model.params, linear_model.normalized_cov_params)\n",
    "predicted_elastic = elastic_model.predict(features_test)\n",
    "mse = mean_squared_error(y_true=target_test, y_pred=predicted_elastic)\n",
    "print(\"MSE =\", mse)\n",
    "print(\"RMSE =\", mse ** .5)\n",
    "print('R2 = ', r2_score(y_true=target_test, y_pred=predicted_elastic))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### 3. [1 point] Explore the values of the parameters of the resulting models and compare the number of zero weights in them. Comment on the significance of the coefficients, overal model significance and other related factors from the results table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "        <td>Model:</td>               <td>OLS</td>         <td>Adj. R-squared:</td>      <td>0.914</td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <td>Dependent Variable:</td>       <td>price</td>             <td>AIC:</td>         <td>707856.7639</td>\n",
       "</tr>\n",
       "<tr>\n",
       "         <td>Date:</td>        <td>2022-10-16 19:31</td>        <td>BIC:</td>         <td>707943.4887</td>\n",
       "</tr>\n",
       "<tr>\n",
       "   <td>No. Observations:</td>        <td>43152</td>        <td>Log-Likelihood:</td>   <td>-3.5392e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "       <td>Df Model:</td>              <td>9</td>           <td>F-statistic:</td>      <td>5.078e+04</td> \n",
       "</tr>\n",
       "<tr>\n",
       "     <td>Df Residuals:</td>          <td>43142</td>      <td>Prob (F-statistic):</td>    <td>0.00</td>    \n",
       "</tr>\n",
       "<tr>\n",
       "      <td>R-squared:</td>            <td>0.914</td>            <td>Scale:</td>        <td>7.7894e+05</td> \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "     <td></td>        <th>Coef.</th>   <th>Std.Err.</th>     <th>t</th>     <th>P>|t|</th>   <th>[0.025</th>     <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th>    <td>2624.2705</td>  <td>16.3907</td> <td>160.1069</td> <td>0.0000</td>  <td>2592.1444</td>  <td>2656.3967</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>carat</th>    <td>2699.0777</td>  <td>21.4817</td> <td>125.6453</td> <td>0.0000</td>  <td>2656.9731</td>  <td>2741.1823</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>cut</th>       <td>52.8693</td>   <td>4.2725</td>   <td>12.3744</td> <td>0.0000</td>   <td>44.4951</td>    <td>61.2434</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>color</th>    <td>-190.5337</td>  <td>2.6168</td>  <td>-72.8112</td> <td>0.0000</td>  <td>-195.6627</td>  <td>-185.4047</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>clarity</th>  <td>214.5259</td>   <td>2.5507</td>   <td>84.1037</td> <td>0.0000</td>  <td>209.5264</td>   <td>219.5253</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>depth</th>    <td>-46.6299</td>   <td>9.4944</td>   <td>-4.9113</td> <td>0.0000</td>  <td>-65.2391</td>   <td>-28.0208</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>table</th>    <td>-121.5332</td>  <td>4.6368</td>  <td>-26.2106</td> <td>0.0000</td>  <td>-130.6214</td>  <td>-112.4450</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x</th>       <td>-1372.6718</td>  <td>89.2151</td> <td>-15.3861</td> <td>0.0000</td> <td>-1547.5351</td> <td>-1197.8085</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>y</th>        <td>1893.3603</td>  <td>87.5187</td>  <td>21.6338</td> <td>0.0000</td>  <td>1721.8220</td>  <td>2064.8987</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>z</th>        <td>-188.8980</td>  <td>66.9466</td>  <td>-2.8216</td> <td>0.0048</td>  <td>-320.1145</td>  <td>-57.6814</td> \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "     <td>Omnibus:</td>    <td>7912.396</td>  <td>Durbin-Watson:</td>     <td>1.997</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <td>Prob(Omnibus):</td>   <td>0.000</td>  <td>Jarque-Bera (JB):</td> <td>51815.141</td>\n",
       "</tr>\n",
       "<tr>\n",
       "       <td>Skew:</td>       <td>0.724</td>      <td>Prob(JB):</td>       <td>0.000</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "     <td>Kurtosis:</td>     <td>8.169</td>   <td>Condition No.:</td>      <td>152</td>   \n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary2.Summary'>\n",
       "\"\"\"\n",
       "                  Results: Ordinary least squares\n",
       "====================================================================\n",
       "Model:              OLS              Adj. R-squared:     0.914      \n",
       "Dependent Variable: price            AIC:                707856.7639\n",
       "Date:               2022-10-16 19:31 BIC:                707943.4887\n",
       "No. Observations:   43152            Log-Likelihood:     -3.5392e+05\n",
       "Df Model:           9                F-statistic:        5.078e+04  \n",
       "Df Residuals:       43142            Prob (F-statistic): 0.00       \n",
       "R-squared:          0.914            Scale:              7.7894e+05 \n",
       "--------------------------------------------------------------------\n",
       "             Coef.    Std.Err.    t     P>|t|    [0.025     0.975]  \n",
       "--------------------------------------------------------------------\n",
       "const       2624.2705  16.3907 160.1069 0.0000  2592.1444  2656.3967\n",
       "carat       2699.0777  21.4817 125.6453 0.0000  2656.9731  2741.1823\n",
       "cut           52.8693   4.2725  12.3744 0.0000    44.4951    61.2434\n",
       "color       -190.5337   2.6168 -72.8112 0.0000  -195.6627  -185.4047\n",
       "clarity      214.5259   2.5507  84.1037 0.0000   209.5264   219.5253\n",
       "depth        -46.6299   9.4944  -4.9113 0.0000   -65.2391   -28.0208\n",
       "table       -121.5332   4.6368 -26.2106 0.0000  -130.6214  -112.4450\n",
       "x          -1372.6718  89.2151 -15.3861 0.0000 -1547.5351 -1197.8085\n",
       "y           1893.3603  87.5187  21.6338 0.0000  1721.8220  2064.8987\n",
       "z           -188.8980  66.9466  -2.8216 0.0048  -320.1145   -57.6814\n",
       "--------------------------------------------------------------------\n",
       "Omnibus:              7912.396      Durbin-Watson:         1.997    \n",
       "Prob(Omnibus):        0.000         Jarque-Bera (JB):      51815.141\n",
       "Skew:                 0.724         Prob(JB):              0.000    \n",
       "Kurtosis:             8.169         Condition No.:         152      \n",
       "====================================================================\n",
       "\n",
       "\"\"\""
      ]
     },
     "execution_count": 369,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear_results.summary2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "print(linear_results.pvalues[linear_results.pvalues > 0.05].index.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "        <td>Model:</td>               <td>OLS</td>         <td>Adj. R-squared:</td>      <td>0.911</td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <td>Dependent Variable:</td>       <td>price</td>             <td>AIC:</td>         <td>709152.9296</td>\n",
       "</tr>\n",
       "<tr>\n",
       "         <td>Date:</td>        <td>2022-10-16 19:31</td>        <td>BIC:</td>         <td>709239.6544</td>\n",
       "</tr>\n",
       "<tr>\n",
       "   <td>No. Observations:</td>        <td>43152</td>        <td>Log-Likelihood:</td>   <td>-3.5457e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "       <td>Df Model:</td>              <td>9</td>           <td>F-statistic:</td>      <td>4.913e+04</td> \n",
       "</tr>\n",
       "<tr>\n",
       "     <td>Df Residuals:</td>          <td>43142</td>      <td>Prob (F-statistic):</td>    <td>0.00</td>    \n",
       "</tr>\n",
       "<tr>\n",
       "      <td>R-squared:</td>            <td>0.911</td>            <td>Scale:</td>        <td>8.0269e+05</td> \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "     <td></td>       <th>Coef.</th>   <th>Std.Err.</th>     <th>t</th>     <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th>   <td>2238.4162</td>  <td>16.6388</td> <td>134.5302</td> <td>0.0000</td> <td>2205.8039</td> <td>2271.0285</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>carat</th>   <td>2165.6306</td>  <td>21.8068</td>  <td>99.3099</td> <td>0.0000</td> <td>2122.8888</td> <td>2208.3723</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>cut</th>     <td>117.0268</td>   <td>4.3371</td>   <td>26.9825</td> <td>0.0000</td> <td>108.5260</td>  <td>125.5277</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>color</th>   <td>-161.3356</td>  <td>2.6564</td>  <td>-60.7343</td> <td>0.0000</td> <td>-166.5422</td> <td>-156.1290</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>clarity</th> <td>246.8583</td>   <td>2.5893</td>   <td>95.3368</td> <td>0.0000</td> <td>241.7832</td>  <td>251.9334</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>depth</th>   <td>-56.4293</td>   <td>9.6380</td>   <td>-5.8549</td> <td>0.0000</td> <td>-75.3201</td>  <td>-37.5386</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>table</th>   <td>-128.3472</td>  <td>4.7070</td>  <td>-27.2675</td> <td>0.0000</td> <td>-137.5730</td> <td>-119.1215</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x</th>       <td>169.0071</td>   <td>90.5651</td>  <td>1.8661</td>  <td>0.0620</td>  <td>-8.5022</td>  <td>346.5164</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>y</th>       <td>500.0043</td>   <td>88.8430</td>  <td>5.6280</td>  <td>0.0000</td> <td>325.8702</td>  <td>674.1383</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>z</th>       <td>183.2733</td>   <td>67.9596</td>  <td>2.6968</td>  <td>0.0070</td>  <td>50.0712</td>  <td>316.4755</td> \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "     <td>Omnibus:</td>    <td>8038.335</td>  <td>Durbin-Watson:</td>     <td>1.996</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <td>Prob(Omnibus):</td>   <td>0.000</td>  <td>Jarque-Bera (JB):</td> <td>36234.753</td>\n",
       "</tr>\n",
       "<tr>\n",
       "       <td>Skew:</td>       <td>0.849</td>      <td>Prob(JB):</td>       <td>0.000</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "     <td>Kurtosis:</td>     <td>7.156</td>   <td>Condition No.:</td>      <td>152</td>   \n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary2.Summary'>\n",
       "\"\"\"\n",
       "                  Results: Ordinary least squares\n",
       "====================================================================\n",
       "Model:              OLS              Adj. R-squared:     0.911      \n",
       "Dependent Variable: price            AIC:                709152.9296\n",
       "Date:               2022-10-16 19:31 BIC:                709239.6544\n",
       "No. Observations:   43152            Log-Likelihood:     -3.5457e+05\n",
       "Df Model:           9                F-statistic:        4.913e+04  \n",
       "Df Residuals:       43142            Prob (F-statistic): 0.00       \n",
       "R-squared:          0.911            Scale:              8.0269e+05 \n",
       "---------------------------------------------------------------------\n",
       "            Coef.    Std.Err.     t      P>|t|     [0.025     0.975] \n",
       "---------------------------------------------------------------------\n",
       "const     2238.4162   16.6388  134.5302  0.0000  2205.8039  2271.0285\n",
       "carat     2165.6306   21.8068   99.3099  0.0000  2122.8888  2208.3723\n",
       "cut        117.0268    4.3371   26.9825  0.0000   108.5260   125.5277\n",
       "color     -161.3356    2.6564  -60.7343  0.0000  -166.5422  -156.1290\n",
       "clarity    246.8583    2.5893   95.3368  0.0000   241.7832   251.9334\n",
       "depth      -56.4293    9.6380   -5.8549  0.0000   -75.3201   -37.5386\n",
       "table     -128.3472    4.7070  -27.2675  0.0000  -137.5730  -119.1215\n",
       "x          169.0071   90.5651    1.8661  0.0620    -8.5022   346.5164\n",
       "y          500.0043   88.8430    5.6280  0.0000   325.8702   674.1383\n",
       "z          183.2733   67.9596    2.6968  0.0070    50.0712   316.4755\n",
       "--------------------------------------------------------------------\n",
       "Omnibus:              8038.335      Durbin-Watson:         1.996    \n",
       "Prob(Omnibus):        0.000         Jarque-Bera (JB):      36234.753\n",
       "Skew:                 0.849         Prob(JB):              0.000    \n",
       "Kurtosis:             7.156         Condition No.:         152      \n",
       "====================================================================\n",
       "\n",
       "\"\"\""
      ]
     },
     "execution_count": 371,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ridge_result.summary2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['x']\n"
     ]
    }
   ],
   "source": [
    "print(features_train.columns[ridge_result.pvalues > 0.05].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "        <td>Model:</td>               <td>OLS</td>         <td>Adj. R-squared:</td>      <td>0.913</td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <td>Dependent Variable:</td>       <td>price</td>             <td>AIC:</td>         <td>708261.0125</td>\n",
       "</tr>\n",
       "<tr>\n",
       "         <td>Date:</td>        <td>2022-10-16 19:31</td>        <td>BIC:</td>         <td>708347.7374</td>\n",
       "</tr>\n",
       "<tr>\n",
       "   <td>No. Observations:</td>        <td>43152</td>        <td>Log-Likelihood:</td>   <td>-3.5412e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "       <td>Df Model:</td>              <td>9</td>           <td>F-statistic:</td>      <td>5.026e+04</td> \n",
       "</tr>\n",
       "<tr>\n",
       "     <td>Df Residuals:</td>          <td>43142</td>      <td>Prob (F-statistic):</td>    <td>0.00</td>    \n",
       "</tr>\n",
       "<tr>\n",
       "      <td>R-squared:</td>            <td>0.913</td>            <td>Scale:</td>        <td>7.8627e+05</td> \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "     <td></td>       <th>Coef.</th>   <th>Std.Err.</th>     <th>t</th>     <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th>   <td>2593.6833</td>  <td>16.4677</td> <td>157.5013</td> <td>0.0000</td> <td>2561.4063</td> <td>2625.9603</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>carat</th>   <td>2702.1952</td>  <td>21.5826</td> <td>125.2026</td> <td>0.0000</td> <td>2659.8930</td> <td>2744.4975</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>cut</th>      <td>62.5648</td>   <td>4.2925</td>   <td>14.5752</td> <td>0.0000</td>  <td>54.1514</td>   <td>70.9783</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>color</th>   <td>-190.6321</td>  <td>2.6291</td>  <td>-72.5084</td> <td>0.0000</td> <td>-195.7852</td> <td>-185.4790</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>clarity</th> <td>216.1488</td>   <td>2.5627</td>   <td>84.3440</td> <td>0.0000</td> <td>211.1259</td>  <td>221.1718</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>depth</th>   <td>-59.4398</td>   <td>9.5390</td>   <td>-6.2313</td> <td>0.0000</td> <td>-78.1363</td>  <td>-40.7433</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>table</th>   <td>-134.7719</td>  <td>4.6586</td>  <td>-28.9299</td> <td>0.0000</td> <td>-143.9028</td> <td>-125.6410</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x</th>       <td>238.3825</td>   <td>89.6340</td>  <td>2.6595</td>  <td>0.0078</td>  <td>62.6982</td>  <td>414.0668</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>y</th>       <td>239.0742</td>   <td>87.9296</td>  <td>2.7189</td>  <td>0.0066</td>  <td>66.7305</td>  <td>411.4179</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>z</th>       <td>-145.8133</td>  <td>67.2609</td>  <td>-2.1679</td> <td>0.0302</td> <td>-277.6459</td> <td>-13.9807</td> \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "     <td>Omnibus:</td>    <td>7859.417</td>  <td>Durbin-Watson:</td>     <td>1.996</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <td>Prob(Omnibus):</td>   <td>0.000</td>  <td>Jarque-Bera (JB):</td> <td>53481.710</td>\n",
       "</tr>\n",
       "<tr>\n",
       "       <td>Skew:</td>       <td>0.706</td>      <td>Prob(JB):</td>       <td>0.000</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "     <td>Kurtosis:</td>     <td>8.268</td>   <td>Condition No.:</td>      <td>152</td>   \n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary2.Summary'>\n",
       "\"\"\"\n",
       "                  Results: Ordinary least squares\n",
       "====================================================================\n",
       "Model:              OLS              Adj. R-squared:     0.913      \n",
       "Dependent Variable: price            AIC:                708261.0125\n",
       "Date:               2022-10-16 19:31 BIC:                708347.7374\n",
       "No. Observations:   43152            Log-Likelihood:     -3.5412e+05\n",
       "Df Model:           9                F-statistic:        5.026e+04  \n",
       "Df Residuals:       43142            Prob (F-statistic): 0.00       \n",
       "R-squared:          0.913            Scale:              7.8627e+05 \n",
       "---------------------------------------------------------------------\n",
       "            Coef.    Std.Err.     t      P>|t|     [0.025     0.975] \n",
       "---------------------------------------------------------------------\n",
       "const     2593.6833   16.4677  157.5013  0.0000  2561.4063  2625.9603\n",
       "carat     2702.1952   21.5826  125.2026  0.0000  2659.8930  2744.4975\n",
       "cut         62.5648    4.2925   14.5752  0.0000    54.1514    70.9783\n",
       "color     -190.6321    2.6291  -72.5084  0.0000  -195.7852  -185.4790\n",
       "clarity    216.1488    2.5627   84.3440  0.0000   211.1259   221.1718\n",
       "depth      -59.4398    9.5390   -6.2313  0.0000   -78.1363   -40.7433\n",
       "table     -134.7719    4.6586  -28.9299  0.0000  -143.9028  -125.6410\n",
       "x          238.3825   89.6340    2.6595  0.0078    62.6982   414.0668\n",
       "y          239.0742   87.9296    2.7189  0.0066    66.7305   411.4179\n",
       "z         -145.8133   67.2609   -2.1679  0.0302  -277.6459   -13.9807\n",
       "--------------------------------------------------------------------\n",
       "Omnibus:              7859.417      Durbin-Watson:         1.996    \n",
       "Prob(Omnibus):        0.000         Jarque-Bera (JB):      53481.710\n",
       "Skew:                 0.706         Prob(JB):              0.000    \n",
       "Kurtosis:             8.268         Condition No.:         152      \n",
       "====================================================================\n",
       "\n",
       "\"\"\""
      ]
     },
     "execution_count": 373,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lasso_result.summary2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "print(features_train.columns[lasso_result.pvalues > 0.05].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "        <td>Model:</td>               <td>OLS</td>         <td>Adj. R-squared:</td>      <td>0.913</td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <td>Dependent Variable:</td>       <td>price</td>             <td>AIC:</td>         <td>708461.6585</td>\n",
       "</tr>\n",
       "<tr>\n",
       "         <td>Date:</td>        <td>2022-10-16 19:31</td>        <td>BIC:</td>         <td>708548.3834</td>\n",
       "</tr>\n",
       "<tr>\n",
       "   <td>No. Observations:</td>        <td>43152</td>        <td>Log-Likelihood:</td>   <td>-3.5422e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "       <td>Df Model:</td>              <td>9</td>           <td>F-statistic:</td>      <td>5.000e+04</td> \n",
       "</tr>\n",
       "<tr>\n",
       "     <td>Df Residuals:</td>          <td>43142</td>      <td>Prob (F-statistic):</td>    <td>0.00</td>    \n",
       "</tr>\n",
       "<tr>\n",
       "      <td>R-squared:</td>            <td>0.913</td>            <td>Scale:</td>        <td>7.8994e+05</td> \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "     <td></td>       <th>Coef.</th>   <th>Std.Err.</th>     <th>t</th>     <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th>   <td>2436.0075</td>  <td>16.5060</td> <td>147.5830</td> <td>0.0000</td> <td>2403.6553</td> <td>2468.3596</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>carat</th>   <td>2459.2494</td>  <td>21.6328</td> <td>113.6814</td> <td>0.0000</td> <td>2416.8486</td> <td>2501.6501</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>cut</th>      <td>87.3153</td>   <td>4.3025</td>   <td>20.2939</td> <td>0.0000</td>  <td>78.8822</td>   <td>95.7483</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>color</th>   <td>-177.7663</td>  <td>2.6352</td>  <td>-67.4577</td> <td>0.0000</td> <td>-182.9314</td> <td>-172.6012</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>clarity</th> <td>229.5775</td>   <td>2.5687</td>   <td>89.3760</td> <td>0.0000</td> <td>224.5429</td>  <td>234.6122</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>depth</th>   <td>-45.6816</td>   <td>9.5612</td>   <td>-4.7778</td> <td>0.0000</td> <td>-64.4217</td>  <td>-26.9416</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>table</th>   <td>-132.9628</td>  <td>4.6694</td>  <td>-28.4753</td> <td>0.0000</td> <td>-142.1149</td> <td>-123.8106</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x</th>       <td>396.1678</td>   <td>89.8426</td>  <td>4.4096</td>  <td>0.0000</td> <td>220.0746</td>  <td>572.2610</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>y</th>       <td>276.1604</td>   <td>88.1343</td>  <td>3.1334</td>  <td>0.0017</td> <td>103.4155</td>  <td>448.9052</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>z</th>       <td>-103.6976</td>  <td>67.4174</td>  <td>-1.5381</td> <td>0.1240</td> <td>-235.8371</td>  <td>28.4418</td> \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "     <td>Omnibus:</td>    <td>7996.346</td>  <td>Durbin-Watson:</td>     <td>1.996</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <td>Prob(Omnibus):</td>   <td>0.000</td>  <td>Jarque-Bera (JB):</td> <td>44977.191</td>\n",
       "</tr>\n",
       "<tr>\n",
       "       <td>Skew:</td>       <td>0.779</td>      <td>Prob(JB):</td>       <td>0.000</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "     <td>Kurtosis:</td>     <td>7.753</td>   <td>Condition No.:</td>      <td>152</td>   \n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary2.Summary'>\n",
       "\"\"\"\n",
       "                  Results: Ordinary least squares\n",
       "====================================================================\n",
       "Model:              OLS              Adj. R-squared:     0.913      \n",
       "Dependent Variable: price            AIC:                708461.6585\n",
       "Date:               2022-10-16 19:31 BIC:                708548.3834\n",
       "No. Observations:   43152            Log-Likelihood:     -3.5422e+05\n",
       "Df Model:           9                F-statistic:        5.000e+04  \n",
       "Df Residuals:       43142            Prob (F-statistic): 0.00       \n",
       "R-squared:          0.913            Scale:              7.8994e+05 \n",
       "---------------------------------------------------------------------\n",
       "            Coef.    Std.Err.     t      P>|t|     [0.025     0.975] \n",
       "---------------------------------------------------------------------\n",
       "const     2436.0075   16.5060  147.5830  0.0000  2403.6553  2468.3596\n",
       "carat     2459.2494   21.6328  113.6814  0.0000  2416.8486  2501.6501\n",
       "cut         87.3153    4.3025   20.2939  0.0000    78.8822    95.7483\n",
       "color     -177.7663    2.6352  -67.4577  0.0000  -182.9314  -172.6012\n",
       "clarity    229.5775    2.5687   89.3760  0.0000   224.5429   234.6122\n",
       "depth      -45.6816    9.5612   -4.7778  0.0000   -64.4217   -26.9416\n",
       "table     -132.9628    4.6694  -28.4753  0.0000  -142.1149  -123.8106\n",
       "x          396.1678   89.8426    4.4096  0.0000   220.0746   572.2610\n",
       "y          276.1604   88.1343    3.1334  0.0017   103.4155   448.9052\n",
       "z         -103.6976   67.4174   -1.5381  0.1240  -235.8371    28.4418\n",
       "--------------------------------------------------------------------\n",
       "Omnibus:              7996.346      Durbin-Watson:         1.996    \n",
       "Prob(Omnibus):        0.000         Jarque-Bera (JB):      44977.191\n",
       "Skew:                 0.779         Prob(JB):              0.000    \n",
       "Kurtosis:             7.753         Condition No.:         152      \n",
       "====================================================================\n",
       "\n",
       "\"\"\""
      ]
     },
     "execution_count": 375,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "elastic_result.summary2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['z']\n"
     ]
    }
   ],
   "source": [
    "print(features_train.columns[elastic_result.pvalues > 0.05].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### 4. [1 point] Implement one of the elimination algorithms that were described in the Seminar_4 (Elimination by P-value, Forward elimination, Backward elimination), make conclusions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from pandas import DataFrame\n",
    "\n",
    "\n",
    "def eliminate_by_pvalue(model, alpha):\n",
    "    if isinstance(model, sm.OLS):\n",
    "        x = DataFrame(data=model.exog, columns=[model.exog_names])\n",
    "        y = DataFrame(data=model.endog, columns=[model.endog_names])\n",
    "\n",
    "        max_iter = model.exog.shape[1]\n",
    "\n",
    "        for i in range(max_iter):\n",
    "            res = sm.OLS(y, x).fit()\n",
    "            max_pvalue = res.pvalues.max()\n",
    "            if max_pvalue >= alpha:\n",
    "                col_to_drop = res.pvalues.index.values[res.pvalues.argmax()]\n",
    "                x.drop(labels=col_to_drop, axis=1, level=0, inplace=True)\n",
    "                continue\n",
    "            break\n",
    "\n",
    "        return sm.OLS(y, x)\n",
    "\n",
    "    raise Exception('only OLS models are supported')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "        <td>Model:</td>               <td>OLS</td>         <td>Adj. R-squared:</td>      <td>0.914</td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <td>Dependent Variable:</td>       <td>price</td>             <td>AIC:</td>         <td>707856.7639</td>\n",
       "</tr>\n",
       "<tr>\n",
       "         <td>Date:</td>        <td>2022-10-16 19:31</td>        <td>BIC:</td>         <td>707943.4887</td>\n",
       "</tr>\n",
       "<tr>\n",
       "   <td>No. Observations:</td>        <td>43152</td>        <td>Log-Likelihood:</td>   <td>-3.5392e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "       <td>Df Model:</td>              <td>9</td>           <td>F-statistic:</td>      <td>5.078e+04</td> \n",
       "</tr>\n",
       "<tr>\n",
       "     <td>Df Residuals:</td>          <td>43142</td>      <td>Prob (F-statistic):</td>    <td>0.00</td>    \n",
       "</tr>\n",
       "<tr>\n",
       "      <td>R-squared:</td>            <td>0.914</td>            <td>Scale:</td>        <td>7.7894e+05</td> \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "     <td></td>        <th>Coef.</th>   <th>Std.Err.</th>     <th>t</th>     <th>P>|t|</th>   <th>[0.025</th>     <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th>    <td>2624.2705</td>  <td>16.3907</td> <td>160.1069</td> <td>0.0000</td>  <td>2592.1444</td>  <td>2656.3967</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>carat</th>    <td>2699.0777</td>  <td>21.4817</td> <td>125.6453</td> <td>0.0000</td>  <td>2656.9731</td>  <td>2741.1823</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>cut</th>       <td>52.8693</td>   <td>4.2725</td>   <td>12.3744</td> <td>0.0000</td>   <td>44.4951</td>    <td>61.2434</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>color</th>    <td>-190.5337</td>  <td>2.6168</td>  <td>-72.8112</td> <td>0.0000</td>  <td>-195.6627</td>  <td>-185.4047</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>clarity</th>  <td>214.5259</td>   <td>2.5507</td>   <td>84.1037</td> <td>0.0000</td>  <td>209.5264</td>   <td>219.5253</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>depth</th>    <td>-46.6299</td>   <td>9.4944</td>   <td>-4.9113</td> <td>0.0000</td>  <td>-65.2391</td>   <td>-28.0208</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>table</th>    <td>-121.5332</td>  <td>4.6368</td>  <td>-26.2106</td> <td>0.0000</td>  <td>-130.6214</td>  <td>-112.4450</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x</th>       <td>-1372.6718</td>  <td>89.2151</td> <td>-15.3861</td> <td>0.0000</td> <td>-1547.5351</td> <td>-1197.8085</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>y</th>        <td>1893.3603</td>  <td>87.5187</td>  <td>21.6338</td> <td>0.0000</td>  <td>1721.8220</td>  <td>2064.8987</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>z</th>        <td>-188.8980</td>  <td>66.9466</td>  <td>-2.8216</td> <td>0.0048</td>  <td>-320.1145</td>  <td>-57.6814</td> \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "     <td>Omnibus:</td>    <td>7912.396</td>  <td>Durbin-Watson:</td>     <td>1.997</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <td>Prob(Omnibus):</td>   <td>0.000</td>  <td>Jarque-Bera (JB):</td> <td>51815.141</td>\n",
       "</tr>\n",
       "<tr>\n",
       "       <td>Skew:</td>       <td>0.724</td>      <td>Prob(JB):</td>       <td>0.000</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "     <td>Kurtosis:</td>     <td>8.169</td>   <td>Condition No.:</td>      <td>152</td>   \n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary2.Summary'>\n",
       "\"\"\"\n",
       "                  Results: Ordinary least squares\n",
       "====================================================================\n",
       "Model:              OLS              Adj. R-squared:     0.914      \n",
       "Dependent Variable: price            AIC:                707856.7639\n",
       "Date:               2022-10-16 19:31 BIC:                707943.4887\n",
       "No. Observations:   43152            Log-Likelihood:     -3.5392e+05\n",
       "Df Model:           9                F-statistic:        5.078e+04  \n",
       "Df Residuals:       43142            Prob (F-statistic): 0.00       \n",
       "R-squared:          0.914            Scale:              7.7894e+05 \n",
       "--------------------------------------------------------------------\n",
       "             Coef.    Std.Err.    t     P>|t|    [0.025     0.975]  \n",
       "--------------------------------------------------------------------\n",
       "const       2624.2705  16.3907 160.1069 0.0000  2592.1444  2656.3967\n",
       "carat       2699.0777  21.4817 125.6453 0.0000  2656.9731  2741.1823\n",
       "cut           52.8693   4.2725  12.3744 0.0000    44.4951    61.2434\n",
       "color       -190.5337   2.6168 -72.8112 0.0000  -195.6627  -185.4047\n",
       "clarity      214.5259   2.5507  84.1037 0.0000   209.5264   219.5253\n",
       "depth        -46.6299   9.4944  -4.9113 0.0000   -65.2391   -28.0208\n",
       "table       -121.5332   4.6368 -26.2106 0.0000  -130.6214  -112.4450\n",
       "x          -1372.6718  89.2151 -15.3861 0.0000 -1547.5351 -1197.8085\n",
       "y           1893.3603  87.5187  21.6338 0.0000  1721.8220  2064.8987\n",
       "z           -188.8980  66.9466  -2.8216 0.0048  -320.1145   -57.6814\n",
       "--------------------------------------------------------------------\n",
       "Omnibus:              7912.396      Durbin-Watson:         1.997    \n",
       "Prob(Omnibus):        0.000         Jarque-Bera (JB):      51815.141\n",
       "Skew:                 0.724         Prob(JB):              0.000    \n",
       "Kurtosis:             8.169         Condition No.:         152      \n",
       "====================================================================\n",
       "\n",
       "\"\"\""
      ]
     },
     "execution_count": 378,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "threshold = 0.05\n",
    "elimination_model = eliminate_by_pvalue(linear_model, threshold)\n",
    "elimination_model.fit().summary2()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### 5. [1 point] Find the best (in terms of RMSE) $\\alpha$ for Lasso regression using cross-validation with 4 folds. You must select values from range $[10^{-4}, 10^{3}]$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "alphas = np.logspace(-4, 3, 1000)\n",
    "searcher = GridSearchCV(Ridge(), [{\"alpha\": alphas}], scoring=\"neg_mean_squared_error\", cv=4)\n",
    "searcher.fit(features_train, target_train)\n",
    "print(\"Best alpha = %.4f\" % searcher.best_params_[\"alpha\"])\n",
    "\n",
    "plt.plot(alphas, -searcher.cv_results_[\"mean_test_score\"])\n",
    "plt.xscale(\"log\")\n",
    "plt.xlabel(\"alpha\")\n",
    "plt.ylabel(\"CV score\")\n",
    "#print(alphas) 2 59"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Gradient descent\n",
    "\n",
    "#### 6. [3.5 points] Implement a Ridge regression model for the MSE loss function, trained by gradient descent.\n",
    "\n",
    "All calculations must be vectorized, and python loops can only be used for gradient descent iterations. As a stop criterion, you must use (simultaneously):\n",
    "\n",
    "* checking for the Absolute-value norm of the weight difference on two adjacent iterations (for example, less than some small number of the order of $10^{-6}$, set by the `tolerance` parameter);\n",
    "* reaching the maximum number of iterations (for example, 10000, set by the `max_iter` parameter).\n",
    "\n",
    "You need to implement:\n",
    "\n",
    "* Full gradient descent:\n",
    "\n",
    "$$\n",
    "w_{k + 1} = w_{k} - \\eta_{k} \\nabla_{w} Q(w_{k}).\n",
    "$$\n",
    "\n",
    "* Stochastic Gradient Descent:\n",
    "\n",
    "$$\n",
    "w_{k + 1} = w_{k} - \\eta_{k} \\nabla_{w} q_{i_{k}}(w_{k}).\n",
    "$$\n",
    "\n",
    "$\\nabla_{w} q_{i_{k}}(w_{k}) \\, $ is the estimate of the gradient over the batch of objects selected randomly.\n",
    "\n",
    "* Momentum method:\n",
    "\n",
    "$$\n",
    "h_0 = 0, \\\\\n",
    "h_{k + 1} = \\alpha h_{k} + \\eta_k \\nabla_{w} Q(w_{k}), \\\\\n",
    "w_{k + 1} = w_{k} - h_{k + 1}.\n",
    "$$\n",
    "\n",
    "* Adagrad method:\n",
    "\n",
    "$$\n",
    "G_0 = 0, \\\\\n",
    "G_{k + 1} = G_{k} + (\\nabla_{w} Q(w_{k+1}))^2, \\\\\n",
    "w_{k + 1} = w_{k} - \\eta * \\frac{\\nabla_{w} Q(w_{k+1})}{\\sqrt{G_{k+1} + \\epsilon}}.\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "To make sure that the optimization process really converges, we will use the `loss_history` class attribute. After calling the `fit` method, it should contain the values of the loss function for all iterations, starting from the first one (before the first step on the anti-gradient).\n",
    "\n",
    "You need to initialize the weights with a random vector from normal distribution. The following is a template class that needs to contain the code implementing all variations of the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator\n",
    "\n",
    "class LinReg(BaseEstimator):\n",
    "    def __init__(self, delta=1.0, gd_type='Momentum',\n",
    "                 tolerance=1e-4, max_iter=1000, w0=None, eta=1e-2, alpha=1e-3):\n",
    "        \"\"\"\n",
    "        gd_type: str\n",
    "            'GradientDescent', 'StochasticDescent', 'Momentum', 'Adagrad'\n",
    "        delta: float\n",
    "            proportion of object in a batch (for stochastic GD)\n",
    "        tolerance: float\n",
    "            for stopping gradient descent\n",
    "        max_iter: int\n",
    "            maximum number of steps in gradient descent\n",
    "        w0: np.array of shape (d)\n",
    "            init weights\n",
    "        eta: float\n",
    "            learning rate\n",
    "        alpha: float\n",
    "            momentum coefficient\n",
    "        reg_cf: float\n",
    "            regularization coefficient\n",
    "        epsilon: float\n",
    "            numerical stability\n",
    "        \"\"\"\n",
    "        np.random.seed(10)\n",
    "        self.delta = delta\n",
    "        self.gd_type = gd_type\n",
    "        self.tolerance = tolerance\n",
    "        self.max_iter = max_iter\n",
    "        self.w0 = w0\n",
    "        self.alpha = alpha\n",
    "        self.w = None\n",
    "        self.eta = eta\n",
    "        self.loss_history = None # list of loss function values at each training iteration\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        X: np.array of shape (l, d)\n",
    "        y: np.array of shape (l)\n",
    "        ---\n",
    "        output: self\n",
    "        \"\"\"\n",
    "\n",
    "        supported_types = ['Momentum', 'GradientDescent', 'StochasticDescent']\n",
    "\n",
    "        if self.gd_type in supported_types:\n",
    "            np.random.seed(0)\n",
    "            self.loss_history = []\n",
    "            if self.w0 is None:\n",
    "                self.w0 = np.zeros(X.shape[1])\n",
    "            self.w = np.array(self.w0)\n",
    "            cur_w = np.array(self.w)\n",
    "            h = np.zeros(X.shape[1])\n",
    "\n",
    "            for i in range(0, self.max_iter):\n",
    "                if self.gd_type == supported_types[1]:\n",
    "                    self.w -= self.eta * self.calc_gradient(X, y)\n",
    "                elif self.gd_type == supported_types[2]:\n",
    "                    indexes = np.random.choice(X.shape[0], int(X.shape[0] * self.delta))\n",
    "                    self.w -= self.eta * self.calc_gradient(np.take(X, indexes, axis=0), np.take(y, indexes))\n",
    "                else:\n",
    "                    indexes = np.random.choice(X.shape[0], int(X.shape[0] * self.delta))\n",
    "                    h = self.alpha * h + self.eta * self.calc_gradient(np.take(X, indexes, axis=0), np.take(y, indexes))\n",
    "                    self.w -= h\n",
    "                self.loss_history.append(self.calc_loss(X, y))\n",
    "                if np.linalg.norm(self.w - cur_w) < self.tolerance:\n",
    "                    break\n",
    "                cur_w = np.array(self.w)\n",
    "\n",
    "            return self\n",
    "        raise Exception('Unknown type')\n",
    "\n",
    "\n",
    "    def predict(self, X):\n",
    "        if self.w is None:\n",
    "            raise Exception('Not trained yet')\n",
    "\n",
    "        return np.dot(X, self.w)\n",
    "\n",
    "\n",
    "    def calc_gradient(self, X, y):\n",
    "        \"\"\"\n",
    "        X: np.array of shape (l, d) (l can be equal to 1 if stochastic)\n",
    "        y: np.array of shape (l)\n",
    "        ---\n",
    "        output: np.array of shape (d)\n",
    "        \"\"\"\n",
    "        return 2 * np.dot(X.T, np.dot(X, self.w) - y) / y.shape[0]\n",
    "\n",
    "\n",
    "    def calc_loss(self, X, y):\n",
    "        \"\"\"\n",
    "        X: np.array of shape (l, d)\n",
    "        y: np.array of shape (l)\n",
    "        ---\n",
    "        output: float\n",
    "        \"\"\"\n",
    "        return np.mean((self.predict(X) - y) ** 2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### 7. [1 points] Train and validate \"hand-written\" models on the same data, and compare the quality with the Sklearn or StatsModels methods. Investigate the effect of the `max_iter` and `alpha` parameters on the optimization process. Is it consistent with your expectations?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "momentum = LinReg(gd_type='Momentum', max_iter=1000).fit(features_train, target_train)\n",
    "print(\"Test RMSE = %.4f\" % sqrt(mean_squared_error(target_test, momentum.predict(features_test))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "gd_types = np.array(['GradientDescent', 'StochasticDescent', 'Momentum'])\n",
    "max_iters = np.arange(50, 5000, 200)\n",
    "alphas = np.logspace(-4, 3, 20)\n",
    "searcher = GridSearchCV(LinReg(), [{\"gd_type\": gd_types, \"max_iter\": max_iters, \"alpha\": alphas}],\n",
    "                        scoring=\"neg_mean_squared_error\", cv=4)\n",
    "searcher.fit(features_train, target_train)\n",
    "print(\"Best gd_type = %s\" % searcher.best_params_[\"gd_type\"])\n",
    "print(\"Best max_iter = %d\" % searcher.best_params_[\"max_iter\"])\n",
    "print(\"Best alpha = %.4f\" % searcher.best_params_[\"alpha\"])\n",
    "# print(\"Test RMSE = %.4f\" % sqrt(mean_squared_error(target_test, LinReg(gd_type='', max_iter=, alpha=).fit(features_train, target_train).predict(features_test))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### 8. [1 points] Plot graphs (on the same picture) of the dependence of the loss function value on the iteration number for Full GD, SGD, Momentum and Adagrad. Draw conclusions about the rate of convergence of various modifications of gradient descent.\n",
    "\n",
    "Don't forget about what *beautiful* graphics should look like!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# your code here\n",
    "#â•°( Í¡Â° ÍœÊ– Í¡Â° )ã¤â”€â”€â˜†*:ãƒ»ï¾Ÿ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
